<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-01-23T18:10:37+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Thomas Carstens</title><subtitle>Thomas Carstens ~ Tech portfolio</subtitle><author><name>Thomas Carstens</name><email>thomas.carstens@edu.devinci.fr</email></author><entry><title type="html">Benchmarking drone flight</title><link href="http://localhost:4000/project/drone-benchmarking/" rel="alternate" type="text/html" title="Benchmarking drone flight" /><published>2019-05-31T00:00:00+02:00</published><updated>2019-05-31T00:00:00+02:00</updated><id>http://localhost:4000/project/drone-benchmarking</id><content type="html" xml:base="http://localhost:4000/project/drone-benchmarking/">&lt;p&gt;G-JXW6H528P6&lt;/p&gt;

&lt;h2 id=&quot;section:hardware&quot;&gt;Hardware Environment&lt;/h2&gt;
&lt;h3 id=&quot;drone-selection-process&quot;&gt;Drone Selection Process&lt;/h3&gt;
&lt;p&gt;Several drones were compared using custom criteria for drone development. These custom criteria are based on ease of use and programmability. The Dimensions criterion aims to minimize the drone size and weight. The Reconfigurable criterion investigates the modularity of the hardware layout. The Programmable criterion looks at the available interfaces for communicating with the firmware. The Autonomous Flight criterion looks at the compatibility of state estimation and trajectory planning algorithms.&lt;/p&gt;
&lt;div class=&quot;table*&quot;&gt;
&lt;div class=&quot;flushleft&quot;&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Criteria&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Snapdragon Flight Pro&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Bitcraze Crazyflie&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Tello Drone&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Custom Flight Controller&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;!-- &lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Flight Pro&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Crazyflie&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Drone&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Controller&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt; --&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Dimensions&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9673; &amp;#9673; &amp;#9673; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9678; &amp;#9678; &amp;#9678; &amp;#9678;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9678; &amp;#9678; &amp;#9678; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9678; &amp;#9673; &amp;#9673; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Reconfigurable&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9678; &amp;#9678; &amp;#9678; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9678; &amp;#9678; &amp;#9678; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9673; &amp;#9673; &amp;#9673; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9678; &amp;#9678; &amp;#9678; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Programmable&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9678; &amp;#9678; &amp;#9673; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9678; &amp;#9678; &amp;#9678; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9678; &amp;#9678; &amp;#9673; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9678; &amp;#9673; &amp;#9673; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Autonomous Flight&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9678; &amp;#9678; &amp;#9673; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9678; &amp;#9678; &amp;#9678; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9678; &amp;#9678; &amp;#9678; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678; &amp;#9673; &amp;#9673; &amp;#9673; &amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Selection&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The Crazyflie Drone has several advantages over other drones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Autonomous Flight&lt;/strong&gt;. State estimation and trajectory planning are managed by the Crazyflie firmware. The operating procedure is simplified to sending setpoint commands from a remote PC. &lt;a class=&quot;citation&quot; href=&quot;#chaari_cheikhrouhou_koubâa_youssef_hmam_2021&quot;&gt;(Chaari et al., 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;crazyflie_docs&quot;&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Programmability&lt;/strong&gt;. At the moment of writing, there are two APIs known to send high-level commands to the drone. &lt;a class=&quot;citation&quot; href=&quot;#crazyflie_docs&quot;&gt;(“Crazyflie Documentation (Accessed 30 September 2021),” 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;crazyflie_docs&quot;&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Dimensions&lt;/strong&gt;. Due to our space constraints, a small, light drone is preferable. Our payload of motion-capture markers brings the Crazyflie’s mass to 33 grams.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Reconfigurability&lt;/strong&gt;. The Crazyflie is easily assembled and maintainable. It is compatible with a range of sensor modules for different activities. &lt;a class=&quot;citation&quot; href=&quot;#crazyflie_docs&quot;&gt;(“Crazyflie Documentation (Accessed 30 September 2021),” 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;crazyflie_docs&quot;&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;flight-arena-and-spatial-localization&quot;&gt;Flight Arena and Spatial Localization&lt;/h3&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/testbed/platform.jpg&quot; style=&quot;width:2cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/testbed/camera_layout/cork_material.png&quot; style=&quot;width:2cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The Flight Area (Figure &lt;a href=&quot;#fig:flight_arena&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:flight_arena&quot;&gt;[fig:flight_arena]&lt;/a&gt;) measures 3 x 2 meters, with a table-to-ceiling distance of 1.3m. In order to dampen the impact of falling drones, the table is layered with anti-vibration cork material (Figure &lt;a href=&quot;#fig:cork&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:cork&quot;&gt;[fig:cork]&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Optitrack &lt;a class=&quot;citation&quot; href=&quot;#crazyflie_docs&quot;&gt;(“Crazyflie Documentation (Accessed 30 September 2021),” 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;optitrack&quot;&gt;&lt;/span&gt; was adopted as the Motion Capture since the equipment was available in the laboratory. It is compatible with the swarm management solution of Section &lt;a href=&quot;#section:swarm&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;section:swarm&quot;&gt;1.5.5&lt;/a&gt;. Optitrack uses a Point Cloud reconstruction engine &lt;a class=&quot;citation&quot; href=&quot;#optitrack&quot;&gt;(Point, n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;optitrack_docs&quot;&gt;&lt;/span&gt;. That is, it triangulates two-dimensional points from camera images into coordinates in a three-dimensional space. For this purpose, four Flex 13 cameras are set up on the Flight Arena (as seen in Figure &lt;a href=&quot;#fig:flight_arena&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:flight_arena&quot;&gt;[fig:flight_arena]&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The Flex 13 cameras &lt;a class=&quot;citation&quot; href=&quot;#optitrack_docs&quot;&gt;(&lt;i&gt;Optitrack Documentation Version 2.2 (Accessed 30 July 2021)&lt;/i&gt;, 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;optitrack_docs&quot;&gt;&lt;/span&gt; are infrared cameras, and so they must have an unobstructed view of any tracked object.&lt;/p&gt;
&lt;p&gt;The exact positions of the cameras give a certain coverage of the Flight Arena. The next section determines how much of the Flight Arena is localized by the cameras.&lt;/p&gt;
&lt;h3 id=&quot;lightray-coverage-study&quot;&gt;Lightray Coverage Study&lt;/h3&gt;
&lt;p&gt;We investigate how much of the flight arena is localized by the motion capture. The drones can only be flown in a space covered by the infrared cameras, therefore we perform a design study to maximize this flight space.&lt;/p&gt;
&lt;h4 id=&quot;lightray-simulation&quot;&gt;Lightray Simulation&lt;/h4&gt;
&lt;p&gt;A model is designed in Solidworks &lt;a class=&quot;citation&quot; href=&quot;#optitrack_docs&quot;&gt;(&lt;i&gt;Optitrack Documentation Version 2.2 (Accessed 30 July 2021)&lt;/i&gt;, 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;solidworks&quot;&gt;&lt;/span&gt; to simulate the coverage of our cameras. Figure &lt;a href=&quot;#fig:45&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:45&quot;&gt;[fig:45]&lt;/a&gt; simulates the camera coverage on a table the size of the Flight Arena. A key factor is the camera’s pitch angle down from the horizontal. Figure &lt;a href=&quot;#fig:45&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:45&quot;&gt;[fig:45]&lt;/a&gt; compares an orientation at 45° from the horizontal to one at 30 degrees.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/testbed/camera_layout/45_iso.PNG&quot; style=&quot;width:5cm;height:4cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/testbed/camera_layout/30_iso.PNG&quot; style=&quot;height:4cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/testbed/camera_layout/intersection/firstray.PNG&quot; style=&quot;height:2cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/testbed/camera_layout/intersection/allrays.PNG&quot; style=&quot;height:2cm&quot; alt=&quot;image&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;../../assets/images/testbed/camera_layout/intersection/intersection.PNG&quot; style=&quot;height:2cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/testbed/camera_layout/intersection/final.PNG&quot; style=&quot;height:2cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The coverage percentage is determined as the volume of space localized by the flight cameras over the total usable volume above the Flight Arena. Figure &lt;a href=&quot;#fig:intersection_procedure&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:intersection_procedure&quot;&gt;[fig:intersection_procedure]&lt;/a&gt; shows the modelling process of ray coverage volumes. The design requirements are as follow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The cameras are placed above the table corners. within the net region so as to have a clear view of the drones when the net is lowered&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There are a total of 4 cameras available during motion capture installation. The flight space measures 3×2×1.3 m.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Flex 13 cameras have a 56° field of view, and this is replicated in simulation (Figure &lt;a href=&quot;#fig:intersection_procedure&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:intersection_procedure&quot;&gt;[fig:intersection_procedure]&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In order to triangulate a position, the motion capture requires a minimum of 2 rays to intersect &lt;a class=&quot;citation&quot; href=&quot;#solidworks&quot;&gt;(Dassault Systems, 2010)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;optitrack_docs&quot;&gt;&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These volumes can then be determined in Solidworks using its Volumetric Tool &lt;a class=&quot;citation&quot; href=&quot;#optitrack_docs&quot;&gt;(&lt;i&gt;Optitrack Documentation Version 2.2 (Accessed 30 July 2021)&lt;/i&gt;, 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;solidworks_docs&quot;&gt;&lt;/span&gt;. For a pitch angle of 30&lt;span class=&quot;math inline&quot;&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;, The volumes of the flight space and of the intersection area above, are respectively of 7.8 &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;m&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt; and 6.134 &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;m&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;. As a result, we determine that the usable region for flight is 78.64% of the 3×2×1.3 m flight space. This demonstrates that 20% of the flight space is unusable. This is not surprising, considering that 4 cameras are directly above the table and constrained by the netting.&lt;/p&gt;
&lt;h4 id=&quot;coverage-optimisation-study&quot;&gt;Coverage Optimisation Study&lt;/h4&gt;
&lt;p&gt;The camera’s pitch angle is varied to determine the point of optimal coverage. Figure &lt;a href=&quot;#fig:volumes&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:volumes&quot;&gt;[fig:volumes]&lt;/a&gt; shows the volumes generated during the study.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/testbed/camera_layout/corner_20deg.PNG&quot; style=&quot;height:3.8cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/testbed/camera_layout/corner_25deg.PNG&quot; style=&quot;height:3.8cm&quot; alt=&quot;image&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;../../assets/images/testbed/camera_layout/corner_30deg.PNG&quot; style=&quot;height:4cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/testbed/camera_layout/corner_35deg.PNG&quot; style=&quot;height:4cm&quot; alt=&quot;image&quot; /&gt;&lt;br /&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The study has two parts. The first study varies by increments of 5&lt;span class=&quot;math inline&quot;&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;, in order to determine the range of volume maxima. The second study finetunes by increments of 1&lt;span class=&quot;math inline&quot;&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;, with the help of a Solidworks Design Study &lt;a class=&quot;citation&quot; href=&quot;#solidworks_docs&quot;&gt;(&lt;i&gt;Solidworks 2021 Documentation (Accessed 30 July 2021)&lt;/i&gt;, 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;solidworks_docs&quot;&gt;&lt;/span&gt;. This simulation tool is used to generate volumes automatically. It has trouble generating volumes if the angle increments are too large, therefore the first study is done manually.&lt;/p&gt;
&lt;div class=&quot;table*&quot;&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Pitch&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Volume&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;%&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Max&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;(deg)&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;(&lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;m&lt;/em&gt;&lt;em&gt;m&lt;/em&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/span&gt;)&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Covered&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Range&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;10&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;6 607 805 800.35&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;84.7154&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;15&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;6 970 142 481.03&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;89.3608&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;20&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;7 014 173 181.97&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;89.9253&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;25&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;6 804 720 310.30&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;87.2400&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;30&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;4 855 500 995.33&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;62.2500&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;35&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;5 006 693 602.87&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;64.1884&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;40&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;3 747 076 773.66&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;48.0394&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;
&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Pitch&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Volume&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;%&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Max&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;(deg)&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;(&lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;m&lt;/em&gt;&lt;em&gt;m&lt;/em&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/span&gt;)&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Covered&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Range&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;16&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;7007549683.27&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;89.8404&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;17&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;7034381844.70&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;90.1844&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;18&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;7050625519.44&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;90.3926&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;19&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;7056144612.83&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;90.4634&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;20&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;7014172714.71&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;89.9253&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9678;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;21&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;6999358686.15&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;89.7354&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;22&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;6972124192.42&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;89.3862&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;#9673;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Through these studies, the coverage volume was increased from 78.64% by % up to 89.95%, and by % to . This demonstrates that about 10% of the flight space is still out of reach. While the netting constraint forces the cameras to have this inconvenience, the study could be further optimised by varying the yaw angle of the cameras and moving them away from the corners.&lt;/p&gt;
&lt;h3 id=&quot;flight-stability-tests&quot;&gt;Flight Stability Tests&lt;/h3&gt;
&lt;p&gt;Tests of the stability of robotic systems are routinely performed to measure their robustness to external forces. This is a key challenge in drone development &lt;a class=&quot;citation&quot; href=&quot;#solidworks_docs&quot;&gt;(&lt;i&gt;Solidworks 2021 Documentation (Accessed 30 July 2021)&lt;/i&gt;, 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;crazyflie_docs&quot;&gt;&lt;/span&gt;, where a drone maintains dynamic stability by counterbalancing six directions of freedom, as opposed to two for wheeled systems in static stability.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/controlloop_bw.png&quot; style=&quot;width:6cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Drone Selection Matrix.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;The Crazyswarm ecosystem &lt;a class=&quot;citation&quot; href=&quot;#crazyflie_docs&quot;&gt;(“Crazyflie Documentation (Accessed 30 September 2021),” 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;crazyflie_docs&quot;&gt;&lt;/span&gt; makes use of a position and rate controller for each drone in its swarm. This means that a setpoint is sent to each drone separately, each correcting their current pose towards the setpoint. The streaming setpoints are broadcasted from one or more antennas. As the number of drones in the swarm increases, they receive less frequent broadcasts from the antenna &lt;a class=&quot;citation&quot; href=&quot;#crazyflie_docs&quot;&gt;(“Crazyflie Documentation (Accessed 30 September 2021),” 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;preiss_hönig_sukhatme_ayanian_2017&quot;&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The purpose of this test is to determine the response of the drone’s position and angle controllers to the natural disturbance during hovering. This experiment investigates the effect of antenna distance and interference on drone flight. With multiple drones to a single antenna, we evaluate if the system demonstrates any performance limits.&lt;/p&gt;
&lt;h4 id=&quot;hypothesis&quot;&gt;Hypothesis&lt;/h4&gt;
&lt;p&gt;The hypothesis is as such: the error in drone pose will correlate with the distance of the drones from the antenna.&lt;/p&gt;
&lt;h4 id=&quot;prediction&quot;&gt;Prediction&lt;/h4&gt;
&lt;p&gt;A hover stability test is a good measure of system performance since it requires quick readjustments of the drone to counter natural disturbances during hovering. In &lt;a class=&quot;citation&quot; href=&quot;#preiss_hönig_sukhatme_ayanian_2017&quot;&gt;(Preiss et al., 2017)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;experimental_tuning&quot;&gt;&lt;/span&gt;, determine the performance of their flight controller by comparing the attitude of the drone in relation to the demanded null value of angular rotations. In contrast, our input is a setpoint. The output is a set of translation and rotational angles relative to a demanded null value for translation and rotation. This output is graphed as a deviation over time. The shape of the response charts are associated with flight stability over time.&lt;/p&gt;
&lt;h4 id=&quot;experiment-methodology&quot;&gt;Experiment Methodology&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/chore_pictures/3drones/pic.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;The Crazyflie 2.1 miniature quadcopter with four motion-capture markers, expansion boards (not visib&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;Hover stability is examined on the Flight Arena. The telemetry recording and external video cameras are programmed to launch with the swarm control interface. Three drones are hovered in the Flight Arena at an altitude of around 1 m. It was possible to record a 20-s long autonomous flight during which the flight controller attempted to stabilize the quadcopter. During that time, the quadcopter remained within a radius of two meters from its takeoff location.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: In preparation for the flight, each of the three drones are inspected for minimal positional displacement of less than (1cm + 0.01 rad). This ensures fully functional position controllers for the drones.&lt;/p&gt;
&lt;h4 id=&quot;results&quot;&gt;Results&lt;/h4&gt;
&lt;p&gt;The flightpaths of the three drones are plotted alongside . The topview and the sideview are featured below.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/chore_pictures/3drones/x_translations_ann.png&quot; style=&quot;width:6cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/chore_pictures/3drones/y_translations_ann.png&quot; style=&quot;width:6cm&quot; alt=&quot;image&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;../../assets/images/chore_pictures/3drones/z_translations_ann.png&quot; style=&quot;width:6cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/chore_pictures/3drones/anglerates_ann.png&quot; style=&quot;width:6cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/chore_pictures/3drones/3dplot_paths_inv.png&quot; style=&quot;width:4.5cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Overview of Selection Process.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;The flightpaths are smooth and generally show very minimal jerking. There are very little discontinuities, attesting to a continuous localization process.&lt;/p&gt;
&lt;div class=&quot;table*&quot;&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;strong&gt;Criteria&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;strong&gt;Drone 1&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;strong&gt;Drone 2&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Min&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Max&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Range&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Min&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Max&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Range&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;X&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;-0.84&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;-0.80&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.04&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.44&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.48&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.04&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;y&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;-0.0272&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;-0.0228&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.0044&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;-0.0289&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;-0.0255&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.0034&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Z&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.31&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.37&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.06&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.28&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.34&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.06&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Roll&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;-0.018&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.009&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.027&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.009&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.017&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.008&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Pitch&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;-0.020&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.036&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.056&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;-0.003&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.026&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.029&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Yaw&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;-0.034&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.039&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.073&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;-0.014&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.021&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0.035&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Selection&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;\ding{55}&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&amp;amp;#1000;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&amp;amp;#9675; &amp;#9673; &amp;amp;#1000; &amp;amp;#x02A2F; Drone 2 has less variation in both translations and rotations than Drone 1. This is confirmed in Table &lt;a href=&quot;#tab:stability_comparison&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;tab:stability_comparison&quot;&gt;[tab:stability_comparison]&lt;/a&gt;. Drone 2 is more stable in this test than Drone 1. The sample hover error is .&lt;/p&gt;
&lt;p&gt;The discrepancy between the two drones could be attributed to a number of factors. This work may be improved with a second test, where the two drones’ positions are inversed. All in all, the flight is substantially accurate, with a peak translation of 6cm.&lt;/p&gt;
&lt;h4 id=&quot;conclusion-of-test&quot;&gt;Conclusion of Test&lt;/h4&gt;
&lt;p&gt;Drone 2 is further from the arena and exhibits more stability. The drone that is furthest from the antenna does not have more pose error, and the hypothesis is rejected.&lt;/p&gt;</content><author><name>Thomas Carstens</name><email>thomas.carstens@edu.devinci.fr</email></author><category term="Project" /><category term="Autonomy stack for robotics" /><category term="Python" /><category term="Motion capture" /><category term="Trajectory planning algorithms" /><summary type="html">We perform real-life demonstrations of our progress in our custom flight arena.</summary></entry><entry><title type="html">Design of a Flight Copilot</title><link href="http://localhost:4000/project/copilot-solution/" rel="alternate" type="text/html" title="Design of a Flight Copilot" /><published>2019-05-31T00:00:00+02:00</published><updated>2019-05-31T00:00:00+02:00</updated><id>http://localhost:4000/project/copilot-solution</id><content type="html" xml:base="http://localhost:4000/project/copilot-solution/">&lt;style&gt;
.img-container {
text-align: center;
}
&lt;/style&gt;
&lt;h1 id=&quot;c1&quot;&gt;A Testbed Environment for Task Development &lt;/h1&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;According to &lt;a class=&quot;citation&quot; href=&quot;#swarm_review&quot;&gt;(Navarro &amp;amp; Matía, 2012)&lt;/a&gt;, swarm robotics is an approach to collective robotics that takes inspiration from the self-organized behaviors of social animals. Through simple rules and local interactions, swarm robotics aims for robust, scalable and flexible collective behaviors for the coordination of large numbers of robots. In contrast, the term swarm engineering &lt;a class=&quot;citation&quot; href=&quot;#swarm_review&quot;&gt;(Navarro &amp;amp; Matía, 2012)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;swarm_engineering&quot;&gt;&lt;/span&gt; describes the design of predictable, controllable robot swarms with well-defined goals and the ability to function under certain conditions. Swarm engineering focuses mainly on concepts that could be relevant for real-world applications, therefore shifting swarm robotics to engineering applications. We motivate a smarter ecosystem for task development upon drones by beginning with the infrastructure for new technologies and for prototyping functionalities. A centralised swarm framework serves to set up flight performance monitoring systems, a fundamental asset to the development of robots and multi-robot groups &lt;a class=&quot;citation&quot; href=&quot;#swarm_engineering&quot;&gt;(Brambilla et al., 2013)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;gcs_validation&quot;&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Multi-robot systems and swarms of unmanned aerial vehicles (UAVs) in particular have many practical applications. &lt;a class=&quot;citation&quot; href=&quot;#gcs_validation&quot;&gt;(Larrabee et al., 2013)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;overview_uavsensors&quot;&gt;&lt;/span&gt; from March 2021 shows applications as diverse as surveillance and monitoring, inventory management, search and rescue, or in the entertainment industry. Swarm intelligence has, by definition, a distributed nature. Yet performing experiments in truly distributed systems is not always possible, as much of the underlying ecosystem employed requires some sort of central control. Indeed, in experimental proofs of concept, most research relies on more traditional connectivity solutions and centralized approaches &lt;a class=&quot;citation&quot; href=&quot;#overview_uavsensors&quot;&gt;(Balestrieri et al., 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;gcs_validation&quot;&gt;&lt;/span&gt;&lt;a class=&quot;citation&quot; href=&quot;#gcs_validation&quot;&gt;(Larrabee et al., 2013)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;buzz_swarm_stack&quot;&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In recent years there have been significant advancements in this research field. However, very rarely do UAV swarms leave the controlled and safe environment of laboratories, and when they do it is for short-duration experiments. The current use of swarms is generally based on custom, centralized solutions, in environments with reliable communication &lt;a class=&quot;citation&quot; href=&quot;#buzz_swarm_stack&quot;&gt;(Varadharajan et al., 2017)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;buzz_swarm_stack&quot;&gt;&lt;/span&gt;. There remains large challenges to reliable flight of UAVs: the reliability of software, the limits on the hardware, test and validation of new elements on pre-existing systems &lt;a class=&quot;citation&quot; href=&quot;#buzz_swarm_stack&quot;&gt;(Varadharajan et al., 2017)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;gcs_validation&quot;&gt;&lt;/span&gt;. A further challenge concerns multi-robot functionality. Current UAVs have very limited functionality for multi-robot coordination. Market drones are too limited for swarm research, as they only supports a single point-to-point link between a program and the drone, thus, a program can only communicate with a single drone.&lt;/p&gt;
&lt;p&gt;In line with the thesis goal, we seek to better understand how distributed systems can offer smart assistance to multi-robot development. We explore research and techniques designed to coordinate multiple robots.&lt;/p&gt;
&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;
&lt;p&gt;In this section, we examine how researchers tackle the challenge of swarm engineering in the past. Prior work exists in a variety of drone laboratories &lt;a class=&quot;citation&quot; href=&quot;#gcs_validation&quot;&gt;(Larrabee et al., 2013)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;flightmare&quot;&gt;&lt;/span&gt; &lt;a class=&quot;citation&quot; href=&quot;#flightmare&quot;&gt;(Song et al., 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;fma_paper&quot;&gt;&lt;/span&gt;. Drones Spatial Localization, UAV Architectures and UAV Swarm Frameworks require tradeoffs.&lt;/p&gt;
&lt;h3 id=&quot;uav-spatial-localization&quot;&gt;UAV Spatial Localization&lt;/h3&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/drones_setup.jpg&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;An example of drones tracked by two motion capture cameras.  &lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;The Flight of UAVs, as with any robotic system, requires accurate positioning. However, a drone suffers from cumulative drift in position data &lt;a class=&quot;citation&quot; href=&quot;#fma_paper&quot;&gt;(Lupashin et al., 2014)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;uav_components&quot;&gt;&lt;/span&gt;. In order to achieve autonomous flight, a drone will need to know if it is following a trajectory correctly. A localization technology allows for this centimeter level accuracy.&lt;/p&gt;
&lt;p&gt;It is by using highly precise equipment, that UAVs can have highly precise state estimation. This setup includes the selection of onboard positioning systems as well as external positioning solutions. We focus on optical motion capture coupled with algorithms for state estimation primarily a set of technologies commonly used by UAV laboratories&lt;a class=&quot;citation&quot; href=&quot;#uav_components&quot;&gt;(“Unmanned Systems Technology: Explanations (Viewed on 28 November 2021) ,” 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;phan_hönig_ayanian_2018&quot;&gt;&lt;/span&gt; &lt;a class=&quot;citation&quot; href=&quot;#phan_hönig_ayanian_2018&quot;&gt;(Phan et al., 2018)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;laclau_tempez_ruffier_natalizio_mouret_2020&quot;&gt;&lt;/span&gt; &lt;a class=&quot;citation&quot; href=&quot;#laclau_tempez_ruffier_natalizio_mouret_2020&quot;&gt;(Laclau et al., 2020)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;fma_paper&quot;&gt;&lt;/span&gt;:.&lt;/p&gt;
&lt;p&gt;The high level of precision from a motion capture system allows us to synchronously hover multiple UAVs. The tracker precision can reach sub-millimeter accuracy, and drones are hovered at a precision of a few centimeters.&lt;/p&gt;
&lt;h3 id=&quot;uav-architectures&quot;&gt;UAV Architectures&lt;/h3&gt;
&lt;p&gt;UAVs such as AscTec Pelican, Parrot AR.Drone, and Erle-Copter are other examples of UAVs commonly used in the literature. These MAVs have Software Development Kits (SDK) that enable applications from third-party developers to communicate with the drones. However, both SDKs are too limited for swarm research, as they only supports a single point-to-point link between a program and the drone, thus, a program can only communicate with a single drone.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/DSCF0871.jpg&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;The Crazyflie 2.1 miniature quadcopter.  &lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;In comparison, the Crazyflie packages the full robotic stack. This robotic stack includes its own state estimator, control architecture and trajectory follower, which work out of the box. FreeRTOS handles the scheduling of processes and control the flight calculations. The Crazyflie contains a 32-bit, 168MHz ARM microcontroller with floating-point unit that is capable of significant onboard computation. The FreeRTOS firmware is opensource and modifiable.&lt;/p&gt;
&lt;p&gt;The Crazyflie’s small size makes it suitable for indoor flight in dense formations. As a result, it has been used widely in research. As of 2021, this drone is used to validate research: from new algorithms for agile flight &lt;a class=&quot;citation&quot; href=&quot;#fma_paper&quot;&gt;(Lupashin et al., 2014)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;laclau_tempez_ruffier_natalizio_mouret_2020&quot;&gt;&lt;/span&gt; to drone swarm research &lt;a class=&quot;citation&quot; href=&quot;#laclau_tempez_ruffier_natalizio_mouret_2020&quot;&gt;(Laclau et al., 2020)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;phan_hönig_ayanian_2018&quot;&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;uav-swarm-frameworks&quot;&gt;UAV Swarm Frameworks&lt;/h3&gt;
&lt;p&gt;Unmanned Aerial Vehicle (UAV) swarms have been used indoors for formation flight and collaborative behaviors, outdoors to demonstrate swarming algorithms, and in the media for artistic shows. According to &lt;em&gt;&lt;/em&gt; &lt;a class=&quot;citation&quot; href=&quot;#phan_hönig_ayanian_2018&quot;&gt;(Phan et al., 2018)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;swarm_review&quot;&gt;&lt;/span&gt;, The group of robots has some special characteristics, which are found in swarms of insects, that is, decentralised control, lack of synchronisation, simple and (quasi) identical members. In this section, we explore the requirements placed on the software framework for interacting with a robot swarm.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#swarm_review&quot;&gt;(Navarro &amp;amp; Matía, 2012)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;pinciroli_lee-brown_beltrame_2015&quot;&gt;&lt;/span&gt; &lt;em&gt;&lt;/em&gt; discuss the key requirements a successful programming language for swarm robotics must meet. According to them, the level of abstraction need be adapted to the task at hand. The complexity of concentrating on individual robots and their interactions, i.e., a bottom-up approach, increases steeply with the size of the swarm. Conversely, a purely top-down approach, i.e., focused on the behaviour of the swarm as a whole, might lack expressive power to fine-tune specific robot behaviors. The runtime platform of the language must ensure acceptable levels of &lt;strong&gt;scalability&lt;/strong&gt; (for increasing swarm sizes) and &lt;strong&gt;robustness&lt;/strong&gt; (in case of temporary communication issues).&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#pinciroli_lee-brown_beltrame_2015&quot;&gt;(Pinciroli et al., 2015)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;fma_paper&quot;&gt;&lt;/span&gt; &lt;em&gt;&lt;/em&gt; present The Flying Machine Arena. This was put in place in 2014 with the goal of becoming a &quot;demo-and-development&quot; arena. They include both single and multi-robot experiments. One key element of their work is that the UAV swarm can be heterogeneous. Additionally, the position controller runs offboard, that is, the UAVs all rely on a companion computer to position themselves. The additional computational power is used for a latency compensation algorithm to improve accuracy for high-speed flights. Despite this, the framework remains robust: swarms of up to 5 UAVs are flown on a regular basis.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#fma_paper&quot;&gt;(Lupashin et al., 2014)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;phan_hönig_ayanian_2018&quot;&gt;&lt;/span&gt; &lt;em&gt;&lt;/em&gt; define a system architecture for a large swarm of miniature quadcopters flying in dense formation indoors. The main challenges in swarm robotics are addressed in this framework, namely by reducing communication latency to 26ms. This is done in major part via the structure of messages broadcasted to the UAV. Preiss et al. &lt;a class=&quot;citation&quot; href=&quot;#phan_hönig_ayanian_2018&quot;&gt;(Phan et al., 2018)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;phan_hönig_ayanian_2018&quot;&gt;&lt;/span&gt; use a programmable UAV with an onboard position controller, making the system more robust to communication packet drops. With this method, a swarm of 49 Crazyflies have been flown using 3 radios. As a result, the drone swarm framework allows for robotics developers to send commands to drones in a fleet. A scalable and robust run-time platform is, in this way, a key element for real-world deployment of swarm behaviors.&lt;/p&gt;
&lt;h3 id=&quot;uav-software-and-middleware&quot;&gt;UAV Software and Middleware&lt;/h3&gt;
&lt;p&gt;UAVs have a long tradition of being controlled with the Robotic Operating System. ROS is a meta-operating system designed for the construction of distributed systems. It provides a set of extensible tools for managing distributed robotic applications. The main goals of ROS are package management, hardware abstraction, low-level device control, message exchange between processes, and implementation of several functionalities. As a result, there are many ROS packages devoted to controlling such UAVs as individuals.&lt;/p&gt;
&lt;p&gt;However, using multiple UAVs creates entirely new challenges that such packages cannot address. These new challenges include, but are not limited to, the physical space required to operate the robots, the interference of sensors and network communication, and safety requirements. In &lt;a class=&quot;citation&quot; href=&quot;#phan_hönig_ayanian_2018&quot;&gt;(Phan et al., 2018)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;hönig_ayanian_2020&quot;&gt;&lt;/span&gt; and &lt;a class=&quot;citation&quot; href=&quot;#hönig_ayanian_2020&quot;&gt;(Hönig &amp;amp; Ayanian, 2020)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;phan_hönig_ayanian_2018&quot;&gt;&lt;/span&gt;, thus motivates the use of a hardware abstraction layer on top of the Crazyflie. This abstraction layer, in the form of a ROS layer, is only used on the PC controlling one or more Crazyflies. The ROS driver sends the data to the different quadcopters using the protocol defined in the Crazyflie firmware.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#phan_hönig_ayanian_2018&quot;&gt;(Phan et al., 2018)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;hönig_ayanian_2020&quot;&gt;&lt;/span&gt; &lt;em&gt;&lt;/em&gt; demonstrates interoperability between the PC and UAV components. The &lt;em&gt;crazyflie_ros&lt;/em&gt; framework helps wrap CRTP within a ROS framework, which is useful for scenarios of hovering and waypoint following from a single robot to the more complex multi-UAV case. It provides not only standard operating system services (hardware abstraction, contention management, process management), but also high-level functionalities (asynchronous and synchronous calls, centralised database, a robot configuration system, etc.). Additionally, this includes command-line tools and a GUI for mass rebooting, firmware updates, firmware version query, and battery voltage checks over the radio.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/controlloop_bw.png&quot; style=&quot;width:6cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Overview of the Crazyswarm Control Loop, as per the Crazyswarm official documentation (August 2021) &lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#hönig_ayanian_2020&quot;&gt;(Hönig &amp;amp; Ayanian, 2020)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;preiss_hönig_sukhatme_ayanian_2017&quot;&gt;&lt;/span&gt; &lt;em&gt;&lt;/em&gt; furthers this work by offering all the necessary components for controlling multiple drones remotely, by relating the drone flight controller of the Crazyflie to a set of controllers on the PC, but also by offering ways to send trajectories to the drones in realtime. Crazyswarm attempts to couple an external motion capture technology like Optitrack with the rest of a drone’s control loop: knowing its position, the drone will be able to generate and follow a trajectory more precisely. When viewing a single body, motion capture certainly has sub-millimeter accuracy. However, as the number of drones increases, there are two limiting factors to the reliability of the control loop: the first is recognition of the drones by the optical capture system, and the second is low communication bandwidth. Multiple algorithms are therefore incorporated into this framework to mitigate the effects of these processes.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#preiss_hönig_sukhatme_ayanian_2017&quot;&gt;(Preiss et al., 2017)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;chaari_cheikhrouhou_koubâa_youssef_hmam_2021&quot;&gt;&lt;/span&gt; &lt;em&gt;&lt;/em&gt; design a distributed cloud robotic architecture for computation offloading based on Kafka middleware as messaging broker. Empowering robots with cloud computing comes with a fundamental tradeoff. Offloading the execution of a computationally intensive algorithm to the cloud can reduce resource utilization, including CPU, memory, and the battery. However, this comes with a cost: communicating with cloud resources over a congested network increases latency and can lead to delay for real-time applications. &lt;em&gt;&lt;/em&gt; showcase that an offloading decision need not reduce the overall execution time of the application.&lt;/p&gt;
&lt;h2 id=&quot;system-overview&quot;&gt;System Overview&lt;/h2&gt;
&lt;h3 id=&quot;functionality&quot;&gt;Functionality&lt;/h3&gt;
&lt;p&gt;The testbed is designed according to four functional requirements.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Managing the interface with drone firmware.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Localizing the drones in a Flight Arena.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Rendering the drones in a simulated environment.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Managing the flow of offboard code for each drone.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These elements occur separately and simultaneously. They manage individual drones asynchronously from one another, a key element in swarm engineering. Each of these requirements is fulfilled respectively by Crazyswarm, Optitrack, Unity and the Task Manager. Each of these are explored in turn in this chaper.&lt;/p&gt;
&lt;h3 id=&quot;section:network&quot;&gt;Network Architecture&lt;/h3&gt;
&lt;p&gt;Figure &lt;a href=&quot;#fig:network_interfaces&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:network_interfaces&quot;&gt;[fig:network_interfaces]&lt;/a&gt; gives a brief overview of the data interfaces between the four main components of this architecture.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/testbed_arch/ros_pin.png&quot; style=&quot;width:10cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Overview of Network Interfaces.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;The data flow in the Flight Arena is as follows: vehicle/object pose measurements are provided by a motion capture system to software modules running on companion computers running consumer operating systems. Within task-specific modules (&quot;user code&quot;) and the Crazyflie communication channels, estimation and control pipelines produce vehicle motion commands. The appropriate commands are transmitted to the vehicles. Onboard the vehicles, high-frequency controllers track these commands using on-board inertial sensors in feedback. All intermodule communication is via multicast UDP and the vehicles commands are sent over a dedicated wireless channel.&lt;/p&gt;
&lt;h3 id=&quot;chapter-structure&quot;&gt;Chapter Structure&lt;/h3&gt;
&lt;p&gt;The drone testbed is comprised of a hardware and a software environment. First, Section &lt;a href=&quot;#section:network&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;section:network&quot;&gt;1.3.2&lt;/a&gt; presents the Network Interfaces. The Hardware Environment consists of the physical Flight Arena. Section &lt;a href=&quot;#section:hardware&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;section:hardware&quot;&gt;1.4&lt;/a&gt; presents this Arena, the drone model and the localization system. Section &lt;a href=&quot;#section:software&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;section:software&quot;&gt;1.5&lt;/a&gt; then touches on swarm management followed by task management.&lt;/p&gt;

&lt;h2 id=&quot;section:software&quot;&gt;Software Environment&lt;/h2&gt;
&lt;h3 id=&quot;modules-involved-in-software-environment&quot;&gt;Modules involved in Software Environment&lt;/h3&gt;
&lt;p&gt;This section is a brief mention of all the platforms, systems, services, and processes the software environment would depend on.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Motive &lt;a class=&quot;citation&quot; href=&quot;#experimental_tuning&quot;&gt;(Waliszkiewicz et al., 2020)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;optitrack&quot;&gt;&lt;/span&gt; processes OptiTrack camera data to deliver global 3D positions, marker IDs and rotational data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Crazyswarm &lt;a class=&quot;citation&quot; href=&quot;#optitrack&quot;&gt;(Point, n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;preiss_hönig_sukhatme_ayanian_2017&quot;&gt;&lt;/span&gt; is an swarm management layer that allows multi-drone flight of Bitcraze Crazyflie drones in tight, synchronized formations,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ROS &lt;a class=&quot;citation&quot; href=&quot;#preiss_hönig_sukhatme_ayanian_2017&quot;&gt;(Preiss et al., 2017)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;ros&quot;&gt;&lt;/span&gt; is a set of software libraries and tools that assist in building robot applications.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SMACH &lt;a class=&quot;citation&quot; href=&quot;#ros&quot;&gt;(Stanford Artificial Intelligence Laboratory et al., 2018)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;smach&quot;&gt;&lt;/span&gt; is a task-level architecture for rapidly creating complex robot behavior and integrating ROS utilities,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Unity &lt;a class=&quot;citation&quot; href=&quot;#smach&quot;&gt;(et al., n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;unity3d&quot;&gt;&lt;/span&gt; is a cross-platform game engine used in a range of mixed reality research &lt;a class=&quot;citation&quot; href=&quot;#unity3d&quot;&gt;(Nicholas Francis &amp;amp; Helgason, 2018)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;mixed_reality_robotics&quot;&gt;&lt;/span&gt;&lt;a class=&quot;citation&quot; href=&quot;#mixed_reality_robotics&quot;&gt;(Hönig et al., 2015)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;phan_hönig_ayanian_2018&quot;&gt;&lt;/span&gt;&lt;a class=&quot;citation&quot; href=&quot;#phan_hönig_ayanian_2018&quot;&gt;(Phan et al., 2018)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;rossharp&quot;&gt;&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the sake of replicability, the version of each module is documented in the references.&lt;/p&gt;
&lt;h3 id=&quot;operating-systems&quot;&gt;Operating Systems&lt;/h3&gt;
&lt;p&gt;We adopt a distributed systems approach, whereas various components are spread across multiple computers on a network. These devices split up the work, coordinating their efforts to complete the job more efficiently than if a single device had been responsible for the task. This section is a brief description of relationships between the modules and system features. Figure &lt;a href=&quot;#fig:OS_diagram&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:OS_diagram&quot;&gt;[fig:OS_diagram]&lt;/a&gt; encapsulates the software modules into their respective operating systems, Ubuntu and Windows.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/testbed_arch/network_os.png&quot; style=&quot;width:11cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;The Motion Capture Table, net and Flex 13 cameras positioned above the platform.  &lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;Each OS accommodates compatible software technologies used in this architecture. Optitrack and Unity have been developed for Windows systems. A Windows 10 OS is loaded on a standalone PC. On the other hand, ROS have been developed for Ubuntu systems. An Ubuntu 18.04 OS is loaded on a standalone PC. The interface between the two is managed by the ROS middleware, which is expanded upon in Section &lt;a href=&quot;#section:middleware&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;section:middleware&quot;&gt;1.5.3&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;section:middleware&quot;&gt;Middleware Solution&lt;/h3&gt;
&lt;p&gt;In a middleware &lt;a class=&quot;citation&quot; href=&quot;#rossharp&quot;&gt;(Bischoff, 2019)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;ros_docs&quot;&gt;&lt;/span&gt;, modules do not need to be linked within a single process, and this instead can be separated into the following elements.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Package management&lt;/strong&gt;: drivers and other algorithms can be contained in standalone executables,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hardware abstraction&lt;/strong&gt;: in software, this refers to a sets of routines that provide programs with access to hardware resources through programming interfaces. This is explored in Section &lt;a href=&quot;#section:SPI&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;section:SPI&quot;&gt;1.6&lt;/a&gt;: Swarm Programming Interface.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Low-level device control&lt;/strong&gt;: the ROS interface serves as a communication layer with onboard devices such as motors and the battery sensor,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Message exchange between processes&lt;/strong&gt;: inter-process communications allows to pass data between modules, such as data from drone poses shown in Figure &lt;a href=&quot;#fig:ROS_in_system&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:ROS_in_system&quot;&gt;[fig:ROS_in_system]&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Managing robotics-related functionalities&lt;/strong&gt;: handling the concurrent activity of multiple robots via a global parameter manager and a global task manager.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main objective for this system’s Middleware Solution is a more flexible, more reconfigurable and generally modular layout. This proves useful in a development and demonstration environment that requires many critical moving parts. This system’s network interface is shown in Figure &lt;a href=&quot;#fig:ROS_in_system&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:ROS_in_system&quot;&gt;[fig:ROS_in_system]&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/testbed_arch/ros_pin.png&quot; style=&quot;width:12cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Protective cork layer.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;ROS provides a central role of &lt;strong&gt;resource management&lt;/strong&gt;, from managing various interfaces in the system implementation to further hardware abstractions.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/testbed_arch/optitrack_pin.png&quot; style=&quot;width:5.8cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Lightray simulation on the Table for 45° angle from the horizonta&lt;/figcaption&gt;

&lt;/div&gt;
&lt;h3 id=&quot;section:virtualisation&quot;&gt;Virtualisation of Physical Objects&lt;/h3&gt;
&lt;p&gt;Once localized by the motion capture setup, pose data is transferred to the middleware layer. The pose data of physical objects, including the drones, becomes available in real-time to a range of companion software, via this ROS middleware layer.&lt;/p&gt;
&lt;h3 id=&quot;section:swarm&quot;&gt;Swarm Management Layer&lt;/h3&gt;
&lt;p&gt;The Crazyswarm framework &lt;a class=&quot;citation&quot; href=&quot;#ros_docs&quot;&gt;(“ROS Documentation (Accessed 30 September 2021),” 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;crazyswarm_docs&quot;&gt;&lt;/span&gt; is adopted as an control layer for the Crazyflie drone. The main advantages of the Crazyswarm over other frameworks are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Motion capture integration&lt;/strong&gt;. Crazyswarm contains drivers for the Optitrack System. In contrast, the Crazyflie proprietary API can send position measurements to the Crazyflie, but does not know how to get position measurements from mocap hardware.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Python firmware bindings&lt;/strong&gt;. Crazyswarm’s simulator is built upon automatically generated Python bindings for certain modules in the Crazyflie firmware. The binding system can be helpful when developing new firmware modules, especially when they are mathematically complex and hard to debug.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;ROS foundation&lt;/strong&gt;. The Crazyswarm server program is a ROS node. The Python API Reference is a thin wrapper around the ROS interface. The ROS interface is explored in this section.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/testbed_arch/virtual_pin.png&quot; style=&quot;width:5.8cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Lightray simulation on the Table for 45° angle from the horizonta&lt;/figcaption&gt;

&lt;/div&gt;
&lt;h3 id=&quot;simulation-environment-layer&quot;&gt;Simulation Environment Layer&lt;/h3&gt;
&lt;p&gt;The first objective of the simulated environment is to serve as a graphical interface in order to develop tasks otherwise too difficult to deploy. The priority of the virtual reality is therefore set on rendering capabilities, and the ability to obtain camera streams from this environment. The robotics backend, described in the previous elements, can interact with the Unity3D game engine.&lt;/p&gt;
&lt;p&gt;As shown in Figure &lt;a href=&quot;#fig:virtual_pin&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:virtual_pin&quot;&gt;[fig:virtual_pin]&lt;/a&gt;, ROS has a steady stream of poses from the physical drones, allowing for virtual visualisation. Key events and data can be exchanged between ROS and Unity3D. The way this is achieved is examined in Section &lt;a href=&quot;#section:xreality&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;section:xreality&quot;&gt;[section:xreality]&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;task-management-layer&quot;&gt;Task Management Layer&lt;/h3&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/testbed_arch/taskmanager_pin.png&quot; style=&quot;width:5.8cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Modelling the Coverage Volume&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;A Task Manager assists in the scheduling of flight tasks relative to one another. The task manager has multiple responsibilities in this framework.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, it loads the description of all tasks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It then provides a service to start or stop a given task,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It keeps track of the status of all tasks currently running or recently terminated.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It is also responsible for instantiating the task scheduler that manages the threads in which tasks actually run.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This manager is implemented with a Client-Server communication as seen in Figure &lt;a href=&quot;#fig:client-server&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:client-server&quot;&gt;[fig:client-server]&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/testbed_arch/one_state_server.png&quot; style=&quot;width:10cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Modelling the Coverage Volume&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;The Client directly tracks the state of each process in a larger decision process. The Action Server interacts with automated functionality, and the Flight Server interacts with the robot instruction stream. The building blocks of this approach are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A Client State Machine&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Client-Server Messages&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Server Handling of Actions&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A Distributed Parameter Handler&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Scaling to Multiple Drones&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&quot;a-client-state-machine&quot;&gt;1 | A Client State Machine&lt;/h4&gt;
&lt;p&gt;The client requires a decision-maker between each state and a set of possible future states. A state machine is chosen to coordinate the transition between different usecases. For this, the SMACH library is used &lt;a class=&quot;citation&quot; href=&quot;#crazyswarm_docs&quot;&gt;(&lt;i&gt;Crazyswarm Documentation (Accessed 30 July 2021)&lt;/i&gt;, 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;smach&quot;&gt;&lt;/span&gt;. Task handling is implemented with several scheduling elements.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Concurrency&lt;/strong&gt;: the ability for a program to be decomposed into parts that can run independently from each other. This means that tasks can be executed out of order and the result would still be the same as if they are executed in order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Preemption&lt;/strong&gt;: the act of temporarily interrupting an executing task, with the intention of resuming it at a later time. This interrupt is done by an external scheduler with no assistance or cooperation from the task.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Interruption&lt;/strong&gt;: a process tells the task manager to stop running the current program so that a new one can be started.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;client-server-messages&quot;&gt;2 | Client-Server Messages&lt;/h4&gt;
&lt;p&gt;A message transmits data values during client-server communication. ROS uses a simplified messages description language &lt;a class=&quot;citation&quot; href=&quot;#smach&quot;&gt;(et al., n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;ros_docs&quot;&gt;&lt;/span&gt; for describing the data values (aka messages) that ROS nodes publish. This description makes it easy for ROS tools to automatically generate source code for the message type in several target languages.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/microservice/ros_msg_structure.png&quot; style=&quot;width:11cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Lightray simulation on the Table for 45° angle from the horizontal&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;In an Action Client-Server interaction, communication is ensured ROS Messages with three distinct roles: the goal, the feedback and the result &lt;a class=&quot;citation&quot; href=&quot;#ros_docs&quot;&gt;(“ROS Documentation (Accessed 30 September 2021),” 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;ros_docs&quot;&gt;&lt;/span&gt;. An action is executed when the goal requests an action with a set of parameters to the server. Feedback parameters can selected for monitoring during the action’s execution. The result informs any concurrent threads of the final state of the action.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/testbed_arch/one_state_server.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Lightray simulation for Intersection of any 2 lightrays.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;h4 id=&quot;server-handling-of-actions&quot;&gt;3 | Server Handling of Actions&lt;/h4&gt;
&lt;p&gt;The Action Server executes an action in the form of a callback functions. A task completes when a particular condition is met. In order to manage this, an action callback can incorporate a condition in its execution. The next section examines this in more depth.&lt;/p&gt;
&lt;p&gt;Each of these pre-loaded behaviours needs to be scheduled. For this, an open-source Function Handler is used, referred to as the ROS Action Server &lt;a class=&quot;citation&quot; href=&quot;#ros_docs&quot;&gt;(“ROS Documentation (Accessed 30 September 2021),” 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;ros_docs&quot;&gt;&lt;/span&gt;.As a result, each function for a specific task server will be launched from a central &lt;strong&gt;launchfile&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;div class=&quot;minted&quot;&gt;
&lt;p&gt;&lt;span&gt;html&lt;/span&gt; &amp;lt;launch&amp;gt; &amp;lt;group&amp;gt; &amp;lt;remap from=’_goTo’ to=’drone1_goTo’/&amp;gt; &amp;lt;node name=’drone1’ pkg=’crazyswarm’ type=’ros_action_server.py’&amp;gt; &amp;lt;/node&amp;gt; &amp;lt;/group&amp;gt;&lt;/p&gt;
&lt;p&gt;&amp;lt;group&amp;gt; &amp;lt;remap from=’_goTo’ to=’drone2_goTo’/&amp;gt; &amp;lt;node name=’drone2’ pkg=’crazyswarm’ type=’ros_action_server.py’&amp;gt; &amp;lt;/node&amp;gt; &amp;lt;/group&amp;gt; &amp;lt;/launch&amp;gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This ROS launchfile loads the functions declared in the Action Server, and remaps them to each drone in the choreography. This allocates a thread under the form of a ROS node. These ROS nodes act as separate Request/Response instances.&lt;/p&gt;
&lt;h4 id=&quot;parameter_handler&quot;&gt;4 | ROS Parameter Handler&lt;/h4&gt;
&lt;p&gt;The ROS main thread includes a commonly-used component called the Parameter Server, implemented in the form of XMLRPC, and which is, as the name implies, a centralised database within which nodes can store data and, in so doing, share system-wide parameters.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;div class=&quot;minted&quot;&gt;
&lt;p&gt;&lt;span&gt;python&lt;/span&gt; crazyflies: - channel: 35 id: 1 initialPosition: [0.0, 0.0, 0.0] type: default - channel: 27 id: 2 initialPosition: [1.0, 0.0, 0.0] type: default - channel: 27 id: 5 initialPosition: [4.0, 0.0, 0.0] type: default&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Multiple programs query this file upon initialization of the swarm management layer. Each robot is distinguished by their channel, id and initialPosition. This allows for identifying drone ids and other unique information.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/testbed_arch/virtual_pin.png&quot; style=&quot;width:5.8cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Lightray simulation for Intersection of any 2 lightrays.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;h4 id=&quot;scaling-to-multiple-drones&quot;&gt;5 | Scaling to Multiple Drones&lt;/h4&gt;
&lt;p&gt;Similarly to &lt;a class=&quot;citation&quot; href=&quot;#ros_docs&quot;&gt;(“ROS Documentation (Accessed 30 September 2021),” 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;pradalier_2020&quot;&gt;&lt;/span&gt;, a procedural task-based programming approach is adopted. This can be likened to a centralized server that services multiple drones. Figure &lt;a href=&quot;#fig:task_management_architecture&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:task_management_architecture&quot;&gt;[fig:task_management_architecture]&lt;/a&gt; shows the full Task Management Layer Architecture.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/testbed_arch/task_man_architecture.png&quot; style=&quot;width:11cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Results of Coverage Optimisation Study.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;This approach values granularity, being lightweight, and the ability to share similar processes across multiple apps. As a result, it is therefore highly reusable for new tasks.&lt;/p&gt;
&lt;h2 id=&quot;section:SPI&quot;&gt;High Level Interface&lt;/h2&gt;
&lt;p&gt;A high level interface is an abstraction layer for development activities. In order to simplify task development, and align with the thesis goals, we develop a framework for high level interaction between the operator and the functionalities of the testbed.&lt;/p&gt;
&lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;The Testbed, as described in Section. For such purposes, it is required to test user code. Therefore an interface is a key element for the user. There are several advantages to specialised tasks for the testbed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Handling sub-tasks&lt;/strong&gt; to various levels of depth: microservices help automate sub-tasks at a desired complexity. When encapsulated in this way, are separate modules fit for demonstration, that can later be optimised and refined during development.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Monitoring the swarm&lt;/strong&gt;: a central monitoring system can run in parallel with the particular algorithms that are tested and validated. For instance, a battery voltage threshold helps to monitor a correct running of the hardware.This allows for preventive maintenance during demonstrations but also during development.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, more ‘complex tasks’ will allow for the automation of separate subtasks in a controlled manner.&lt;/p&gt;
&lt;h3 id=&quot;conceptual-overview&quot;&gt;Conceptual Overview&lt;/h3&gt;
&lt;p&gt;The high-level interface combines three major elements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the management of low-level devices upon each robot,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;communication with the swarm, and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;scheduling of instructions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to achieve this, the intermediary structures for tasks are laid out here. Figure &lt;a href=&quot;#fig:loop&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:loop&quot;&gt;[fig:loop]&lt;/a&gt; labels a hierarchy of tasks. The drone is instructed to alternate between two waypoints until a software condition is triggered.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/SPI/concept_spi_bw.png&quot; style=&quot;width:10cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Determining the intersection area of lightrays&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;This example serves to illustrate the conceptualisation of a subtask and a multi-step task. The concurrence of waypoints and the software trigger is consider a sub task, and within a state machine, which is referred to as a multi-step task. This framework offers a definition for complex tasks, as tasks that coordinate the scheduling of instructions, with multi-robot instructions.&lt;/p&gt;
&lt;h3 id=&quot;architectural-approach&quot;&gt;Architectural Approach&lt;/h3&gt;
&lt;p&gt;To create these complex tasks, this high level interface has the following architectural choices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Encapsulating robot instructions&lt;/strong&gt; Robot commands are assimilated into this programming interface as individual tasks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Encapsulating swarm instructions&lt;/strong&gt; Robot instructions are included in generic functions as swarm instructions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Encapsulating sub tasks&lt;/strong&gt; Scheduling processes such as a concurrence runs in a function encapsulating it.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This architecture is written in Python, known for its ease of use and flexibility. In this way, multi-step tasks manage the swarm stack, from executing single-robot commands to ensuring the dynamic management of swarms.&lt;/p&gt;
&lt;h4 id=&quot;robot-instructions&quot;&gt;Robot instructions&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/SPI/08.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Overview of the Crazyswarm Control Loop, as per the Crazyswarm official documentation (August 2021) &lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;The Crazyswarm API from Section &lt;a href=&quot;#section:swarm&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;section:swarm&quot;&gt;1.5.5&lt;/a&gt; interfaces with low-level hardware for landing, takeoff and further behaviours that can be coded remotely. However, for the purpose of &lt;strong&gt;centralised task management&lt;/strong&gt;, the execution of each instruction should be monitored accordingly. &lt;a class=&quot;citation&quot; href=&quot;#pradalier_2020&quot;&gt;(Pradalier, 2020)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;hönig_ayanian_2020&quot;&gt;&lt;/span&gt; offer ROS telemetry tools, such as battery monitoring and a reset utility. These can be used as conditions in the execution process. Ultimately, an additional layer of abstraction is required for multi-drone instructions.&lt;/p&gt;
&lt;h4 id=&quot;multi-robot-instructions&quot;&gt;Multi-robot instructions&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/SPI/06.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;View of the Flight Arena during the 2 Drone Hover experiment.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;This section outlines the functions developed &lt;strong&gt;for group behaviours&lt;/strong&gt;: concurrent takeoffs and landings, querying multiple drones for low battery level, etc. Fly-Octogon and Land-all are examples of multi-robot instructions.&lt;/p&gt;
&lt;p&gt;This microservice model is a major component of optimizing swarm programming towards the multipurpose task model outlined in the objectives.&lt;/p&gt;
&lt;h4 id=&quot;sub-task-management&quot;&gt;Sub task management&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/SPI/traj_shape_simultaneous.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Hover Experiment: Stability Tests on Flight Arena.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;The objective for sub tasks is to assist in creating, and coordinating, &lt;strong&gt;higher-level behaviours&lt;/strong&gt;. An example of this is the concurrent_traj module, whereas two drones are told to fly simultaneous trajectories. This is of particular interest as two drones will take &lt;strong&gt;indeterminate amounts of time&lt;/strong&gt; to respond to commands. &lt;strong&gt;Concurrency&lt;/strong&gt; — in the context of programming — is the ability for a program to be decomposed into parts that can run independently of each other.&lt;/p&gt;
&lt;h4 id=&quot;multi-step-tasks&quot;&gt;Multi-step tasks&lt;/h4&gt;
&lt;p&gt;A decision process combines the various modules developed above into a sequence of tasks. This is achieved with a Finite State Machine, which is implemented programmatically with the SMACH python library &lt;a class=&quot;citation&quot; href=&quot;#hönig_ayanian_2020&quot;&gt;(Hönig &amp;amp; Ayanian, 2020)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;smach&quot;&gt;&lt;/span&gt;. A choreographic state machine is implemented in section &lt;a href=&quot;#section:choreography&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;section:choreography&quot;&gt;1.7&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;section:choreography&quot;&gt;Testbed Demonstration&lt;/h2&gt;
&lt;p&gt;A drone choreography is designed as a live demonstration of the Testbed’s functionality. The experiment data is accessible publicly &lt;a class=&quot;citation&quot; href=&quot;#smach&quot;&gt;(et al., n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;choreography_data&quot;&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;choreography-design&quot;&gt;Choreography Design&lt;/h3&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/SPI/02.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Hover Experiment: Stability Tests on Flight Arena.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;The State Machine for the full choreography is available in Figure &lt;a href=&quot;#diagram:fsm&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;diagram:fsm&quot;&gt;[diagram:fsm]&lt;/a&gt;. This demonstration includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Takeoff and landing, separately and concurrently.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A pre-loaded trajectory, from a Bezier curve: concurrently.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A polygonial shape flown by two drones, demonstrating simultaneous movement through a set of waypoints.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Autonomous state changes.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This state machine functions any number of drones: using the swarm building blocks developed in Section &lt;a href=&quot;#section:SPI&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;section:SPI&quot;&gt;1.6&lt;/a&gt;, the dronesexecute trajectories simultaneously; it then moves to certain waypoints indefinitely. In this case a figure of 8 is executed on both drones followed by an octogon. Finally, upon an operator signal, the drones land. The state machine is such that the drones also land if one does not reach its corresponding waypoint in time.&lt;/p&gt;
&lt;h4 id=&quot;pre-loaded-trajectory&quot;&gt;1 | Pre-loaded Trajectory&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/SPI/traj_shape_simultaneous.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;3D Plot of 2 Drone Hover.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;Two Figures of 8 are flown simultaneously. The figures of 8 are concurrent Bezier shapes, pre-loaded onboard each drone’s trajectory follower &lt;a class=&quot;citation&quot; href=&quot;#choreography_data&quot;&gt;(Carstens &amp;amp; Dufaure, n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;crazyswarm_docs&quot;&gt;&lt;/span&gt;. This is coded using the high level interface as in Fig &lt;a href=&quot;#code:exec_Fo8&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;code:exec_Fo8&quot;&gt;[code:exec_Fo8]&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;div class=&quot;minted&quot;&gt;
&lt;p&gt;&lt;span&gt;python&lt;/span&gt; fig8_sm = concurrent_trajs(selected_drones = ids, traj_id = 8) StateMachine.add(’FIG8_EXECUTE’, fig8_sm, transitions=&lt;span&gt;’succeeded’ : ’NEXT_STATE’, ’aborted’ : ’land_all’, ’preempted’ : ’land_all’&lt;/span&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The Figure of 8 is assigned an id of 8. Other trajectories are assigned other ids. The concurrent_trajs function is thus called upon with the required drones and their required ids.&lt;/p&gt;
&lt;h4 id=&quot;multi-point-trajectory.&quot;&gt;2 | Multi-point Trajectory.&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/SPI/03.png&quot; style=&quot;width:3cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Stability Comparison of two Drones.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;This state loads a custom trajectory on the drone, which is executed, before moving to an indefinite octogonal trajectory. The use of &lt;strong&gt;waypoint following&lt;/strong&gt; is an automation of the motion to specific points.&lt;/p&gt;
&lt;h4 id=&quot;topic-monitor&quot;&gt;3 | Topic Monitor&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/SPI/05.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Hover Experiment: Pitch, Yaw and Roll of the two drones.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;The use of a &lt;strong&gt;Topic Monitor&lt;/strong&gt; is useful to interface with active topics. For instance, at any one moment that a drone gets too close to a particular point, it initiates a landing. The intended behaviour is represented visually alongside.&lt;/p&gt;
&lt;p&gt;This is another such subtask that fulfils the initial goal: monitoring the swarm with preventive measures during demonstration as well as training phases. This is performed programmatically with a concurrence between a drone and the /collision topic.&lt;/p&gt;
&lt;h4 id=&quot;choreography-state-machine&quot;&gt;4 | Choreography State Machine&lt;/h4&gt;
&lt;p&gt;The previous sections are integrated into a State Machine. The configuration of the state machine is displayed in Figure &lt;a href=&quot;#fig:fsm&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:fsm&quot;&gt;[fig:fsm]&lt;/a&gt;. Individual tasks are coloured in green and swarm tasks in orange.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/SPI/02.png&quot; style=&quot;width:10cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Network Interfaces Encapsulated in Operating Systems. &lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;Three drones are positioned about the Flight Arena as in Figure &lt;a href=&quot;#fig:chore_initialisation&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:chore_initialisation&quot;&gt;[fig:chore_initialisation]&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&quot;choreography-execution&quot;&gt;5 | Choreography Execution&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/chore_pictures/octogon/pic.png&quot; style=&quot;width:4cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Network interfaces with ROS.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;The state machine is run on a separate thread as in Figure &lt;a href=&quot;#code:exec_choreography&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;code:exec_choreography&quot;&gt;[code:exec_choreography]&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;myswarm = swarmInterface(all_ids = [1,3,5])
sm0 = myswarm.execTrajandOctogon (ids = [1,3], traj_shape = 8)
myswarm.start_sm_on_thread(sm0)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This invocation of the state machine clearly shows drone ids [1,3,5] as extracted from the Parameter Server, in order to act as the drones 1,2,3. The trajectory shape 8 refers to the Figure of 8.&lt;/p&gt;
&lt;h3 id=&quot;results-1&quot;&gt;Results&lt;/h3&gt;
&lt;p&gt;We proceed with an inspection of the demonstration. The flightpaths of all three drones are plotted together.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/choreography/trajectory_choreography2.png&quot; style=&quot;width:13cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Swarm solution interactions with System Architecture&lt;/figcaption&gt;

&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/chore_pictures/octogon/3d2.png&quot; style=&quot;width:6cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Crazyswarm within the Network Implementation.&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;Overall, the flightpaths are smooth. The Figures of 8 are traced distinctly, as well as the two octogons. The figure of 8 of drone 3 is discontinuous, and yet there is no apparent effect on the shape. This suggests that the drone moved beyond the area localized by motion capture. When examining the octogons, the top view shows a near perfect superposition: showing small differences in position of less than 2cm. Finally, a line connects the two shapes. This shows that these figures did actually occur in sequence.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/chore_pictures/octogon/front2.png&quot; style=&quot;width:5.5cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/chore_pictures/octogon/top2.png&quot; style=&quot;width:5.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Further inspection of the octogons requires a topview and a sideview. The Octogon is traced very clearly. Near the end of the experiment, there is a noticeable wobble in the blue line. This behaviour is due to a low battery level. A notable difference is the wobble in the xz plane, which demonstrates a loss of precision as the drones get closer to the ground. There is a symmetrical behaviour. Further tests can determine what this is attributed to.&lt;/p&gt;
&lt;div class=&quot;table*&quot;&gt;
&lt;div class=&quot;flushleft&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Test Description&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;Value&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Volume of Flight Arena Localized by Motion Capture&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Maximum Flight Error Recorded in Hover Test&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/discussion/rosbuzz_comparison.png&quot; style=&quot;width:5.5cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Simulation environment interactions with System Architecture&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;In this chapter, a swarm programming approach is developed along similar lines to &lt;a class=&quot;citation&quot; href=&quot;#crazyswarm_docs&quot;&gt;(&lt;i&gt;Crazyswarm Documentation (Accessed 30 July 2021)&lt;/i&gt;, 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;pinciroli_lee-brown_beltrame_2015&quot;&gt;&lt;/span&gt;, the swarm API that was developed for swarms of nanodrones. Both frameworks load a set of functions, they allow the user to select which drones perform a certain task, and group drones according to the task at hand. While Buzz manages membership with a dedicated hash table, our interface makes use of a global parameter handler &lt;a class=&quot;citation&quot; href=&quot;#pinciroli_lee-brown_beltrame_2015&quot;&gt;(Pinciroli et al., 2015)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;buzz_swarm_stack&quot;&gt;&lt;/span&gt;. Both architecture allows for the development different modules can be developed independently and related dynamically.&lt;/p&gt;
&lt;p&gt;With a high-level interface, this work concerns itself with a swarm-specific language that is not &quot;too top-down&quot; or &quot;too bottom-up&quot;. This distinction is seen with increasing swarm sizes, and for creating user tasks more focused on development, or on improving safety and phone interference for demonstrations.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;

&lt;img src=&quot;../../assets/images/testbed/discussion/copilot_structure_bw.jpg&quot; style=&quot;width:7cm&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;Task Manager interactions with System Architecture&lt;/figcaption&gt;

&lt;/div&gt;
&lt;p&gt;Figure &lt;a href=&quot;#diagram:swarm_stack&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;diagram:swarm_stack&quot;&gt;[diagram:swarm_stack]&lt;/a&gt; demonstrates that Buzz uses comparable structures for swarm engineering.). ROS communicates with device hardware via MAVROS, a ROS library for compatible Micro Aerial Vehicules. It then has a control distribution layer that is comparable to our Task Manager, a swarm communication layer like Crazyswarm and a swarm control layer like our high level interface. The swarm stack in other research may be composed of other technologies, but retains this structure.&lt;/p&gt;
&lt;p&gt;The Flying Machine Arena &lt;a class=&quot;citation&quot; href=&quot;#buzz_swarm_stack&quot;&gt;(Varadharajan et al., 2017)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;fma_paper&quot;&gt;&lt;/span&gt; is an active area of research for drone development and demonstration, and their ’Copilot’ is described as a flight monitoring solution. Figure &lt;a href=&quot;#diagram:copilot&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;diagram:copilot&quot;&gt;[diagram:copilot]&lt;/a&gt; demonstrates the types of activities achieved via the copilot: updating drone poses during code execution (a), executing playback on recorded poses (b), and executing procedures in simulation (c). These elements are handled by the Task Manager described in this chapter, demonstrating the pertinence of a flight management solution.&lt;/p&gt;
&lt;p&gt;The procedural task-based approach of this chapter is not unique: it figures in &lt;a class=&quot;citation&quot; href=&quot;#fma_paper&quot;&gt;(Lupashin et al., 2014)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;pradalier_2020&quot;&gt;&lt;/span&gt; who develops a generic pythonic form that need not depend on middleware for task management. All in all, this approach can aid in development, in ways that can be outlined here.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;preventing mechanical failure upon software failure.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;assist in creating, and coordinating, higher-level behaviours.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;monitor the state of every UAV asynchronously.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Assist in troubleshooting with a modular layout.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;This flight has demonstrated multiple working functionalities. The first is the use of the Swarm Programming Interface. Using the building blocks developed in this chapter, it is possible to develop a multi-stage process, one that includes preloaded trajectories as well as waypoint trajectories, choreographic positioning, and escape cases upon a system abort. With such tools for assistance during development, this set of functionalities pushes beyond previous work, as it offers a layer beyond the crazyswarm’s robot instruction set.&lt;/p&gt;

&lt;h2 id=&quot;chapter-summary&quot;&gt;Chapter Summary&lt;/h2&gt;
&lt;p&gt;The proposed state-based architecture is a first step towards creating UAV operations to perform complex tasks, collaboratively or otherwise. After all, this framework has put in place the monitoring tools and the task-based framework to execute complex behaviours; and beyond that, putting in place a Flight Arena has already helped to validated these tools. Such services can easily be tested and deployed from a framework like this one.&lt;/p&gt;
&lt;p&gt;Certain elements in this framework are taken a step further in Chapter &lt;a href=&quot;#c2&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;c2&quot;&gt;[c2]&lt;/a&gt;: the ability to send streaming setpoints to a drone opens the possibility of flight piloting through other means. This would not be possible without the foundation established in this chapter: the drone architecture, the motion capture setup and the swarm framework.&lt;/p&gt;
&lt;p&gt;The tools that were established here may need to be challenged by further research. One direction is the decentralisation of agents with respect to the platform: where this framework has a central role in allocating behaviours, one would opt for a framework that gives each agent the ability to act independently. However, the central monitoring can remain a major asset when developing such a swarm, as it serves as a safety recourse to prevent any hardware damage.&lt;/p&gt;
&lt;p&gt;The testbed makes great use of distributed networking, and it aligns with the first approach of the thesis for task creation. From handling specific parameters, to managing the task execution and scheduling in a centralised manner, the middleware monitors the different agents in an asynchronous manner. As opposed to non-distributed systems, such as direct one-to-one links to onboard devices, it allows the developer to divert their focus from system communication to performance-cri&lt;a class=&quot;citation&quot; href=&quot;#pradalier_2020&quot;&gt;(Pradalier, 2020)&lt;/a&gt;tical applications.&lt;/p&gt;
&lt;p&gt;data-cites=”&lt;/p&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;swarm_review&quot;&gt;Navarro, I., &amp;amp; Matía, F. (2012). An Introduction to Swarm Robotics. In &lt;i&gt;Hindawi&lt;/i&gt;. https://www.hindawi.com/journals/isrn/2013/608164/&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;swarm_engineering&quot;&gt;Brambilla, M., Ferrante, E., Birattari, M., &amp;amp; Dorigo, M. (2013). Swarm Robotics: A Review from the Swarm Engineering Perspective. &lt;i&gt;Swarm Intelligence&lt;/i&gt;, &lt;i&gt;7&lt;/i&gt;, 1–41. https://doi.org/10.1007/s11721-012-0075-2&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;gcs_validation&quot;&gt;Larrabee, T., Chao, H., Kumar, T., Gururajan, S., Gu, Y., &amp;amp; Napolitano, M. (2013). Design, simulation, and flight test validation of a UAV ground control station for Aviation safety research and PILOT MODELING. In &lt;i&gt;Design, Simulation, and Flight Test Validation of a UAV Ground Control Station for Aviation Safety Research and Pilot Modeling | Guidance, Navigation, and Control and Co-located Conferences&lt;/i&gt;. Aerospace Research Central. https://arc.aiaa.org/doi/10.2514/6.2013-5008&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;overview_uavsensors&quot;&gt;Balestrieri, E., Daponte, P., De Vito, L., &amp;amp; Lamonaca, F. (2021). Sensors and measurements for Unmanned Systems: An overview. In &lt;i&gt;MDPI&lt;/i&gt;. Multidisciplinary Digital Publishing Institute. https://www.mdpi.com/1424-8220/21/4/1518/htm&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;buzz_swarm_stack&quot;&gt;Varadharajan, V., St-Onge, D., Svogor, I., &amp;amp; Beltrame, G. (2017, June). &lt;i&gt;A Software Ecosystem for Autonomous UAV Swarms&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;flightmare&quot;&gt;Song, Y., Naji, S., Kaufmann, E., Loquercio, A., &amp;amp; Scaramuzza, D. (2021). Flightmare: A FLEXIBLE Quadrotor simulator. In &lt;i&gt;arXiv.org&lt;/i&gt;. 4th Conference on Robot Learning (CoRL), Cambridge MA, USA. 2020. https://arxiv.org/abs/2009.00563&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;fma_paper&quot;&gt;Lupashin, S., Hehn, M., Mueller, M. W., Schoellig, A. P., Sherback, M., &amp;amp; D’Andrea, R. (2014). A platform for aerial robotics research and demonstration: The flying machine arena. In &lt;i&gt;ScienceDirect&lt;/i&gt;. Pergamon. https://www.sciencedirect.com/science/article/abs/pii/S0957415813002262&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;uav_components&quot;&gt;Unmanned systems technology: explanations (viewed on 28 November 2021) . (2021). In &lt;i&gt;Unmanned Systems Technology&lt;/i&gt;. https://www.unmannedsystemstechnology.com&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;phan_hönig_ayanian_2018&quot;&gt;Phan, T., Hönig, W., &amp;amp; Ayanian, N. (2018). Mixed reality collaboration between human-agent teams. In &lt;i&gt;IEEE Xplore&lt;/i&gt;. IEEE. https://ieeexplore.ieee.org/document/8446542&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;laclau_tempez_ruffier_natalizio_mouret_2020&quot;&gt;Laclau, P., Tempez, V., Ruffier, F., Natalizio, E., &amp;amp; Mouret, J.-B. (2020). Signal-based self-organization of a chain of uavs for subterranean exploration. In &lt;i&gt;arXiv.org&lt;/i&gt;. Frontiers in Robotics and AI (Journal). https://arxiv.org/abs/2003.04409&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;pinciroli_lee-brown_beltrame_2015&quot;&gt;Pinciroli, C., Lee-Brown, A., &amp;amp; Beltrame, G. (2015). Buzz: An extensible programming language for self-organizing heterogeneous robot swarms. In &lt;i&gt;arXiv.org&lt;/i&gt;. 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). https://arxiv.org/abs/1507.05946&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hönig_ayanian_2020&quot;&gt;Hönig, W., &amp;amp; Ayanian, N. (2020). Flying multiple UAVs using ROS. In &lt;i&gt;TC MRS&lt;/i&gt;. IEEE Robotics and Automation Society. http://multirobotsystems.org/?q=node%2F354&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;preiss_hönig_sukhatme_ayanian_2017&quot;&gt;Preiss, J. A., Hönig, W., Sukhatme, G. S., &amp;amp; Ayanian, N. (2017). Crazyswarm: A Large nano-quadcopter swarm. In &lt;i&gt;IEEE Xplore&lt;/i&gt;. 2017 IEEE International Conference on Robotics and Automation (ICRA). https://ieeexplore.ieee.org/document/7989376&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;experimental_tuning&quot;&gt;Waliszkiewicz, M., Wojtowicz, K., Rochala, Z., &amp;amp; Balestrieri, E. (2020). The Design and Implementation of a Custom Platform for the Experimental Tuning of a Quadcopter Controller. &lt;i&gt;Sensors (Basel).&lt;/i&gt;, &lt;i&gt;20&lt;/i&gt;(7). https://doi.org/10.3390/s20071940&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;optitrack&quot;&gt;Point, N. &lt;i&gt;Optitrack Motive API&lt;/i&gt; (Motive 2.2).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ros&quot;&gt;Stanford Artificial Intelligence Laboratory et al. (2018). &lt;i&gt;Robotic Operating System &lt;/i&gt;(ROS Melodic Morenia). https://www.ros.org&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;smach&quot;&gt;et al., S. A. I. L. &lt;i&gt;SMACH Executive Framework&lt;/i&gt; (ROS Melodic, 2.5.0). GitHub. https://github.com/ros/executive_smach&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;unity3d&quot;&gt;Nicholas Francis, J. A., &amp;amp; Helgason, D. (2018). &lt;i&gt;Unity3D&lt;/i&gt; (Version 2018.4.20).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mixed_reality_robotics&quot;&gt;Hönig, W., Milanes, C., Scaria, L., Phan, T., Bolas, M., &amp;amp; Ayanian, N. (2015). Mixed reality for robotics. &lt;i&gt;2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)&lt;/i&gt;, 5382–5387. https://doi.org/10.1109/IROS.2015.7354138&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rossharp&quot;&gt;Bischoff, M. (2019). &lt;i&gt;ROS# Framework&lt;/i&gt; (Version 1.5). GitHub. https://github.com/siemens/ros-sharp/releases/tag/v1.5&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ros_docs&quot;&gt;ROS Documentation (accessed 30 September 2021). (2021). In &lt;i&gt;Ros.org&lt;/i&gt;. https://www.ros.org/blog/getting-started/&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;crazyswarm_docs&quot;&gt;&lt;i&gt;Crazyswarm Documentation (accessed 30 July 2021)&lt;/i&gt;. (2021). https://crazyswarm.readthedocs.io/en/latest/&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;pradalier_2020&quot;&gt;Pradalier, C. (2020). A task scheduler for ROS. In &lt;i&gt;HAL Open Archives&lt;/i&gt;. GeorgiaTech-CNRS. https://hal.archives-ouvertes.fr/hal-01435823&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;choreography_data&quot;&gt;Carstens, T., &amp;amp; Dufaure, B. &lt;i&gt;Experiment \refsection:choreography Dataset&lt;/i&gt;. \urlhttps://drive.google.com/drive/folders/1hh6v1r74IvHczOsqgnFKYhRzEkGcKciW?usp=sharing.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Thomas Carstens</name><email>thomas.carstens@edu.devinci.fr</email></author><category term="Project" /><category term="ROS library" /><category term="Python" /><category term="Motion capture" /><category term="Swarm robotics" /><summary type="html">Design Decisions for the Software Copilot Architecture.</summary></entry><entry><title type="html">Gesture Piloting: a performance analysis</title><link href="http://localhost:4000/project/gesture-piloting/" rel="alternate" type="text/html" title="Gesture Piloting: a performance analysis" /><published>2019-05-31T00:00:00+02:00</published><updated>2019-05-31T00:00:00+02:00</updated><id>http://localhost:4000/project/gesture-piloting</id><content type="html" xml:base="http://localhost:4000/project/gesture-piloting/">&lt;style&gt;
.img-container {
text-align: center;
}
&lt;/style&gt;
&lt;style&gt;
.row {
margin-left:-5px;
margin-right:-5px;
}
&lt;/style&gt;
&lt;style&gt;
.column {
float: left;
width: 50%;
padding: 5px;
}
&lt;/style&gt;
&lt;h1 id=&quot;c2&quot;&gt;Experimentations for Human-Drone Interfaces&lt;/h1&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In (&amp;lt;span class=&quot;citation&quot; tezza_andujar_2019&quot;&amp;gt;&amp;lt;/span&amp;gt;), define Human-Drone Interaction (HDI) as a field of research that consists of understanding, designing and evaluating drone systems for use by humans, and in contact with humans. This field is similar to human-robot interaction (HRI), however, a drone’s unique characteristic to freely fly in a 3D space, and unprecedented shape makes human-drone interaction a research topic of its own. Researchers develop control modalities and better understand means of communicating with a drone.&lt;/p&gt;
&lt;p&gt;Human-drone interaction is a broad research field, for instance, a researcher can design new drones’ shapes with friendly-like appearance, while another researcher can focus on designing new user interfaces that allow non-skilled pilots to accurately operate drones without extensive training.&lt;/p&gt;
&lt;p&gt;In line with the thesis goal, we look at two types of smart systems that enhance the interactions between humans and drones. The first allows the human to pilot a drone through a gesture interface. The second looks to a virtual interface as a training ground for a real drone to avoid a virtual object. A distributed system is used to to communicate and coordinate these pipelines by passing messages to one another from any system. To better understand their tradeoffs, the distributed systems are explored and evaluated in this chapter.&lt;/p&gt;
&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_fields_bw.png&quot; style=&quot;width:6cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&quot;exploring-more-intuitive-gesture-control&quot;&gt;Exploring more Intuitive Gesture Control&lt;/h3&gt;
&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#tezza_andujar_2019&quot;&gt;(Tezza &amp;amp; Andujar, 2019)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;cauchard_e_zhai_landay_2015&quot;&gt;&lt;/span&gt; &lt;em&gt;&lt;/em&gt; focalise on innovative methods to interact with drones, including gesture, speech, brain-computer interfaces, and others (Figure &lt;a href=&quot;#hdi:fields&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;hdi:fields&quot;&gt;[hdi:fields]&lt;/a&gt;). As drones have different characteristics than ground robots, such as not allowing touch interaction, it is unclear whether existing techniques can be adapted to flying robots. Their user-centric design strategy seeks to understand how users naturally interact with drones.&lt;/p&gt;
&lt;h3 id=&quot;computer-vision-for-uav-research&quot;&gt;Computer Vision for UAV Research&lt;/h3&gt;
&lt;p&gt;With state-of-art computer vision technology, gesture-based interaction is growing and several publications are identified.&lt;/p&gt;
&lt;div class=&quot;margintable&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;&lt;strong&gt;Number&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;&lt;strong&gt;Name&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;&lt;strong&gt;Gesture&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Help&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;img src=&quot;../../assets/images/hdi_discussion/Help.PNG&quot; style=&quot;width:0.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;2&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Ok&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;img src=&quot;../../assets/images/hdi_discussion/Ok.PNG&quot; style=&quot;width:0.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;3&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Nothing&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;img src=&quot;../../assets/images/hdi_discussion/Nothing.PNG&quot; style=&quot;width:0.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;4&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Peace&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;img src=&quot;../../assets/images/hdi_discussion/Peace.PNG&quot; style=&quot;width:0.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;5&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;Punch&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;img src=&quot;../../assets/images/hdi_discussion/Punch.PNG&quot; style=&quot;width:0.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_discussion/drone_and_me.PNG&quot; style=&quot;width:4cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#cauchard_e_zhai_landay_2015&quot;&gt;(Cauchard et al., 2015)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;liu_szirányi_2021&quot;&gt;&lt;/span&gt; &lt;em&gt;&lt;/em&gt; contribute to an opensource database of body gestures which they test in practice with a drone (Figure &lt;a href=&quot;#tab:rescue_dataset&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;tab:rescue_dataset&quot;&gt;[tab:rescue_dataset]&lt;/a&gt;). This paper contributes with an outdoor recorded drone video dataset for action recognition, an outdoor dataset for UAV control and gesture recognition, and a dataset for object detection and tracking. These datasets are developed for emergency rescue services, which reveals how critical these applications can be.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#liu_szirányi_2021&quot;&gt;(Liu &amp;amp; Szirányi, 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;gesture_interface&quot;&gt;&lt;/span&gt; &lt;em&gt;&lt;/em&gt; explores real-time vision-based Human Drone Interaction with multi-robot systems. To create a team the user focuses attention on an individual robot by simply looking at it, then adds or removes it from the current team with a motion-based hand gesture. Another gesture commands the entire team to begin task execution.&lt;/p&gt;
&lt;p&gt;Compared to wearable sensor-based approaches, automated methods for video analysis based on computer vision technology are almost non-invasive. This is beneficial, and even critical, for applications in emergency rescue services.&lt;/p&gt;
&lt;h3 id=&quot;pose-recognition-algorithms&quot;&gt;Pose Recognition Algorithms&lt;/h3&gt;
&lt;p&gt;As a result, performance becomes application-critical for automated methods for video analysis. This, however, remains a technical challenge. According to &lt;a class=&quot;citation&quot; href=&quot;#gesture_interface&quot;&gt;(Monajjemi et al., 2013)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;48292&quot;&gt;&lt;/span&gt;, “Robust real-time hand perception is a decidedly challenging computer vision task, as body parts often occlude themselves or each other (e.g. finger/palm occlusions and handshakes) and lack high contrast patterns (e.g. between fingers).” To respond to this challenge, the Mediapipe framework (missing reference)&lt;span class=&quot;citation&quot; data-cites=&quot;48292&quot;&gt;&lt;/span&gt; bases itself on a Machine Learning model, and on techniques for efficient resource management for low latency performance on CPU and GPU.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/openpose.png&quot; style=&quot;width:6cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In contrast, OpenPose (missing reference)&lt;span class=&quot;citation&quot; data-cites=&quot;liu_szirányi_2021&quot;&gt;&lt;/span&gt; employs a convolutional neural network to produce two heap-maps, one for predicting joint positions, and the other for partnering the joints into human skeletons. In brief, the input to OpenPose is an image and the output is the skeletons of all the people this algorithm detects. Each skeleton has 18 joints, counting head, neck, arms, and legs. Each joint position is spoken to within the image arranged with coordinate values of x and y, so there’s an add up to 36 values of each skeleton.&lt;/p&gt;
&lt;h3 id=&quot;mixed-reality-for-uav-research&quot;&gt;Mixed Reality for UAV Research&lt;/h3&gt;
&lt;p&gt;Simulation systems have long been an integral part of the development of robotic vehicles. They allow engineers to identify errors early on in the development process, and allow researchers to rapidly prototype and demonstrate their idea.&lt;/p&gt;
&lt;p&gt;One of the first simulators that could recreate complex worlds in 3D is Gazebo, circa 2004 &lt;a class=&quot;citation&quot; href=&quot;#liu_szirányi_2021&quot;&gt;(Liu &amp;amp; Szirányi, 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;simulator_history&quot;&gt;&lt;/span&gt;. The difference between Gazebo and different 3D simulation software of that time is that Gazebo was one of the first to focus on resembling the world as realistic as possible for the robot instead of for the human. Immersive robotic simulations can be used to judge the performance of the robot and/or its concept &lt;a class=&quot;citation&quot; href=&quot;#simulator_history&quot;&gt;(Hofstede, 2015)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;gcs_validation&quot;&gt;&lt;/span&gt;. In this way, simulators can increase the efficiency and decrease the costs of the development &lt;a class=&quot;citation&quot; href=&quot;#gcs_validation&quot;&gt;(Larrabee et al., 2013)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;simulator_history&quot;&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The first published definition of Mixed Reality (MR) was given by Milgram and Kishino &lt;a class=&quot;citation&quot; href=&quot;#simulator_history&quot;&gt;(Hofstede, 2015)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;mixed_reality&quot;&gt;&lt;/span&gt; as the merging of physical and virtual worlds. In their definition, Augmented Reality (AR) and Augmented Virtuality (AV) are seen as special instances of MR. In Augmented Reality, virtual objects are projected onto the physical environment, while in Augmented Virtuality, physical objects are incorporated into a virtual environment.&lt;/p&gt;
&lt;p&gt;In &lt;a class=&quot;citation&quot; href=&quot;#mixed_reality&quot;&gt;(Milgram &amp;amp; Kishino, 1994)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;mixed_reality_robotics&quot;&gt;&lt;/span&gt;, the definition of Mixed Reality is expanded to robotics by accommodating seamless interaction between physical and virtual objects in any number of physical or virtual environments. It is further demonstrated in &lt;a class=&quot;citation&quot; href=&quot;#mixed_reality_robotics&quot;&gt;(Hönig et al., 2015)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;phan_hönig_ayanian_2018&quot;&gt;&lt;/span&gt; that Mixed Reality can reduce the gap between simulation and implementation by enabling the prototyping of algorithms on a combination of physical and virtual objects within a single virtual environment.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_sota/applications_flightmare.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In drone research, immersive simulators have various applications &lt;a class=&quot;citation&quot; href=&quot;#phan_hönig_ayanian_2018&quot;&gt;(Phan et al., 2018)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;simulator_history&quot;&gt;&lt;/span&gt;, of which two are explored here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Generating exteroceptive sensor data&lt;/strong&gt;: capturing sensor feeds of the environment for one or more drones simultaneously.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Testing navigation behaviour&lt;/strong&gt;: Testing flight patterns subject to simulated environment stimuli, prior to real-world deployment.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;generating-exteroceptive-sensor-data&quot;&gt;Generating exteroceptive sensor data&lt;/h3&gt;
&lt;p&gt;Simulation can be a huge advantage when real robot prototypes or products are not available or cannot be used due to other circumstances. During the development, simulation can be used to assess the basic hardware functionality.&lt;/p&gt;
&lt;p&gt;For instance, FlightGoggles is capable of high-fidelity simulation of various types of exteroceptive sensors, such as RGB-D cameras, time-of-flight distance sensors, and infrared radiation (IR) beacon sensors. This example can be extended to multiple sensors simultaneously, leading the way to richer distributed swarm systems.&lt;/p&gt;
&lt;p&gt;However, older simulators don’t provide an efficient API to access 3D information of the environment &lt;a class=&quot;citation&quot; href=&quot;#simulator_history&quot;&gt;(Hofstede, 2015)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;flightmare&quot;&gt;&lt;/span&gt;. To foster research in this direction, Flightmare provides an interface to export the 3D information of the full environment (or a region of it) as point cloud with any desired resolution.&lt;/p&gt;
&lt;h3 id=&quot;testing-navigation-behaviour&quot;&gt;Testing navigation behaviour&lt;/h3&gt;
&lt;p&gt;Controllers evolved in simulation can be found to be inefficient once transferred onto the physical robot, remains a critical issue in robotics, referred to as the reality gap. the most efficient solutions in simulation often exploit badly modeled phenomena to achieve high fitness values with unrealistic behaviors. This gap highlights a conﬂict between the efficiency of the solutions in simulation and their transferability from simulation to reality. When deploying to real-life scenarios, there are several challenges &lt;a class=&quot;citation&quot; href=&quot;#flightmare&quot;&gt;(Song et al., 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;drl_review&quot;&gt;&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Optimising the flight control of a UAV. This is relevant with changing payloads, unexpected weather conditions (dust, rain, changing wind), as well as preventive maintenance (motor degradation, battery damage).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Optimising the flight path of a UAV. Several sensor inputs can inform the drone’s flight path and flight speed. This gives several ways to optimise the data acquisition process, from more complex data intakes and various activation/triggering optimisations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Prior to real-world deployments, different functional elements on a robot can be tested in parallel and reduce development time. For instance, the algorithms for localization, motion planning or control can be tested, improved, and integrated continuously. There are various artificial intelligence algorithms concerned with the thematic of guidance, navigation and control (GNC). A subset of these algorithms is explored in, pertaining to Deep Reinforcement Learning (DRL). These techniques can improve the drone operation as shown in Table &lt;a href=&quot;#tab:RL_applications&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;tab:RL_applications&quot;&gt;[tab:RL_applications]&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;table*&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;&lt;strong&gt;Task&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;&lt;strong&gt;Input Observations&lt;/strong&gt;&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;&lt;strong&gt;Output Actions&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1. Quadrotor control&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;[&lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;p&lt;/em&gt;, &lt;em&gt;θ&lt;/em&gt;, &lt;em&gt;v&lt;/em&gt;&lt;/span&gt;], dim=10&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;[&lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;c&lt;/em&gt;, &lt;em&gt;ω&lt;/em&gt;&lt;sub&gt;&lt;em&gt;x&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;ω&lt;/em&gt;&lt;sub&gt;&lt;em&gt;y&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;ω&lt;/em&gt;&lt;sub&gt;&lt;em&gt;z&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;]], dim=4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;2. Control under motor failure&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;[&lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;p&lt;/em&gt;, &lt;em&gt;θ&lt;/em&gt;, &lt;em&gt;v&lt;/em&gt;, &lt;em&gt;ω&lt;/em&gt;&lt;/span&gt;], dim=12&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;[&lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;f&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;, &lt;em&gt;f&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;, &lt;em&gt;f&lt;/em&gt;&lt;sub&gt;3&lt;/sub&gt;&lt;/span&gt;], dim=3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;3. Flying through a gate&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;[&lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;p&lt;/em&gt;, &lt;em&gt;θ&lt;/em&gt;, &lt;em&gt;v&lt;/em&gt;, &lt;em&gt;ω&lt;/em&gt;, &lt;em&gt;p&lt;/em&gt;&lt;sub&gt;&lt;em&gt;g&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;θ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;g&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;], dim=18&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;[&lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;f&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;, &lt;em&gt;f&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;, &lt;em&gt;f&lt;/em&gt;&lt;sub&gt;3&lt;/sub&gt;, &lt;em&gt;f&lt;/em&gt;&lt;sub&gt;4&lt;/sub&gt;&lt;/span&gt;], dim=4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Current research of drone quadrotor control employs newly architected neural networks and learning time-optimal controllers for drone racing &lt;a class=&quot;citation&quot; href=&quot;#drl_review&quot;&gt;(Azar et al., 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;drl_review&quot;&gt;&lt;/span&gt;. This element is largely figured in Flightmare’s &lt;a class=&quot;citation&quot; href=&quot;#drl_review&quot;&gt;(Azar et al., 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;flightmare&quot;&gt;&lt;/span&gt; simulation usecases, and echoes the state of the art research in Reinforcement Learning for UAVs &lt;a class=&quot;citation&quot; href=&quot;#flightmare&quot;&gt;(Song et al., 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;drl_review&quot;&gt;&lt;/span&gt;, suggesting that new RL implementations can optimising the flight stability of a UAV as well as new perception pipelines for the navigation of a UAV. Flightmare offers convenient wrappers for reinforcement learning. Those gym wrappers give researchers a user-friendly interface for the interaction between Flightmare and existing RL baselines designed around the gym interface.&lt;/p&gt;
&lt;h2 id=&quot;section:gesture_piloting&quot;&gt;Drone Piloting With Gesture&lt;/h2&gt;
&lt;p&gt;Gestures are the most natural way for people to express information in a non-verbal way. Users can simply control devices or interact without physically touching them. Nowadays, such types of control can be found from smart TV to surgery robots, and UAVs are not the exception.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/intro/step2_diagram.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Drone piloting and other control modalities &lt;a class=&quot;citation&quot; href=&quot;#drl_review&quot;&gt;(Azar et al., 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;tezza_andujar_2019&quot;&gt;&lt;/span&gt; make use of various inputs to assist in flight. Perception modules for drone flight usually consist of data-driven models based on multiple sensor modalities. These inputs can be sensor modalities, such as camera, lidar, and radar, published in autonomous-driving related datasets, but also human commands, in the case on drone piloting. In this way, perception pipelines are routinely developed as a realtime interface for sensor data from multiple perception configurations.&lt;/p&gt;
&lt;p&gt;As of , multiple gesture interfaces have been developed for UAVs &lt;a class=&quot;citation&quot; href=&quot;#tezza_andujar_2019&quot;&gt;(Tezza &amp;amp; Andujar, 2019)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;liu_szirányi_2021&quot;&gt;&lt;/span&gt; &lt;a class=&quot;citation&quot; href=&quot;#liu_szirányi_2021&quot;&gt;(Liu &amp;amp; Szirányi, 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;gesture_interface&quot;&gt;&lt;/span&gt;, but are lacking in drone piloting. Realtime interfaces for drone piloting are discouraged &lt;a class=&quot;citation&quot; href=&quot;#gesture_interface&quot;&gt;(Monajjemi et al., 2013)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;tezza_andujar_2019&quot;&gt;&lt;/span&gt; due to high latency and low control precision compared to other drone control modalities. As of , the literature utilizing the Crazyflie nanodrone does not include realtime streaming commands &lt;a class=&quot;citation&quot; href=&quot;#tezza_andujar_2019&quot;&gt;(Tezza &amp;amp; Andujar, 2019)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;crazyflie_research&quot;&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;overview-of-pipeline&quot;&gt;Overview of Pipeline&lt;/h3&gt;
&lt;p&gt;A pipeline is implemented to ensure that the right gesture is associated to the right drone command, in real-time and continuously. This pipeline is conceptualised as in Figure &lt;a href=&quot;#fig:handpipeline&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:handpipeline&quot;&gt;[fig:handpipeline]&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_system/gesture_pipeline.png&quot; style=&quot;width:11cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The Testbed of Chapter &lt;a href=&quot;#c1&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;c1&quot;&gt;[c1]&lt;/a&gt; offers a working environment, as well as a command streaming interface between a drone and a companion computer. The focus is therefore on the two first elements: a gesture recognition workflow, followed by a drone control workflow.&lt;/p&gt;
&lt;h3 id=&quot;gesture-recognition-layer&quot;&gt;Gesture Recognition Layer&lt;/h3&gt;
&lt;p&gt;In this project, we employ a &lt;strong&gt;3D Pose Estimator&lt;/strong&gt;, described in the following section, followed by a &lt;strong&gt;Gesture Classification&lt;/strong&gt; Script.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hca.png&quot; style=&quot;width:11cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id=&quot;machine-learning-3d-pose-estimation&quot;&gt;Machine Learning 3D Pose Estimation&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_system/hdi_bookmark1.png&quot; style=&quot;width:3cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;MediaPipe Hands &lt;a class=&quot;citation&quot; href=&quot;#crazyflie_research&quot;&gt;(“Published Research That Makes Use of the Crazyflie Drone (Accessed 17 September 2021),” 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;48292&quot;&gt;&lt;/span&gt; is a high-fidelity hand and finger tracking solution. It employs machine learning (ML) to infer 21 landmarks of a hand from just a single video frame.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hand_landmarks.png&quot; style=&quot;width:10cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Precise key-point localization of 21 3D hand-knuckle coordinates remain inside the detected hand regions. This allows us to have the spatial position of each of the joints of a hand using only a normal camera.&lt;/p&gt;
&lt;p&gt;Whereas current state-of-the-art approaches rely primarily on powerful desktop environments for inference (missing reference)&lt;span class=&quot;citation&quot; data-cites=&quot;liu_szirányi_2021&quot;&gt;&lt;/span&gt;, MediaPipe Hands achieves real-time performance on a mobile phone, and even scales to multiple hands, making it an ideal solution for real-time pose tracking.&lt;/p&gt;
&lt;p&gt;The desired gesture is hardcoded by its absolute position. For example, if Figure 1’s landmark 8 is below the landmark 5, it can be interpreted as closing your index.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_system/hdi_bookmark2.png&quot; style=&quot;width:3cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id=&quot;gesture-classification-script&quot;&gt;Gesture Classification Script&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hand_drone_interaction/up_censored.jpg&quot; style=&quot;width:3.5cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/hand_drone_interaction/fist_censored.png&quot; style=&quot;width:3.5cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/hand_drone_interaction/peace_censored.jpg&quot; style=&quot;width:3.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;While the model is precise in landmark detection, there are false detections of gestures due to marginal cases. A special buffer is used to filter the most frequent gesture on a sliding window (for every 10 detections). This helps to remove glitches or inconsistent recognition.&lt;/p&gt;
&lt;h3 id=&quot;drone-control-layer&quot;&gt;Drone Control Layer&lt;/h3&gt;
&lt;p&gt;Using a live gesture recognition module, a system is designed for streaming commands to be sent to the drone.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_system/gesture_pipeline1.png&quot; style=&quot;width:11cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Note that the critical information flow between the components of the system is unidirectional. Bidirectional communication, e.g. telemetry from the vehicles, is supported, but is not required for controlled operation. All communication is done in a distributed, one-way manner, such that the gesture recognition workflow is not affected by the drone listener and there is no reliance on high-level code to keep track of the various components, preventing unnecessary interdependence. The Gesture-to-Command script decrypts the messages encoded into a custom ROS message. This workflow serves as a fallback during experiments and demonstrations.&lt;/p&gt;
&lt;h4 id=&quot;message-passing-between-applications&quot;&gt;Message passing between applications&lt;/h4&gt;
&lt;p&gt;The following message is passed via a ROS topic (TCP/IP message). It is then depacketized upon arrival.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_system/msg_structure.png&quot; style=&quot;width:10cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id=&quot;mode-selection&quot;&gt;Mode Selection&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_system/gesture_pipeline2.png&quot; style=&quot;width:5.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The following element in the drone control layer is the selection of an adequate mode.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Flight Mode 1: Position Update Mode. This mode moves the drone translationally in three axes based on an absolute position.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Flight Mode 2: Velocity Update Mode. This mode moves the drone according to inputted velocities.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In each mode, the drone can move up, down, left or right. The following hand gestures are associated with these directions.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hand_drone_interaction/pose_association_white.png&quot; style=&quot;width:11cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h4 id=&quot;velocity-filter&quot;&gt;Velocity Filter&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_system/gesture_pipeline3.png&quot; style=&quot;width:5.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The velocity filter serves as a safety measure during development and demonstrations. Given a drone’s position in the Flight Arena, it attributes a particular value between 0 and 1. This value is then multiplied to the velocity value determined from the gesture movement.&lt;/p&gt;
&lt;p&gt;The shape of this Velocity Filter was defined using an ellipsoid, with a near constant velocity within the ellipsoid and a sigmoid on the boundaries of this shape. An initial volume is designed on &lt;a href=&quot;#equation:ellipsoid&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;equation:ellipsoid&quot;&gt;[equation:ellipsoid]&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_system/velocity_1_crop2.png&quot; style=&quot;width:3cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;$$A_{1} = \frac{x^2}{a^2} +\frac{y^2}{b^2} +\frac{z^2}{c^2} \text{ with } (a, b, c) = (1.35, 0.85, 1.1)
    \label{equation:ellipsoid}$$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This figure is scaled according to constants (a, b, c). They are determined empirically by measuring the furthest distance measured by the motion capture. This ellipsoid is then transformed to better approximate the required filter.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&lt;em&gt;A&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt; = &lt;em&gt;α&lt;/em&gt; * ((1−&lt;em&gt;A&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;)−&lt;em&gt;β&lt;/em&gt;) with (&lt;em&gt;α&lt;/em&gt;,&lt;em&gt;β&lt;/em&gt;) = (18,0.3)&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_system/velocity_2_crop.png&quot; style=&quot;width:3cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The volume is scaled up by &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;α&lt;/em&gt;&lt;/span&gt; and shifted down by &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;β&lt;/em&gt;&lt;/span&gt;. Determining the &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;α&lt;/em&gt;&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;&lt;em&gt;β&lt;/em&gt;&lt;/span&gt; scaling parameters, leading to the shape in Figure &lt;a href=&quot;#fig:vel2&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:vel2&quot;&gt;[fig:vel2]&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_system/velocity_3_crop.png&quot; style=&quot;width:3cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;$$A_{3} = \frac{1}{1+e^{-A_{2}}}
    \label{equation:sigmoid}$$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A logistic regression allows for a smooth speed transition at the limits of this volume.&lt;/p&gt;
&lt;h4 id=&quot;gesture-speed-and-angle&quot;&gt;Gesture Speed and Angle&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_system/gesture_pipeline4.png&quot; style=&quot;width:5.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Hand movement is separated into angle and speed. As the hand moves in a specific direction on the screen, the components of that vector can be used to calculated the speed and angle of the drone’s movement. To help smoothen the output velocity, the mean pixel distance is taken over a rolling window.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hand_drone_interaction/grab_down_censored.jpg&quot; style=&quot;width:3.5cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/hand_drone_interaction/grab_up_fast_censored.jpg&quot; style=&quot;width:3.5cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/hand_drone_interaction/grab_up_slow_censored.jpg&quot; style=&quot;width:3.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Using these key landmarks, it is possible to discern hand poses and develop a library of drone-piloting hand signals. These are programmed accordingly in the Experimentation Section.&lt;/p&gt;
&lt;h3 id=&quot;performance-analysis&quot;&gt;Performance Analysis&lt;/h3&gt;
&lt;h4 id=&quot;aim&quot;&gt;Aim&lt;/h4&gt;
&lt;p&gt;We put in place a demonstration for flight piloting in real-time using the developed gesture interface. We present the workflow of real-time gesture piloting pipeline and we evaluate it in terms of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;System response time&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Accuracy of gesture recognition&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;evaluation-techniques&quot;&gt;Evaluation Techniques&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/intro/step2_diagram.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;System Response Time&lt;/em&gt;&lt;br /&gt;
The system response time is verified by applying a series of rapid maneuvers to register any significant delays between the pilot’s commands and their execution by the flight control system. Similarly, &lt;a class=&quot;citation&quot; href=&quot;#liu_szirányi_2021&quot;&gt;(Liu &amp;amp; Szirányi, 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;experimental_tuning&quot;&gt;&lt;/span&gt; choose to modify the drone’s angle in a specified direction. This choice is arbitrary and the changes in velocity are used in this case.The input was a demanded velocity in a specified direction. The input was changed randomly by the operator with hand movements, using the workflow described in this chapter. The output was a delay of the velocity change in the drone. Finally, a system response time is determined by averaging the response delays over the experiment.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Gesture Recognition Effectiveness&lt;/em&gt;&lt;br /&gt;
In order to evaluate gesture recognition performance, we identify the false positive and negative rates of the pose detection, to compare with existing research in real-time gesture detection. In comparison, &lt;a class=&quot;citation&quot; href=&quot;#experimental_tuning&quot;&gt;(Waliszkiewicz et al., 2020)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;bolin_crawford_macke_hoffman_beckmann_sen_2017&quot;&gt;&lt;/span&gt; evaluates the false positive and negative rates of the pose detection by manually identifying both the incorrectly recognised gestures, and the unrecognised gestures. Similarly, we identify the false positive and negative rates of the pose detection, but instead of doing it manually, we examine any discrepancies in the UAV’s trajectory flight. Any inconsistencies in recognition are considered false positives.&lt;/p&gt;
&lt;h4 id=&quot;methodology-for-piloting-operation&quot;&gt;Methodology for Piloting Operation&lt;/h4&gt;
&lt;p&gt;We design our experiments for an operator to guide the drone in an intuitive way through hand commands.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/Signal_Mode.JPG&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Dataset&lt;/em&gt; The experiment was filmed from three angles, and a presentation video is uploaded on Youtube &lt;a class=&quot;citation&quot; href=&quot;#bolin_crawford_macke_hoffman_beckmann_sen_2017&quot;&gt;(Bolin et al., 2017)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;piloting_video&quot;&gt;&lt;/span&gt;. The results were saved in a rosbag format on Google Drive &lt;a class=&quot;citation&quot; href=&quot;#piloting_video&quot;&gt;(Carstens &amp;amp; Richalet, n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;piloting_data&quot;&gt;&lt;/span&gt;. The data that is examined extends from 11:19:20 to 11:20:30 on July 29, 2021.&lt;/p&gt;
&lt;p&gt;Throughout this procedure, data is collected as a rosbag, a self-contained file for recording ROS nodes and topics. In post-processing, we timestamp the hand signal stream. This file is available at &lt;a class=&quot;citation&quot; href=&quot;#piloting_data&quot;&gt;(Carstens &amp;amp; Richalet, n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;piloting_data&quot;&gt;&lt;/span&gt; and contains:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The poses of the drone, ordered by timestamp: /tf topic.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The hand gesture message contents: /hand_signal topic.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;results&quot;&gt;Results&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Overview of Results&lt;/strong&gt; The two flight regions were plotted separately. The trajectory is plotted on 3 planes. The drone’s trajectory is first plotted on the X-Z plane (as per Figure &lt;a href=&quot;#fig:refframe1&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:refframe1&quot;&gt;[fig:refframe1]&lt;/a&gt;). The two flight regions are separated by locating the transition gesture’s timestamp.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/coordinates.png&quot; style=&quot;width:3cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_graphs/Piloting_Frontview.png&quot; style=&quot;width:11cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_graphs/Piloting_Topview.png&quot; style=&quot;height:3.1cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/hdi_graphs/piloting_3dview3.png&quot; style=&quot;height:4cm&quot; alt=&quot;image&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;../../assets/images/hdi_graphs/Piloting_Frontview.png&quot; style=&quot;height:3cm&quot; alt=&quot;image&quot; /&gt; &lt;img src=&quot;../../assets/images/hdi_graphs/Piloting_Sideview.png&quot; style=&quot;height:3cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Gesture Recognition Effectiveness&lt;/strong&gt; In preparation for this evaluation, we label the drone positions where each hand signal is detected. Figure &lt;a href=&quot;#fig:command_labels&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:command_labels&quot;&gt;[fig:command_labels]&lt;/a&gt; superposes the drone’s trajectory and the hand gestures identified at that particular point on the drone’s path.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flight Timeline with Annotated Hand Signs&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_graphs/position_mode_gestures.png&quot; style=&quot;width:9cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_graphs/Position_update_mode.png&quot; style=&quot;width:9cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The detection of hand gestures is found to be mostly continuous. Using the methodology outlined earlier, the false positive and false negative rates can be determined.&lt;/p&gt;
&lt;div class=&quot;table*&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Criteria&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;RIGHT&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;LEFT&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;UP&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;DOWN&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;THUMBUP&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;len&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;342&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;280&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;150&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;87&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;201&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;False Positives&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;5&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;33&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;3&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;6&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;201&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;False Negatives&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;30&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;175&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;10&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;5&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Accuracy (% correct)&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;89.7&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;25.7&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;93.33&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;87.35&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;table*&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Criteria&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;INDEX&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;PEACE&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;THUMBDOWN&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;TOTAL&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;len&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;192&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;131&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;100&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;1483&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;False Positives&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;15&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;6&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;100&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;369&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;False Negatives&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;23&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;36&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;299&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Accuracy (% correct)&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;80.2&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;67.94&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;0&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;56.3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Looking at the graphs, it seems that left and down gestures are quite regularly mistaken for one another. In contrast, gestures are different enough (such as right and index) are recognised at 90%.This demonstrates an interesting limitation in the pipeline’s design: the recognition seems stumble between two similar gestures.&lt;/p&gt;
&lt;p&gt;The effectiveness is averaged as the total percentage of correct gestures over the full dataset. The accuracy is determined as of the full gesture dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;System Response Time&lt;/strong&gt; In preparation for this evaluation, we to plot the speeds at which the poses are streamed, as well as the desired speed transmitted from the gesture script. The velocities of the actual drone are calculated as per Equation &lt;a href=&quot;#eq:speeds&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;eq:speeds&quot;&gt;[eq:speeds]&lt;/a&gt; from successive pose data points over the period of interest.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&lt;em&gt;Δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;x&lt;/em&gt;&lt;/sub&gt; = &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;f&lt;/em&gt;&lt;/sub&gt; − &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt; and similarly for &lt;em&gt;Δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;y&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;Δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;z&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;$$v^* = \frac{\delta u}{\delta t} = \frac{\sqrt{\Delta_{x}^2 + \Delta_{y}^2 + \Delta_{z}^2}}{t_f - t_i}
    \label{eq:speeds}$$&lt;/span&gt; This calculation is a simplification given the pose data has a stable &lt;span class=&quot;math inline&quot;&gt;120 ± 0.4&lt;/span&gt; Hz transmission frequency, which is ascertained during the experiment (Figure &lt;a href=&quot;#fig:frequency_check&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:frequency_check&quot;&gt;[fig:frequency_check]&lt;/a&gt;).&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_graphs/freq_hdiC.png&quot; style=&quot;width:8cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Figure &lt;a href=&quot;#fig:velocities&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:velocities&quot;&gt;[fig:velocities]&lt;/a&gt; plots the drone’s position and its associated velocity in Position Update Mode (&lt;strong&gt;blue&lt;/strong&gt;) followed by Velocity Update Mode (&lt;strong&gt;red&lt;/strong&gt;).&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_graphs/actual_velocities.png&quot; style=&quot;width:10cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The red graph is significantly more jaggered. This is expected since the velocity updates depend on fast moving hand movements. In Figure &lt;a href=&quot;#fig:responsiveness&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:responsiveness&quot;&gt;[fig:responsiveness]&lt;/a&gt;, we take a closer look at the interactions between a drone’s trajectory and the signs identified at that particular instance. The velocity commands (in &lt;strong&gt;green&lt;/strong&gt;) are plotted alongside the drone responses (&lt;strong&gt;red&lt;/strong&gt;).&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_graphs/desired_velocities.png&quot; style=&quot;width:10cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The system response time is determined from Figure &lt;a href=&quot;#fig:responsiveness&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:responsiveness&quot;&gt;[fig:responsiveness]&lt;/a&gt; by locating specific spikes of velocity change, in the velocity command stream, and locating spikes of velocity change in the drone flight stream. Their timestamps are recorded in Table &lt;a href=&quot;#tab:velocity_latency&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;tab:velocity_latency&quot;&gt;[tab:velocity_latency]&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;table*&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Points&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;1&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;2&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;3&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;4&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;5&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Velocity Command&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;19.49.636&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;19.50.2053&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;50.4723&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;51.3419&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;52.2437&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Velocity Response&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;19.9789&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;19.568&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;50.728&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;51.6482&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;52.5579&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Duration of Latency&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;342.9&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;362.7&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;255.7&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;306.3&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;314.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;table*&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Points&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;6&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;7&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;8&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;9&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;10&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Velocity Command&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;53.753&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;54.7583&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;55.3996&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;56.4005&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;57.6692&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Velocity Response&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;53.9478&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;54.898&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;55.6579&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;56.6375&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;57.9679&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Duration of Latency&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;194.8&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;139.7&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;258.3&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;237.0&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;298.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The average latency is found to be from the average latency of 10 points. This latency remains rather consistently between 200 and 300 ms, which demonstrates stability over time. We can perform a cross-validation this result with a visual check: we superpose the graphs by eye to determine an approximate value (Figure &lt;a href=&quot;#fig:response_shift&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:response_shift&quot;&gt;[fig:response_shift]&lt;/a&gt;).&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_graphs/velocity_adapted.png&quot; style=&quot;width:11cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To create this superposition, we shift the second graph by a difference of 250ms. This agrees with the experimental latency of 200-300ms.&lt;/p&gt;
&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/DSCF0871.jpg&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;While Chang Liu et al. focus on outdoor datasets for single large drones, this work looks towards interacting specifically on the drone’s position. Such a specific usecase of hand-following seems to be relatively rare in the literature. In fact, Tezza et al. &lt;a class=&quot;citation&quot; href=&quot;#piloting_data&quot;&gt;(Carstens &amp;amp; Richalet, n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;tezza_andujar_2019&quot;&gt;&lt;/span&gt;, despite their survey of the research field, remain sceptical as to whether this method might be the best approach to applications that require fine and precise control, as they pose the problems of higher latency and lower accuracy than other methods such as a remote controller. This vision is coerced with other members of the HDI community, and most datasets focus rather on signaling events to the drone, instead of direct piloting (Figure &lt;a href=&quot;#tab:rescue_dataset&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;tab:rescue_dataset&quot;&gt;[tab:rescue_dataset]&lt;/a&gt; from &lt;a class=&quot;citation&quot; href=&quot;#tezza_andujar_2019&quot;&gt;(Tezza &amp;amp; Andujar, 2019)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;liu_szirányi_2021&quot;&gt;&lt;/span&gt;). .&lt;/p&gt;
&lt;div class=&quot;margintable&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Model&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;Pixel 3&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;Samsung S20&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;iPhone11&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Light&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;6.6&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;5.6&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;1.1&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Full&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;16.1&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;11.1&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;5.3&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Heavy&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;36.9&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;25.8&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;7.5&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;This performance analysis has measured the pipeline latency is evaluated at 270ms. This is in large part thanks to MediaPipe Hands algorithm &lt;a class=&quot;citation&quot; href=&quot;#liu_szirányi_2021&quot;&gt;(Liu &amp;amp; Szirányi, 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;48292&quot;&gt;&lt;/span&gt;. This algorithm is relatively new, and demonstrates real-time inference capabilities, with a maximum inference of 36ms for hand landmarks (as shown in Figure &lt;a href=&quot;#fig:mediapipe_alone&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:mediapipe_alone&quot;&gt;[fig:mediapipe_alone]&lt;/a&gt;). This performance is evidently far different from that of the perception pipeline developed here around the Mediapipe framework, with different equipment.&lt;/p&gt;
&lt;p&gt;This experiment has offered a way to approach hand-drone interaction. Other approaches can offer a fuller exploration of the operator’s ease in controlling the drone, by examining the frequency at which different hand signals are used. As the operator controls the drone by sight, it is possible for them to make minute readjustments. As a result, further research could examine the role of intuition within this gesture loop. It might also be possible to explore instances where the operator does not look at the drone. Without visual feedback, this could give better hints as to the controller’s effectiveness subject to clear hand commands.&lt;/p&gt;
&lt;h4 id=&quot;regarding-the-thesis&quot;&gt;Regarding the Thesis&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_discussion/drone_and_me.PNG&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In creating better service drones, one might wonder if a piloting system is an effective means to research and development. It could easily help manage swarms of drones, but is drone development the type of research that requires the operator to make split-second decisions? After all, certain tasks require split-second reactions: drones doing free-fall recovery for instance. Perhaps it could be the beginning of an era of drone real-time learning, where drones can develop functionalities more rapidly than before, through kinetic means. Figure &lt;a href=&quot;#fig:selfie&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:selfie&quot;&gt;[fig:selfie]&lt;/a&gt; is taken from the DJI website, and shows a gesture instruction for a drone to take a picture. Perhaps functionalities like this can become more natural, more closely coupled with human behaviour.&lt;/p&gt;
&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;The gesture interface used to pilot the drones is given 56% accuracy. While the pipeline is based on MediaPipe Hands, the pose classification was hardcoded, and the software can then be improved with a neural classifier or an ML pipeline. In practice, the errors were filtered out by the drone control pipeline.&lt;/p&gt;
&lt;h2 id=&quot;section:xreality&quot;&gt;Mixed Reality Interface&lt;/h2&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_system/xr_topview_margin.jpg&quot; style=&quot;width:4cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Drone piloting and other control modalities (missing reference)&lt;span class=&quot;citation&quot; data-cites=&quot;tezza_andujar_2019&quot;&gt;&lt;/span&gt; make use of various inputs to assist in flight. Perception modules for drone flight usually consist of data-driven models based on multiple sensor modalities. These inputs can be sensor modalities, such as camera, lidar, and radar, published in autonomous-driving related datasets, but also human commands, in the case on drone piloting. In this way, perception pipelines are routinely developed as a realtime interface for sensor data from multiple perception configurations.&lt;/p&gt;
&lt;p&gt;A mixed reality interface serves to enable data transmission between a physical drone and its virtual equivalent. This section documents the design of a mixed reality environment (Figure &lt;a href=&quot;#fig:reality_interface&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:reality_interface&quot;&gt;[fig:reality_interface]&lt;/a&gt;). The first objective of the simulated environment is to serve as a graphical interface in order to develop tasks otherwise too difficult to deploy. The priority of the virtual reality is therefore set on rendering capabilities, and the ability to obtain camera streams from this environment.&lt;/p&gt;
&lt;h3 id=&quot;selected-modules-and-technologies&quot;&gt;Selected modules and technologies&lt;/h3&gt;
&lt;p&gt;This section is a brief mention of all the platforms, systems, services, and processes this interface depends on.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Unity3D &lt;a class=&quot;citation&quot; href=&quot;#tezza_andujar_2019&quot;&gt;(Tezza &amp;amp; Andujar, 2019)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;unity3d&quot;&gt;&lt;/span&gt; is a popular game engine which offers a simulated environment. It is set up as the virtual companion to the Flight Arena. Unity is well suited since it enables high-fidelity graphical rendering, including realistic pre-baked or real-time lighting, flexible combinations of different meshes, materials, shaders, and textures for 3D objects, skyboxes for generating realistic ambient lighting in the scene, and camera post-processing &lt;a class=&quot;citation&quot; href=&quot;#unity3d&quot;&gt;(Nicholas Francis &amp;amp; Helgason, 2018)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;flightmare&quot;&gt;&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ROS Sharp &lt;a class=&quot;citation&quot; href=&quot;#flightmare&quot;&gt;(Song et al., 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;rossharp&quot;&gt;&lt;/span&gt; is a set of open source software libraries and tools in C# for communicating with ROS from .NET applications, in particular Unity.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ROS &lt;a class=&quot;citation&quot; href=&quot;#rossharp&quot;&gt;(Bischoff, 2019)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;ros&quot;&gt;&lt;/span&gt; is a set of software libraries and tools that assist in building robot applications.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the sake of replicability, the version of each module is documented in the references.&lt;/p&gt;
&lt;h3 id=&quot;conceptual-overview&quot;&gt;Conceptual Overview&lt;/h3&gt;
&lt;p&gt;The link between the real and the mixed reality is designed with the following core capabilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Transmitting the &lt;strong&gt;pose of a real drone&lt;/strong&gt; into a virtual environment.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Transmitting &lt;strong&gt;an event&lt;/strong&gt; between the physical and the virtual environment.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, a drone collision with a virtual object would have the following workflow (Figure &lt;a href=&quot;#proposed_workflow&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;proposed_workflow&quot;&gt;[proposed_workflow]&lt;/a&gt;).&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_system/collision_workflow.png&quot; style=&quot;width:11cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_pres.png&quot; style=&quot;width:5.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The process of transmitting the pose of the drone to the simulator is referred to as &lt;strong&gt;pose injection&lt;/strong&gt;. This is done via the Network interface, from ROS to the simulator. The process of collision occurs in the simulator, between the injected pose, and a virtual body. This is done via a collision interface within Unity.&lt;/p&gt;
&lt;p&gt;These two elements can be readapted to a variety of event-driven scenarios. For this reason, a mixed reality setup offers inexhaustive resources to drone development.&lt;/p&gt;
&lt;h3 id=&quot;overview-of-system-network-interfaces&quot;&gt;Overview of System Network Interfaces&lt;/h3&gt;
&lt;p&gt;In order to establish a two-way mixed reality interface, the simulator and the robotics backend are configured to communicate to each other.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_system/xr_overview.png&quot; style=&quot;width:8cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To return to the System Network Layout (Chapter &lt;a href=&quot;#c1&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;c1&quot;&gt;[c1]&lt;/a&gt;, Figure &lt;a href=&quot;#fig:network_reference&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:network_reference&quot;&gt;[fig:network_reference]&lt;/a&gt;), the Mixed Reality Interface involves Unity3D as well as the Task Manager.&lt;/p&gt;
&lt;h3 id=&quot;event-sharing-workflow&quot;&gt;Event Sharing Workflow&lt;/h3&gt;
&lt;h4 id=&quot;a-network-interface-for-mixed-reality-event-sharing&quot;&gt;A Network Interface for Mixed Reality Event Sharing&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_graphs/collision_process1.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A Network Interface is used for the two objectives: injecting pose messages into the game engine and retrieving event data to be sent to the ROS Task Manager.&lt;/p&gt;
&lt;h4 id=&quot;collision-detection-in-the-virtual-environment&quot;&gt;Collision Detection in the Virtual Environment&lt;/h4&gt;
&lt;p&gt;The virtual environment is a Virtual Arena. A particularity of this arena is that it is 3 times larger than the actual flight arena. In other words, the drone’s recorded position differs from the real arena by a factor of 3, and the drone’s velocity also differs by 3. The two agents are also embodied by virtual characters, annotated as 1 and 2 in the visual below. Video feeds (Figure &lt;a href=&quot;#fig:video_feed&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:video_feed&quot;&gt;[fig:video_feed]&lt;/a&gt;) show the perspectives of both agents, and these are recorded as part of the experiment.&lt;/p&gt;
&lt;h4 id=&quot;an-event-stream-using-this-network-interface&quot;&gt;An Event Stream Using this Network Interface&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_graphs/collision_process2.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;An Event Detection is triggered within the game engine when a particular condition is met, and it then publishes the corresponding message.&lt;/p&gt;
&lt;p&gt;The Message Stream communicates the event data using ROS Messages &lt;a class=&quot;citation&quot; href=&quot;#ros&quot;&gt;(Stanford Artificial Intelligence Laboratory et al., 2018)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;ros_docs&quot;&gt;&lt;/span&gt;. ROS messages cater to a variety of sensor formats, from cameras, to pointcloud data, allowing for the ROS backend to make further decisions upon processing this data.&lt;/p&gt;
&lt;h4 id=&quot;registering-an-event-in-the-robotics-backend&quot;&gt;Registering an Event in the Robotics backend&lt;/h4&gt;
&lt;p&gt;The robotics interface, explored in Chapter &lt;a href=&quot;#c1&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;c1&quot;&gt;[c1]&lt;/a&gt;, functions on a Task basis. Events that are streamed on the network therefore need to be connected to processes for task rescheduling as well as drone state changes. Using the Topic Monitor from Section &lt;a href=&quot;#section:SPI&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;section:SPI&quot;&gt;[section:SPI]&lt;/a&gt;, changes in a streamed message can be made to induce state changes which, in turn, affects task management processes.&lt;/p&gt;
&lt;p&gt;Until the event reaches the task management interface, the real drone is programmed to fly using Chapter &lt;a href=&quot;#c1&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;c1&quot;&gt;[c1]&lt;/a&gt;’s high level interface. The full behaviour of the drone can be visualised as in Figure &lt;a href=&quot;#collision:sm_design&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;collision:sm_design&quot;&gt;[collision:sm_design]&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/choreography/trajectory_diagram.png&quot; style=&quot;width:8cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This state machine functions for a single drone: using the swarm building blocks developed in Section &lt;a href=&quot;#section:SPI&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;section:SPI&quot;&gt;[section:SPI]&lt;/a&gt;, the real drone to move to certain waypoints indefinitely. When a virtual collision is detected by the robotics backend, it induces a state change. The next state loads a custom trajectory on the drone, which is executed, before returning to an its looping trajectory.&lt;/p&gt;
&lt;p&gt;In the next section, an experiment demonstrates the proposed workflow with a collision between a drone and a virtual body, and then to examine the performance of such a system.&lt;/p&gt;
&lt;h3 id=&quot;section:collisions&quot;&gt;Performance Analysis&lt;/h3&gt;
&lt;p&gt;We set up a virtual interface between real and virtual objects in real time. This MR simulation consists of a network interface between a robotics backend (ROS) and virtual environments (Unity3D). Similarly to &lt;a class=&quot;citation&quot; href=&quot;#ros_docs&quot;&gt;(“ROS Documentation (Accessed 30 September 2021),” 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;mr_planetary&quot;&gt;&lt;/span&gt;, the pipeline is then evaluated in terms of communication latency for two separate scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;when transmitting parameters into the simulated environment&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;when transmitting parameters to the robotics backend.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;prediction&quot;&gt;Prediction&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Latency of drone pose into a virtual environment&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_graphs/latencies_method1.png&quot; style=&quot;width:4cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The latency of the pose injection is measured by determining the time difference between the ROS position and the time when it was received by the simulator.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Latency of event from the simulator to ROS&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_graphs/latencies_method2.png&quot; style=&quot;width:4cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We can answer the performance question by investigating the lag time between the moment of collision and the moment the drone reacts. We choose the moment of a Virtual Collision because it is the ideal moment of a collision between the drone’s virtual avatar, and the bot agent. The collision lag time is illustrated in Figure &lt;a href=&quot;#collision_lag_time&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;collision_lag_time&quot;&gt;[collision_lag_time]&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&quot;method&quot;&gt;Method&lt;/h4&gt;
&lt;p&gt;The time of different events is recorded as shown in Figure &lt;a href=&quot;#feedback_loop&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;feedback_loop&quot;&gt;[feedback_loop]&lt;/a&gt;. The experiment runs as such:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;a single drone is flown in the Flight Arena and it is virtualised as the drone agent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Likewise, a virtual bot agent flies a trajectory in a game Engine.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When the drone and the bot collide, the drone is designed to react, by flying a pre-programmed spiral trajectory.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Data logger setups&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_graphs/datalogger_setup1.png&quot; style=&quot;height:4.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_graphs/datalogger_setup2.png&quot; style=&quot;height:4.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This requires two separate data loggers: the one, monitoring the Unity environment, logs the timestamp and pose upon the virtual collision, and the other logs the timestamp of the drone State Change from within the Task Manager.&lt;/p&gt;
&lt;h3 id=&quot;results-1&quot;&gt;Results&lt;/h3&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_graphs/ros_vs_unity_logs_inv3.png&quot; style=&quot;width:6cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Real-to-virtual Recorded Positions&lt;/strong&gt; The drone poses are obtained through ROS and the simulator. In Figure &lt;a href=&quot;#fig:superposition&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:superposition&quot;&gt;[fig:superposition]&lt;/a&gt;, these posees are superposed. The drone can be seen in green for ROS-times at high-rate sampling (120Hz) and in purple for Unity-times at a lower sampling (10Hz). The positions superpose perfectly. This is expected. This low sampling is sufficient to show the accuracy of the real-to-virtual procedure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Real-to-virtual Timestamps&lt;/strong&gt; A second graph examines the differences in timestamps of simulator time (&lt;strong&gt;green&lt;/strong&gt;) relative to ROS-time (&lt;strong&gt;red&lt;/strong&gt;). Figure &lt;a href=&quot;#fig:pos_time&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:pos_time&quot;&gt;[fig:pos_time]&lt;/a&gt; shows a substantial lag in positions.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/coordinates.png&quot; style=&quot;width:4cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_graphs/time_delay_inv.png&quot; style=&quot;width:11cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The avatar positions occur &quot;before&quot; real positions. This is due to the logs being based on simulation time, which records events slower than real time. This simulator clock seems to be affected by a system latency.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Latency of Pose Injection&lt;/strong&gt; We determine the latency of elements when injected into the virtual environment. The latency of the system is associated to the time between simulation and ROS time when recording the same drone pose. The resulting graph is plotted in Figure &lt;a href=&quot;#fig:xr_poses&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:xr_poses&quot;&gt;[fig:xr_poses]&lt;/a&gt; and shows a linear trend.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_graphs/latency_plot_inv2_fakeregression.png&quot; style=&quot;width:11.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;By adapting a linear regression, we determine a gradient of of cumulative latency.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Latency During State Change&lt;/strong&gt; In order to visualise the event sequence, each collision is assigned a &lt;strong&gt;red&lt;/strong&gt; marker, with the moment of robotics backend state change being assigned a &lt;strong&gt;yellow&lt;/strong&gt; marker. All the collisions are taken from the virtual logs and assigned ROS-compatible timestamps.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_graphs/timelineview_legend.png&quot; style=&quot;width:3.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Collision Graphs of the Latency Experiment&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_graphs/timelineview_inv1.png&quot; style=&quot;width:11cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/hdi_graphs/front_inv.png&quot; style=&quot;width:11.3cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The first plot shows a timeline view, where the moments of start and end of the experiment clearly show a change in Z, on three different occasions. These three collisions are associated to state changes.&lt;/p&gt;
&lt;p&gt;These three collisions in particular are investigated, occurring 15 seconds from each other. Each collision latency is calculated according to the method set in the methodology. Figure &lt;a href=&quot;#fig:latency_collisions_xr&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:latency_collisions_xr&quot;&gt;[fig:latency_collisions_xr]&lt;/a&gt; demonstrates a growing lag time that approximates an exponential - a similar performance bottleneck to the previous section.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_discussion/exponent_fit.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_graphs/latency_sm_crop.png&quot; style=&quot;width:9cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This trendline is modelled after an exponential as follows: &lt;span class=&quot;math display&quot;&gt;&lt;em&gt;L&lt;/em&gt;&lt;sub&gt;&lt;em&gt;o&lt;/em&gt;&lt;em&gt;u&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt; = (6.67589×10&lt;sup&gt;−8&lt;/sup&gt;)&lt;em&gt;e&lt;/em&gt;&lt;sup&gt;0.190664&lt;em&gt;t&lt;/em&gt;&lt;/sup&gt; ms&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can model the full end-to-end system latency as the addition of the latency and the 98t ms above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&lt;em&gt;L&lt;/em&gt;&lt;sub&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;y&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;m&lt;/em&gt;&lt;/sub&gt; = &lt;em&gt;L&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt; + &lt;em&gt;L&lt;/em&gt;&lt;sub&gt;&lt;em&gt;o&lt;/em&gt;&lt;em&gt;u&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt; = 89(7.501×10&lt;sup&gt;−7&lt;/sup&gt;&lt;em&gt;e&lt;/em&gt;&lt;sup&gt;0.190664&lt;em&gt;t&lt;/em&gt;&lt;/sup&gt;+&lt;em&gt;t&lt;/em&gt;)ms&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h3&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/xr_discussion/gcs_simulation_bw.png&quot; style=&quot;width:6cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Similarly to the Swarm Application Interface of Chapter &lt;a href=&quot;#c1&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;c1&quot;&gt;[c1]&lt;/a&gt;, Flightmare offers several tasks as part of their simulator, however they do not undergo tests with real hardware. This discrepancy naturally reflects in the differences in latency, where our system is dependent on a robotics backend on top of a simulator. This chapter can be considered a perception pipeline as opposed to a set of tests that undergo in simulation.&lt;/p&gt;
&lt;p&gt;As opposed to simulators with an independent block for the physics engine, this experiment has focused mainly on visualising drone flight. Flight physics modelling is deliberately excluded. This lends itself well to a more photo-realistic, but slower, configuration.&lt;/p&gt;
&lt;p&gt;On a &lt;strong&gt;functional&lt;/strong&gt; standpoint, the proposed workflow worked as expected. A virtual body did come in collision a number of times with the drone’s avatar; through event data, the drone has reacted accordingly. This sequence of events was ensured by the choices of software architectures.&lt;/p&gt;
&lt;p&gt;To respond to the &lt;strong&gt;performance&lt;/strong&gt; question, we focus on the three key aspects highlighted in the Mixed Reality literature review:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Fast prototyping of new environments: &lt;strong&gt;programmability&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A wide suite of sensors and of physical effects: &lt;strong&gt;variability&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A true-to-reality physical environment: the &lt;strong&gt;physical&lt;/strong&gt; model.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the Mixed Reality Interface provides us with a simulated graphics engine, a communication channel was put in place that would communicate virtual events to the robot swarm. However, the collision experiment has demonstrated a cumulative delay of for a single quadrotor, and this can only increase with larger swarms and more complex manoeuvres. Since latency is a primary measure for image streaming and high performance drone tasks, we suggest the exploration of a network interface more focused on performance, and possibly the integration of existing simulators like Flightmare within the testbed.&lt;/p&gt;
&lt;div class=&quot;table*&quot;&gt;
&lt;div class=&quot;flushleft&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Test Description&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;strong&gt;Gesture Piloting&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Gesture Recognition Effectiveness&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;System Response Time&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;strong&gt;Mixed Reality Interface&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Latency of Pose Transmission&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Latency of State Changes&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;System Latency&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&quot;summary-1&quot;&gt;Summary&lt;/h4&gt;
&lt;p&gt;According to &lt;a class=&quot;citation&quot; href=&quot;#mr_planetary&quot;&gt;(Kumar et al., 2020)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;gcs_validation&quot;&gt;&lt;/span&gt;, a fully functional GCS provides for research capabilities which cannot be achieved through R/C flight alone. They emphasize Parameter Identification (PID) research as an alternative for flying a UAV by tracking values of flight and performing precise maneuvers. The second emphasis is into &quot;research into new applications of subscale aircraft for otherwise dangerous or long mundane tasks&quot;. These goals echo the elements for live performances and flight recording used in our testbed.&lt;/p&gt;
&lt;p&gt;Preiss et al. &lt;a class=&quot;citation&quot; href=&quot;#gcs_validation&quot;&gt;(Larrabee et al., 2013)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;phan_hönig_ayanian_2018&quot;&gt;&lt;/span&gt; envision that mixed reality would interconnect a wide variety of physical spaces for collaboration. Humans can work safely within their own physical confines, while their intelligent counterparts can operate in more hazardous environments.&lt;/p&gt;
&lt;p&gt;With these new mixed reality tools, Preiss et al. position their robotics testbed as &quot;serving to acclimate end users with autonomous systems&quot;. They believe their approach is also well suited for mixed reality prototyping since they &quot;will be able to substitute networking and AI components with alternative implementations&quot;, for instance by substituting onboard path finding onto offboard components. They further demonstrate that peer-to-peer networking can better simulate intercommunication between drones. With this, Preiss et al. uphold that mixed reality is a vital addition to better human-drone interfaces.&lt;/p&gt;
&lt;p&gt;This chapter demonstrates a similar vision, through the practical means of human-agent interactions. With the data streaming interfaces of this Mixed Reality section, as well as from the Piloting section, this shows that new modalities can be created for autonomous vehicles.&lt;/p&gt;
&lt;p&gt;Using the networked approach of events and sensor data, further tasks can be prototyped. This aligns with the goals of better service drones. Using the example tasks developed in the previous chapter, various GNC algorithms can be programmed, developed, and tested on hardware conditions.&lt;/p&gt;
&lt;p&gt;This method encompasses a major amount of development on the drone platform. From the development of a custom backend, to the interconnection of a graphical simulator, this completes an ecosystem for research and development. While this work has been in major part, infrastructural, it opens the door to the development and testing of GNC algorithms, for instance Reinforcement Learning algorithms, a common occurrence in recent robotics.&lt;/p&gt;
&lt;h2 id=&quot;chapter-conclusion&quot;&gt;Chapter Conclusion&lt;/h2&gt;
&lt;p&gt;This chapter presents a streaming architecture for piloting UAVs using a webcam, and various forays into Human-Drone interactions. This architecture, which makes use of the drones’ command architecture, but also of a shared network, has lent itself to integrating various inputs – in this case webcam images. The output of this exercise is evident in the precision of the drones’ movement, as it was noticed in the various visualisations of the data.&lt;/p&gt;
&lt;p&gt;However, While the Mixed Reality Interface provides us with a simulated graphics engine, a communication channel was put in place that would communicate virtual events to the robot swarm. However, the collision experiment has demonstrated a cumulative delay of for a single quadrotor, and this can only increase with larger swarms and more complex manoeuvres. Since latency is a primary measure for image streaming and high performance drone tasks, we suggest the exploration of a network interface more focused on performance, and possibly the integration of existing simulators like Flightmare within the testbed.&lt;/p&gt;
&lt;p&gt;Using the networked approach of events and sensor data, further tasks can be prototyped. Various GNC algorithms can be programmed, developed, and tested on hardware conditions.&lt;/p&gt;
&lt;p&gt;All in all, this work has come to demonstrate that a swarm setup can be rather easily adapted to human-drone research, and despite the performance bottlenecks, it succeeds in providing an end-to-end experience for the pilot, and further work should aid in&lt;a class=&quot;citation&quot; href=&quot;#phan_hönig_ayanian_2018&quot;&gt;(Phan et al., 2018)&lt;/a&gt; streamlining this.&lt;/p&gt;</content><author><name>Thomas Carstens</name><email>thomas.carstens@edu.devinci.fr</email></author><category term="Project" /><category term="ROS library" /><category term="Python" /><category term="Unity3D" /><category term="Motion capture" /><category term="Hardware-in-the-loop robotics" /><summary type="html">Testing the Performance of a custom Piloting Pipeline.</summary></entry><entry><title type="html">Mobile app development project</title><link href="http://localhost:4000/project/University-course-app/" rel="alternate" type="text/html" title="Mobile app development project" /><published>2018-11-11T00:00:00+01:00</published><updated>2018-11-11T00:00:00+01:00</updated><id>http://localhost:4000/project/University-course-app</id><content type="html" xml:base="http://localhost:4000/project/University-course-app/">&lt;h1 id=&quot;-app-for-an-engineering-undergraduate-course&quot;&gt;&lt;img src=&quot;/assets/images/favicon.jpg&quot; alt=&quot;favicon&quot; class=&quot;aligned-left&quot; /&gt; App for an Engineering Undergraduate Course&lt;/h1&gt;

&lt;p&gt;Supervised by Bruce Kloot of Mechanical Engineering UCT, I designed and developed a mobile application for enhanced interaction between students and tutors in engineering learning. This is a proof of concept from design to implementation within two undergraduate courses, and I hope to replicate the entire process for other applications in higher education. Contact me if interested. The &lt;a href=&quot;https://github.com/ThomasCarstens/UniversityCourseApp&quot;&gt;code for the app can all be found here.&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;time-bound&quot;&gt;Time-bound:&lt;/h3&gt;
&lt;p&gt;4 months development.&lt;/p&gt;

&lt;h3 id=&quot;limitations&quot;&gt;Limitations&lt;/h3&gt;
&lt;p&gt;There was no guarantee that students would even use the app.&lt;/p&gt;

&lt;h2 id=&quot;aim&quot;&gt;Aim&lt;/h2&gt;
&lt;p&gt;The goal of this project was to design an online learning space as an effective means for students to interact with tutors in the Dynamics 1 course. In this course, students are required to apply the mechanics concepts in weekly exercises. Students studying at odd hours do not necessarily have access to the right resources if they need help. This application offers a way of remotely contacting the tutors. Using the app, students can now post a question in a feed for tutors to be notified immediately. The app was serviced on the Google Playstore, and on Testflight on the iStore (via invite link). Such a tool was novel in the UCT Mechanical Engineering Department.&lt;/p&gt;

&lt;h2 id=&quot;project-presentation&quot;&gt;Project Presentation&lt;/h2&gt;
&lt;p&gt;I presented the project at UCT’s Teaching and Learning Conference (July 2019). I added timestamps for specific sections, &lt;a href=&quot;https://youtu.be/sG-mX4-pG2E&quot;&gt;and they are clickable on Youtube:&lt;/a&gt;&lt;/p&gt;

&lt;!-- Courtesy of embedresponsively.com --&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/sG-mX4-pG2E&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Outline&lt;/td&gt;
      &lt;td&gt;1:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Content&lt;/td&gt;
      &lt;td&gt;1:58&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1. An integration into MEC2023F/S&lt;/td&gt;
      &lt;td&gt;2:33&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2. Participation Statistics&lt;/td&gt;
      &lt;td&gt;3:48&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3. Proposed Features&lt;/td&gt;
      &lt;td&gt;5:10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4. Technical Requirements&lt;/td&gt;
      &lt;td&gt;15:30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5. Suggested Development Timeline&lt;/td&gt;
      &lt;td&gt;19:05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6. Community Centred Measures&lt;/td&gt;
      &lt;td&gt;19:45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7. Generating and trialling platform concepts that are adapted to the learning strategy&lt;/td&gt;
      &lt;td&gt;21:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8. Measures for Implementation&lt;/td&gt;
      &lt;td&gt;25:11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10. Versioning the app&lt;/td&gt;
      &lt;td&gt;29:44&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;11. Seeking feedback from the students&lt;/td&gt;
      &lt;td&gt;30:23&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;project-plan&quot;&gt;Project Plan&lt;/h2&gt;
&lt;p&gt;The success of the project required that an online space be developed and implemented. Additionally, it required a means to measure whether the online space has enhanced the students’ experience of the course. For the first part, the online space must be conceived, developed and implemented within the course; for the second part, an indicator is chosen to measure the success of this space.
This project was planned in a sequence of phases:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/app-for-education/thesis-outline.png&quot; alt=&quot;Thesis outline&quot; /&gt;&lt;/figure&gt;

&lt;h2 id=&quot;poster-project-summary&quot;&gt;Poster: project summary&lt;/h2&gt;
&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/app-for-education/Poster.png&quot; alt=&quot;poster&quot; /&gt;&lt;figcaption&gt;
      Poster

    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;!--
### Written Proposal
&lt;a download href=&quot;/assets/docs/Thomas_Carstens_webCV_11june.pdf&quot;&gt;Get a pdf version here&lt;/a&gt;
&lt;iframe src=&quot;/assets/docs/Thomas_Carstens_webCV_11june.pdf&quot; width=&quot;100%&quot; height=&quot;1000px&quot;&gt;&lt;/iframe&gt;
--&gt;</content><author><name>Thomas Carstens</name><email>thomas.carstens@edu.devinci.fr</email></author><category term="Project" /><category term="Mobile app development" /><category term="Ionic-Angular platform" /><category term="Firebase" /><summary type="html">A custom mobile app developed and implemented at the University of Cape Town.</summary></entry><entry><title type="html">Sampling Environment Data with a Drone</title><link href="http://localhost:4000/project/lighting-scan/" rel="alternate" type="text/html" title="Sampling Environment Data with a Drone" /><published>2017-12-02T00:00:00+01:00</published><updated>2017-12-02T00:00:00+01:00</updated><id>http://localhost:4000/project/lighting-scan</id><content type="html" xml:base="http://localhost:4000/project/lighting-scan/">&lt;h1 id=&quot;-environment-sampling-and-evaluation&quot;&gt;&lt;img src=&quot;/assets/images/favicon.jpg&quot; alt=&quot;favicon&quot; class=&quot;aligned-left&quot; /&gt; Environment Sampling and Evaluation&lt;/h1&gt;

&lt;style&gt;
.img-container {
text-align: center;
}
&lt;/style&gt;
&lt;style&gt;
.row {
margin-left:-5px;
margin-right:-5px;
}
&lt;/style&gt;
&lt;style&gt;
.column {
float: left;
width: 50%;
padding: 5px;
}
&lt;/style&gt;

&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/stage_system/stage_ref/factories_bw.jpg&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Environmental sensing usually requires substantial time for data collection or more distributed sensing systems. The use of atmospheric sensors is a major element in remote sensing &lt;a class=&quot;citation&quot; href=&quot;#vibrationmount_video&quot;&gt;(Carstens et al., n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;metrology_survey&quot;&gt;&lt;/span&gt;. With a platform that can carry multiple types of sensors, a simple field scan can help understand the limitations of a drone task, and the potential for further operations. As an air monitoring solution, this demonstration can be extended to industrial-type solutions, such as air pollution monitoring &lt;a class=&quot;citation&quot; href=&quot;#metrology_survey&quot;&gt;(Deponte et al., 2015)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;sørensen_jacobsen_hansen_2017&quot;&gt;&lt;/span&gt;. All these factors show that drones form part of a trend towards service automation for industrial purposes.&lt;/p&gt;
&lt;p&gt;We do remote sensing upon a UAV to simplify the task of environment sensing. A drone is equipped with remote sensors. It is flown in a field where it detects physical changes in the environment: lighting, humidity, and temperature. This data is then processed to determine the effectiveness of the survey. A presentation video is available (Google Drive) &lt;a class=&quot;citation&quot; href=&quot;#sørensen_jacobsen_hansen_2017&quot;&gt;(Sørensen et al., 2017)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;fieldscan_video&quot;&gt;&lt;/span&gt;. The experiment data is available on Google Drive &lt;a class=&quot;citation&quot; href=&quot;#fieldscan_video&quot;&gt;(Carstens et al., n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;fieldscan_dataset&quot;&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;daq-system-design&quot;&gt;DAQ System Design&lt;/h3&gt;
&lt;p&gt;This section documents the design of the Atmosphere Data Payload selected in Section &lt;a href=&quot;#section:sensor_system&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;section:sensor_system&quot;&gt;1.3.1.3&lt;/a&gt;. It is composed of three stages.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;../../assets/images/stage_system/drone_setup/payload_onboard.png&quot; style=&quot;width:8cm&quot; alt=&quot;image&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The payload is installed on the drone in Step 1. Flight procedures are designed in Step 2. The DAQ Activation is done in Step 3.&lt;/p&gt;
&lt;h4 id=&quot;section:msp_daq&quot;&gt;System for Low-Cost Sensors&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/stage_system/nano_pic.jpg&quot; style=&quot;width:3cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The Arduino sensor range is chosen for prototyping for its low-cost sensors. Using an Arduino Nano &lt;a class=&quot;citation&quot; href=&quot;#fieldscan_dataset&quot;&gt;(Carstens et al., n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;arduino_docs&quot;&gt;&lt;/span&gt;, such sensors can be easily integrated. The Arduino Nano is sold as a small, complete, and breadboard-friendly board based on Arduino’s larger counterpart, the ATmega328. It only weighs 7g with minimal volume.&lt;/p&gt;
&lt;h4 id=&quot;module-design&quot;&gt;Module Design&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;../../assets/images/stage_system/drone_setup/payload_onboard1.png&quot; style=&quot;width:5.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Two separate modules are designed for the Atmospheric Data and the Vibration Data. In each, an independent battery powers the logging unit, and the sensors attached to it. The Pixhawk board is included in the first system since some the GPS and luminosity sensor are logged via the Pixhawk board.&lt;/p&gt;
&lt;h4 id=&quot;daq-control-layer&quot;&gt;DAQ Control Layer&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;../../assets/images/stage_system/drone_setup/payload_onboard2.png&quot; style=&quot;width:5.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The DAQ is configured to activate and deactivate the datalogging process on command. To achieve this, we use custom activation firmware on the PX4 operating system.&lt;/p&gt;
&lt;div class=&quot;table*&quot;&gt;
&lt;div class=&quot;flushleft&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Boards&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;Switch&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;Activation&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;Deactivation&lt;/th&gt;
&lt;th style=&quot;text-align: center;&quot;&gt;Prior to Activation&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;2. Arduino Nano&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;2-2&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;On boot&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;On shutdown&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;None&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;3. Pixhawk&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;2-1&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;On boot&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;On shutdown&lt;/td&gt;
&lt;td style=&quot;text-align: center;&quot;&gt;None&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The data-logging activation file was coded in C++, and compiled into an executable via the MAVLink protocol. During operation, it toggles a pin (Pixhawk’s FMU Channel 6), which is then detected by the Arduino Nano in order to begin and end the logging on the Datalogger.&lt;/p&gt;
&lt;p&gt;A separate custom logger detects Arduino activation and records its timestamp in the PX4 debug log.&lt;/p&gt;
&lt;h3 id=&quot;sensor-system-evaluation&quot;&gt;Sensor System Evaluation&lt;/h3&gt;
&lt;h4 id=&quot;aim&quot;&gt;Aim&lt;/h4&gt;
&lt;p&gt;Using three separate atmospheric variables, we determine the accuracy of the drone sensing solution.&lt;/p&gt;
&lt;h4 id=&quot;prediction-1&quot;&gt;Prediction&lt;/h4&gt;
&lt;p&gt;Sunlit and shaded regions were scanned for relative humidity, luminosity and ambient temperature. The drone’s flightpath is changed randomly by the operator to create region overlaps. The trajectory plots demonstrate any inconsistencies in the readings. We determine the maximal variation per second and per meter as a measure of the fluctuations in lighting and in temperature.&lt;/p&gt;
&lt;h4 id=&quot;method&quot;&gt;Method&lt;/h4&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;../../assets/images/stage_system/drone_setup/payload_onboard3.png&quot; style=&quot;width:5.5cm&quot; alt=&quot;image&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Measurement Equipment Setup&lt;/em&gt; Instruments that were used in the system are pictured in Figure &lt;a href=&quot;#connections:arduino&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;connections:arduino&quot;&gt;[connections:arduino]&lt;/a&gt;. These include the DH11 Temperature and Humidity sensor, as well as a 5mm LDR Luminosity Sensor, and finally, the companion GPS. A particularity is that the DH11 is attached to the Arduino Board, and Logged with the use of an Arduino Data Logger, while the 5mm LDR is connected to an ADC input on the Pixhawk board, containing a self-enclosed data logger.&lt;/p&gt;
&lt;p&gt;The Pixhawk logger supports 100Hz data logging while the Arduino data logger averages at 10Hz logging. Both data loggers support Micro SD cards with a capacity of up to 64GB to store high-resolution video data, photos and flight telemetry.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Experiment Procedure&lt;/em&gt; The flight takes place in an empty field of approximately 100x60m, identified for the differences in lighting between the tree shade and the sunlit field. The drone is piloted by hand. This requires a certain method:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;System checks (battery monitor, screws, etc.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Activating the drone.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Drone takeoff and moving to an altitude of 2m.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Altitude lock.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Activating the Arduino data acquisition with a PX4 trigger application.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Piloted flight across the field, along sunlit and shaded regions.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/stage_graphs/environment_results/Flight_Drone.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Data collection&lt;/em&gt; The data was collected on 24 August 2021, over an empty field of approximately 100x60m. Both the lighting and the GPS data are taken from the Pixhawk Log. They were both sampled at a frequency of 98 Hz. The Arduino Logger was activated 248s after the Pixhawk Logger. The Arduino Logger was active for a duration of 712 seconds, of which 464s are common to both boards. Both the temperature and the humidity are taken from the Arduino Datalogger. They were both sampled at a frequency of 11.7 Hz.&lt;br /&gt;
&lt;em&gt;Dataset&lt;/em&gt; A presentation video is available (Google Drive) &lt;a class=&quot;citation&quot; href=&quot;#arduino_docs&quot;&gt;(“Arduino Documentation (Accessed 30 September 2021),” 2021)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;fieldscan_video&quot;&gt;&lt;/span&gt;. The experiment data is available on Google Drive &lt;a class=&quot;citation&quot; href=&quot;#fieldscan_video&quot;&gt;(Carstens et al., n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;fieldscan_dataset&quot;&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=&quot;results-1&quot;&gt;Results&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Timeline of Environment Sensor Readings&lt;/strong&gt; There is a stark contrast between sunny and shady regions in the data.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/stage_graphs/environment_results/arduino_plot.png&quot; style=&quot;width:12cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This is a change of 20% of the luminosity range, where sunny regions saturate the sensor, and shady regions are marked by sudden drops.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trajectory Plot&lt;/strong&gt; Figure &lt;a href=&quot;#fig:lighting_field&quot; data-reference-type=&quot;ref&quot; data-reference=&quot;fig:lighting_field&quot;&gt;[fig:lighting_field]&lt;/a&gt; plots the lighting readings over the trajectory.&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/stage_graphs/environment_results/Temperature_field_plot.png&quot; style=&quot;width:9cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/stage_graphs/environment_results/Humidity_field_plot.png&quot; style=&quot;width:9cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This confirms that the data is very sensitive to lighting differences. The darker patches in the sunlit area might be explained by the passage of clouds during the procedure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt; A first graph presents the magnitude of the changes in light and temperature, by computing their rates of change over time. The magnitudes are normalized by their operating ranges: 20-90% of Relative Humidity for the DHT11 sensor, 20-150mV of ADC voltage for the LDR sensor.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;$${\frac{\Delta{r}}} = 
{\frac{r_{f} - r_{i}}{r_{max}-r_{min}} * 100}$$&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/stage_graphs/environment_results/sensor_speeds.png&quot; style=&quot;height:3.2cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/stage_graphs/environment_results/changes_m2.png&quot; style=&quot;height:3.2cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;According to (a), the maximum values suggest that the LDR sensor records changes of vs the DHT11 sensor’s . The temperature and humidity vary less rapidly, and this is very apparent in the plots.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt; To better evaluate the sensing speed, we investigate the maximum fluctuations per second, and then per meter. This data recording speed is used in (b) in coordination with the drone velocity as recorded by the Pixhawk setup, in order to obtain fluctuations per meter, independent from speed. This is done with the following equations.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;$${\frac{\Delta{r}}} = 
{\frac{r_{f} - r_{i}}{t_{f} - t_{i}}}* {\frac{1}{\bar{v}}}$$&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;img-container&quot;&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/images/stage_graphs/environment_results/lux_voltage.png&quot; style=&quot;width:5cm&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The following equation is taken from &lt;a class=&quot;citation&quot; href=&quot;#fieldscan_dataset&quot;&gt;(Carstens et al., n.d.)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;ldr_testing&quot;&gt;&lt;/span&gt;, whereas determines an empirical formula to convert the ADC voltage to lux for the Arduino’s Light Dependent Resistor module: &lt;span class=&quot;math display&quot;&gt;log (&lt;em&gt;L&lt;/em&gt;&lt;sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;em&gt;u&lt;/em&gt;&lt;em&gt;x&lt;/em&gt;&lt;/sub&gt;) =  − 1.4 * log {max |&lt;em&gt;V&lt;/em&gt;&lt;sub&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;d&lt;/em&gt;&lt;em&gt;c&lt;/em&gt;&lt;/sub&gt;|} + 7.098&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Whereas the LDR sensor records , the DHT11 detects . This shows the range of local changes over the field and it seems reasonable for stark changes in light vs more gradual changes in humidity.&lt;/p&gt;
&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;
&lt;p&gt;The proposed UAV architecture has proven itself effective at capturing fluctuating environment data. When examining the luminosity readings, the measurements are very consistent with shade/light regions, by changes of as much as 20% of the luminosity range. Sunny regions saturate the sensor, and shady regions are marked by sudden drops. This suggests a high accuracy, especially seen as the drone was piloted by hand.&lt;/p&gt;
&lt;p&gt;The luminosity plot demonstrates very precise readings despite the drone’s velocity. This is facilitated by rapid data logging at 100Hz. The fact that the drone was piloted by hand, on an arbitrary path with region overlaps, illustrates plainly how mobile mapping is a worthwhile tool for rapid data collection.&lt;/p&gt;
&lt;p&gt;There is much less of a correlation between the readings and their position in space. We suggest two potential issues with the DH11 sensor:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Movement may affect the temperature and humidity. We recommend to further investigate how the readings vary with altitude, speed and acceleration.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Heat convection on the drone and the sensor itself may be recorded by the sensor instead of environment temperature. We recommend to further investigate how accumulated heat affect the readings.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, this experiment demonstrates that a payload drone can be extended to other types of sensors for other applications. The data acquisition setup proves to be functional. This setup was developed prior to the experiment with the goal to integrate many other types of sensors.&lt;/p&gt;
&lt;p&gt;The flight was quite smooth and simple to undertake. As opposed to conventional means of environmental sensing &lt;a class=&quot;citation&quot; href=&quot;#ldr_testing&quot;&gt;(Williams, 2015)&lt;/a&gt;&lt;span class=&quot;citation&quot; data-cites=&quot;xiang_xia_zhang_2020&quot;&gt;&lt;/span&gt;, this flight requires no site preparation. This is largely due to the selected drone system, as well as the work done to automate the data acquisition procedures.&lt;/p&gt;
&lt;p&gt;This procedure was greatly assisted by the datalogger, whereas GPS data and atmospheric data could be correlated without major issues. The correlation between different elements have uncovered a topography in an unexpectedly accurate manner. As we examine the systems that aid in practice, we note the importance of the drone as a platform for capturing scans of a 3 dimensional environment in a rapid, and timely manner. At the time of writing, Alliantech is compiling a marketing video for this environment sensing solution.&lt;/p&gt;

&lt;div class=&quot;table*&quot;&gt;
&lt;div class=&quot;flushleft&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Test Description&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Findings&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;DHT11&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;DHT11&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;LDR&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;LDR&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Range of Vibration Sensing System&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;0-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Margin of Error up to Natural Frequency&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;50% margin of error&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Thomas Carstens</name><email>thomas.carstens@edu.devinci.fr</email></author><category term="Project" /><category term="Electro-mechanical Product Design" /><category term="Solidworks" /><summary type="html">Analysis of a drone-sampled dataset.</summary></entry></feed>