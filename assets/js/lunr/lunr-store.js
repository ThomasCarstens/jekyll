var store = [{
        "title": "Sampling Environment Data with a Drone",
        "excerpt":" Environment Sampling and Evaluation           Environmental sensing usually requires substantial time for data collection or more distributed sensing systems. The use of atmospheric sensors is a major element in remote sensing (Carstens et al., n.d.). With a platform that can carry multiple types of sensors, a simple field scan can help understand the limitations of a drone task, and the potential for further operations. As an air monitoring solution, this demonstration can be extended to industrial-type solutions, such as air pollution monitoring (Deponte et al., 2015). All these factors show that drones form part of a trend towards service automation for industrial purposes.  We do remote sensing upon a UAV to simplify the task of environment sensing. A drone is equipped with remote sensors. It is flown in a field where it detects physical changes in the environment: lighting, humidity, and temperature. This data is then processed to determine the effectiveness of the survey. A presentation video is available (Google Drive) (Sørensen et al., 2017). The experiment data is available on Google Drive (Carstens et al., n.d.).  DAQ System Design  This section documents the design of the Atmosphere Data Payload selected in Section 1.3.1.3. It is composed of three stages.      The payload is installed on the drone in Step 1. Flight procedures are designed in Step 2. The DAQ Activation is done in Step 3.  System for Low-Cost Sensors      The Arduino sensor range is chosen for prototyping for its low-cost sensors. Using an Arduino Nano (Carstens et al., n.d.), such sensors can be easily integrated. The Arduino Nano is sold as a small, complete, and breadboard-friendly board based on Arduino’s larger counterpart, the ATmega328. It only weighs 7g with minimal volume.  Module Design      Two separate modules are designed for the Atmospheric Data and the Vibration Data. In each, an independent battery powers the logging unit, and the sensors attached to it. The Pixhawk board is included in the first system since some the GPS and luminosity sensor are logged via the Pixhawk board.  DAQ Control Layer      The DAQ is configured to activate and deactivate the datalogging process on command. To achieve this, we use custom activation firmware on the PX4 operating system.       Boards Switch Activation Deactivation Prior to Activation      2. Arduino Nano 2-2 On boot On shutdown None    3. Pixhawk 2-1 On boot On shutdown None       The data-logging activation file was coded in C++, and compiled into an executable via the MAVLink protocol. During operation, it toggles a pin (Pixhawk’s FMU Channel 6), which is then detected by the Arduino Nano in order to begin and end the logging on the Datalogger.  A separate custom logger detects Arduino activation and records its timestamp in the PX4 debug log.  Sensor System Evaluation  Aim  Using three separate atmospheric variables, we determine the accuracy of the drone sensing solution.  Prediction  Sunlit and shaded regions were scanned for relative humidity, luminosity and ambient temperature. The drone’s flightpath is changed randomly by the operator to create region overlaps. The trajectory plots demonstrate any inconsistencies in the readings. We determine the maximal variation per second and per meter as a measure of the fluctuations in lighting and in temperature.  Method      Measurement Equipment Setup Instruments that were used in the system are pictured in Figure [connections:arduino]. These include the DH11 Temperature and Humidity sensor, as well as a 5mm LDR Luminosity Sensor, and finally, the companion GPS. A particularity is that the DH11 is attached to the Arduino Board, and Logged with the use of an Arduino Data Logger, while the 5mm LDR is connected to an ADC input on the Pixhawk board, containing a self-enclosed data logger.  The Pixhawk logger supports 100Hz data logging while the Arduino data logger averages at 10Hz logging. Both data loggers support Micro SD cards with a capacity of up to 64GB to store high-resolution video data, photos and flight telemetry.  Experiment Procedure The flight takes place in an empty field of approximately 100x60m, identified for the differences in lighting between the tree shade and the sunlit field. The drone is piloted by hand. This requires a certain method:   System checks (battery monitor, screws, etc.)  Activating the drone.  Drone takeoff and moving to an altitude of 2m.  Altitude lock.  Activating the Arduino data acquisition with a PX4 trigger application.  Piloted flight across the field, along sunlit and shaded regions.       Data collection The data was collected on 24 August 2021, over an empty field of approximately 100x60m. Both the lighting and the GPS data are taken from the Pixhawk Log. They were both sampled at a frequency of 98 Hz. The Arduino Logger was activated 248s after the Pixhawk Logger. The Arduino Logger was active for a duration of 712 seconds, of which 464s are common to both boards. Both the temperature and the humidity are taken from the Arduino Datalogger. They were both sampled at a frequency of 11.7 Hz.  Dataset A presentation video is available (Google Drive) (“Arduino Documentation (Accessed 30 September 2021),” 2021). The experiment data is available on Google Drive (Carstens et al., n.d.).  Results  Timeline of Environment Sensor Readings There is a stark contrast between sunny and shady regions in the data.      This is a change of 20% of the luminosity range, where sunny regions saturate the sensor, and shady regions are marked by sudden drops.  Trajectory Plot Figure [fig:lighting_field] plots the lighting readings over the trajectory.          This confirms that the data is very sensitive to lighting differences. The darker patches in the sunlit area might be explained by the passage of clouds during the procedure.   A first graph presents the magnitude of the changes in light and temperature, by computing their rates of change over time. The magnitudes are normalized by their operating ranges: 20-90% of Relative Humidity for the DHT11 sensor, 20-150mV of ADC voltage for the LDR sensor.  $${\\frac{\\Delta{r}}} =  {\\frac{r_{f} - r_{i}}{r_{max}-r_{min}} * 100}$$          According to (a), the maximum values suggest that the LDR sensor records changes of vs the DHT11 sensor’s . The temperature and humidity vary less rapidly, and this is very apparent in the plots.   To better evaluate the sensing speed, we investigate the maximum fluctuations per second, and then per meter. This data recording speed is used in (b) in coordination with the drone velocity as recorded by the Pixhawk setup, in order to obtain fluctuations per meter, independent from speed. This is done with the following equations.  $${\\frac{\\Delta{r}}} =  {\\frac{r_{f} - r_{i}}{t_{f} - t_{i}}}* {\\frac{1}{\\bar{v}}}$$      The following equation is taken from (Carstens et al., n.d.), whereas determines an empirical formula to convert the ADC voltage to lux for the Arduino’s Light Dependent Resistor module: log (Llux) =  − 1.4 * log {max |Vadc|} + 7.098  Whereas the LDR sensor records , the DHT11 detects . This shows the range of local changes over the field and it seems reasonable for stark changes in light vs more gradual changes in humidity.  Discussion  The proposed UAV architecture has proven itself effective at capturing fluctuating environment data. When examining the luminosity readings, the measurements are very consistent with shade/light regions, by changes of as much as 20% of the luminosity range. Sunny regions saturate the sensor, and shady regions are marked by sudden drops. This suggests a high accuracy, especially seen as the drone was piloted by hand.  The luminosity plot demonstrates very precise readings despite the drone’s velocity. This is facilitated by rapid data logging at 100Hz. The fact that the drone was piloted by hand, on an arbitrary path with region overlaps, illustrates plainly how mobile mapping is a worthwhile tool for rapid data collection.  There is much less of a correlation between the readings and their position in space. We suggest two potential issues with the DH11 sensor:   Movement may affect the temperature and humidity. We recommend to further investigate how the readings vary with altitude, speed and acceleration.  Heat convection on the drone and the sensor itself may be recorded by the sensor instead of environment temperature. We recommend to further investigate how accumulated heat affect the readings.   Additionally, this experiment demonstrates that a payload drone can be extended to other types of sensors for other applications. The data acquisition setup proves to be functional. This setup was developed prior to the experiment with the goal to integrate many other types of sensors.  The flight was quite smooth and simple to undertake. As opposed to conventional means of environmental sensing (Williams, 2015), this flight requires no site preparation. This is largely due to the selected drone system, as well as the work done to automate the data acquisition procedures.  This procedure was greatly assisted by the datalogger, whereas GPS data and atmospheric data could be correlated without major issues. The correlation between different elements have uncovered a topography in an unexpectedly accurate manner. As we examine the systems that aid in practice, we note the importance of the drone as a platform for capturing scans of a 3 dimensional environment in a rapid, and timely manner. At the time of writing, Alliantech is compiling a marketing video for this environment sensing solution.        Test Description Findings         DHT11    DHT11    LDR    LDR        Range of Vibration Sensing System 0-   Margin of Error up to Natural Frequency 50% margin of error      ","categories": ["Project"],
        "tags": ["Electro-mechanical Product Design","Solidworks"],
        "url": "/project/lighting-scan/",
        "teaser": "/assets/images/posts/braai-tongs/tongs-header.jpg"
      },{
        "title": "Mobile app development project",
        "excerpt":" App for an Engineering Undergraduate Course   Supervised by Bruce Kloot of Mechanical Engineering UCT, I designed and developed a mobile application for enhanced interaction between students and tutors in engineering learning. This is a proof of concept from design to implementation within two undergraduate courses, and I hope to replicate the entire process for other applications in higher education. Contact me if interested. The code for the app can all be found here.   Time-bound:  4 months development.   Limitations  There was no guarantee that students would even use the app.   Aim  The goal of this project was to design an online learning space as an effective means for students to interact with tutors in the Dynamics 1 course. In this course, students are required to apply the mechanics concepts in weekly exercises. Students studying at odd hours do not necessarily have access to the right resources if they need help. This application offers a way of remotely contacting the tutors. Using the app, students can now post a question in a feed for tutors to be notified immediately. The app was serviced on the Google Playstore, and on Testflight on the iStore (via invite link). Such a tool was novel in the UCT Mechanical Engineering Department.   Project Presentation  I presented the project at UCT’s Teaching and Learning Conference (July 2019). I added timestamps for specific sections, and they are clickable on Youtube:                              Outline       1:00                 Content       1:58                 1. An integration into MEC2023F/S       2:33                 2. Participation Statistics       3:48                 3. Proposed Features       5:10                 4. Technical Requirements       15:30                 5. Suggested Development Timeline       19:05                 6. Community Centred Measures       19:45                 7. Generating and trialling platform concepts that are adapted to the learning strategy       21:00                 8. Measures for Implementation       25:11                 10. Versioning the app       29:44                 11. Seeking feedback from the students       30:23           Project Plan  The success of the project required that an online space be developed and implemented. Additionally, it required a means to measure whether the online space has enhanced the students’ experience of the course. For the first part, the online space must be conceived, developed and implemented within the course; for the second part, an indicator is chosen to measure the success of this space. This project was planned in a sequence of phases:        Poster: project summary            Poster         ","categories": ["Project"],
        "tags": ["Mobile app development","Ionic-Angular platform","Firebase"],
        "url": "/project/University-course-app/",
        "teaser": "/assets/images/posts/app-for-education/framework.png"
      },{
        "title": "Benchmarking drone flight",
        "excerpt":"G-JXW6H528P6   Hardware Environment  Drone Selection Process  Several drones were compared using custom criteria for drone development. These custom criteria are based on ease of use and programmability. The Dimensions criterion aims to minimize the drone size and weight. The Reconfigurable criterion investigates the modularity of the hardware layout. The Programmable criterion looks at the available interfaces for communicating with the firmware. The Autonomous Flight criterion looks at the compatibility of state estimation and trajectory planning algorithms.       Criteria Snapdragon Flight Pro Bitcraze Crazyflie Tello Drone Custom Flight Controller     Dimensions &#9678; &#9673; &#9673; &#9673; &#9673; &#9678; &#9678; &#9678; &#9678; &#9678; &#9678; &#9678; &#9678; &#9678; &#9673; &#9678; &#9678; &#9673; &#9673; &#9673;    Reconfigurable &#9678; &#9678; &#9678; &#9678; &#9673; &#9678; &#9678; &#9678; &#9678; &#9673; &#9678; &#9673; &#9673; &#9673; &#9673; &#9678; &#9678; &#9678; &#9678; &#9673;    Programmable &#9678; &#9678; &#9678; &#9673; &#9673; &#9678; &#9678; &#9678; &#9678; &#9673; &#9678; &#9678; &#9678; &#9673; &#9673; &#9678; &#9678; &#9673; &#9673; &#9673;    Autonomous Flight &#9678; &#9678; &#9678; &#9673; &#9673; &#9678; &#9678; &#9678; &#9678; &#9673; &#9678; &#9678; &#9678; &#9678; &#9673; &#9678; &#9673; &#9673; &#9673; &#9673;    Selection           The Crazyflie Drone has several advantages over other drones.   Autonomous Flight. State estimation and trajectory planning are managed by the Crazyflie firmware. The operating procedure is simplified to sending setpoint commands from a remote PC. (Chaari et al., 2021)  Programmability. At the moment of writing, there are two APIs known to send high-level commands to the drone. (“Crazyflie Documentation (Accessed 30 September 2021),” 2021)  Dimensions. Due to our space constraints, a small, light drone is preferable. Our payload of motion-capture markers brings the Crazyflie’s mass to 33 grams.  Reconfigurability. The Crazyflie is easily assembled and maintainable. It is compatible with a range of sensor modules for different activities. (“Crazyflie Documentation (Accessed 30 September 2021),” 2021)   Flight Arena and Spatial Localization       The Flight Area (Figure [fig:flight_arena]) measures 3 x 2 meters, with a table-to-ceiling distance of 1.3m. In order to dampen the impact of falling drones, the table is layered with anti-vibration cork material (Figure [fig:cork]).  Optitrack (“Crazyflie Documentation (Accessed 30 September 2021),” 2021) was adopted as the Motion Capture since the equipment was available in the laboratory. It is compatible with the swarm management solution of Section 1.5.5. Optitrack uses a Point Cloud reconstruction engine (Point, n.d.). That is, it triangulates two-dimensional points from camera images into coordinates in a three-dimensional space. For this purpose, four Flex 13 cameras are set up on the Flight Arena (as seen in Figure [fig:flight_arena]).  The Flex 13 cameras (Optitrack Documentation Version 2.2 (Accessed 30 July 2021), 2021) are infrared cameras, and so they must have an unobstructed view of any tracked object.  The exact positions of the cameras give a certain coverage of the Flight Arena. The next section determines how much of the Flight Arena is localized by the cameras.  Lightray Coverage Study  We investigate how much of the flight arena is localized by the motion capture. The drones can only be flown in a space covered by the infrared cameras, therefore we perform a design study to maximize this flight space.  Lightray Simulation  A model is designed in Solidworks (Optitrack Documentation Version 2.2 (Accessed 30 July 2021), 2021) to simulate the coverage of our cameras. Figure [fig:45] simulates the camera coverage on a table the size of the Flight Arena. A key factor is the camera’s pitch angle down from the horizontal. Figure [fig:45] compares an orientation at 45° from the horizontal to one at 30 degrees.               The coverage percentage is determined as the volume of space localized by the flight cameras over the total usable volume above the Flight Arena. Figure [fig:intersection_procedure] shows the modelling process of ray coverage volumes. The design requirements are as follow:   The cameras are placed above the table corners. within the net region so as to have a clear view of the drones when the net is lowered  There are a total of 4 cameras available during motion capture installation. The flight space measures 3×2×1.3 m.  Flex 13 cameras have a 56° field of view, and this is replicated in simulation (Figure [fig:intersection_procedure]).  In order to triangulate a position, the motion capture requires a minimum of 2 rays to intersect (Dassault Systems, 2010).   These volumes can then be determined in Solidworks using its Volumetric Tool (Optitrack Documentation Version 2.2 (Accessed 30 July 2021), 2021). For a pitch angle of 30o, The volumes of the flight space and of the intersection area above, are respectively of 7.8 m2 and 6.134 m2. As a result, we determine that the usable region for flight is 78.64% of the 3×2×1.3 m flight space. This demonstrates that 20% of the flight space is unusable. This is not surprising, considering that 4 cameras are directly above the table and constrained by the netting.  Coverage Optimisation Study  The camera’s pitch angle is varied to determine the point of optimal coverage. Figure [fig:volumes] shows the volumes generated during the study.            The study has two parts. The first study varies by increments of 5o, in order to determine the range of volume maxima. The second study finetunes by increments of 1o, with the help of a Solidworks Design Study (Solidworks 2021 Documentation (Accessed 30 July 2021), 2021). This simulation tool is used to generate volumes automatically. It has trouble generating volumes if the angle increments are too large, therefore the first study is done manually.      Pitch Volume % Max    (deg) (mm3) Covered Range    10 6 607 805 800.35 84.7154 &#9673;    15 6 970 142 481.03 89.3608 &#9678;    20 7 014 173 181.97 89.9253 &#9678;    25 6 804 720 310.30 87.2400 &#9678;    30 4 855 500 995.33 62.2500 &#9673;    35 5 006 693 602.87 64.1884 &#9673;    40 3 747 076 773.66 48.0394 &#9673;            Pitch Volume % Max    (deg) (mm3) Covered Range    16 7007549683.27 89.8404 &#9673;    17 7034381844.70 90.1844 &#9673;    18 7050625519.44 90.3926 &#9678;    19 7056144612.83 90.4634 &#9678;    20 7014172714.71 89.9253 &#9678;    21 6999358686.15 89.7354 &#9673;    22 6972124192.42 89.3862 &#9673;      Through these studies, the coverage volume was increased from 78.64% by % up to 89.95%, and by % to . This demonstrates that about 10% of the flight space is still out of reach. While the netting constraint forces the cameras to have this inconvenience, the study could be further optimised by varying the yaw angle of the cameras and moving them away from the corners.  Flight Stability Tests  Tests of the stability of robotic systems are routinely performed to measure their robustness to external forces. This is a key challenge in drone development (Solidworks 2021 Documentation (Accessed 30 July 2021), 2021), where a drone maintains dynamic stability by counterbalancing six directions of freedom, as opposed to two for wheeled systems in static stability.    Drone Selection Matrix.   The Crazyswarm ecosystem (“Crazyflie Documentation (Accessed 30 September 2021),” 2021) makes use of a position and rate controller for each drone in its swarm. This means that a setpoint is sent to each drone separately, each correcting their current pose towards the setpoint. The streaming setpoints are broadcasted from one or more antennas. As the number of drones in the swarm increases, they receive less frequent broadcasts from the antenna (“Crazyflie Documentation (Accessed 30 September 2021),” 2021).  The purpose of this test is to determine the response of the drone’s position and angle controllers to the natural disturbance during hovering. This experiment investigates the effect of antenna distance and interference on drone flight. With multiple drones to a single antenna, we evaluate if the system demonstrates any performance limits.  Hypothesis  The hypothesis is as such: the error in drone pose will correlate with the distance of the drones from the antenna.  Prediction  A hover stability test is a good measure of system performance since it requires quick readjustments of the drone to counter natural disturbances during hovering. In (Preiss et al., 2017), determine the performance of their flight controller by comparing the attitude of the drone in relation to the demanded null value of angular rotations. In contrast, our input is a setpoint. The output is a set of translation and rotational angles relative to a demanded null value for translation and rotation. This output is graphed as a deviation over time. The shape of the response charts are associated with flight stability over time.  Experiment Methodology    The Crazyflie 2.1 miniature quadcopter with four motion-capture markers, expansion boards (not visib   Hover stability is examined on the Flight Arena. The telemetry recording and external video cameras are programmed to launch with the swarm control interface. Three drones are hovered in the Flight Arena at an altitude of around 1 m. It was possible to record a 20-s long autonomous flight during which the flight controller attempted to stabilize the quadcopter. During that time, the quadcopter remained within a radius of two meters from its takeoff location.  Constraints: In preparation for the flight, each of the three drones are inspected for minimal positional displacement of less than (1cm + 0.01 rad). This ensures fully functional position controllers for the drones.  Results  The flightpaths of the three drones are plotted alongside . The topview and the sideview are featured below.            Overview of Selection Process.   The flightpaths are smooth and generally show very minimal jerking. There are very little discontinuities, attesting to a continuous localization process.      Criteria Drone 1   Drone 2       Min Max Range Min Max Range    X -0.84 -0.80 0.04 0.44 0.48 0.04    y -0.0272 -0.0228 0.0044 -0.0289 -0.0255 0.0034    Z 0.31 0.37 0.06 0.28 0.34 0.06    Roll -0.018 0.009 0.027 0.009 0.017 0.008    Pitch -0.020 0.036 0.056 -0.003 0.026 0.029    Yaw -0.034 0.039 0.073 -0.014 0.021 0.035    Selection   \\ding{55}   &amp;#1000;      &amp;#9675; &#9673; &amp;#1000; &amp;#x02A2F; Drone 2 has less variation in both translations and rotations than Drone 1. This is confirmed in Table [tab:stability_comparison]. Drone 2 is more stable in this test than Drone 1. The sample hover error is .  The discrepancy between the two drones could be attributed to a number of factors. This work may be improved with a second test, where the two drones’ positions are inversed. All in all, the flight is substantially accurate, with a peak translation of 6cm.  Conclusion of Test  Drone 2 is further from the arena and exhibits more stability. The drone that is furthest from the antenna does not have more pose error, and the hypothesis is rejected.  ","categories": ["Project"],
        "tags": ["Autonomy stack for robotics","Python","Motion capture","Trajectory planning algorithms"],
        "url": "/project/drone-benchmarking/",
        "teaser": "/assets/images/posts/drone-benchmarking/wide-gates.jpg"
      },{
        "title": "Design of a Flight Copilot",
        "excerpt":" A Testbed Environment for Task Development   Introduction  According to (Navarro &amp; Matía, 2012), swarm robotics is an approach to collective robotics that takes inspiration from the self-organized behaviors of social animals. Through simple rules and local interactions, swarm robotics aims for robust, scalable and flexible collective behaviors for the coordination of large numbers of robots. In contrast, the term swarm engineering (Navarro &amp; Matía, 2012) describes the design of predictable, controllable robot swarms with well-defined goals and the ability to function under certain conditions. Swarm engineering focuses mainly on concepts that could be relevant for real-world applications, therefore shifting swarm robotics to engineering applications. We motivate a smarter ecosystem for task development upon drones by beginning with the infrastructure for new technologies and for prototyping functionalities. A centralised swarm framework serves to set up flight performance monitoring systems, a fundamental asset to the development of robots and multi-robot groups (Brambilla et al., 2013).  Multi-robot systems and swarms of unmanned aerial vehicles (UAVs) in particular have many practical applications. (Larrabee et al., 2013) from March 2021 shows applications as diverse as surveillance and monitoring, inventory management, search and rescue, or in the entertainment industry. Swarm intelligence has, by definition, a distributed nature. Yet performing experiments in truly distributed systems is not always possible, as much of the underlying ecosystem employed requires some sort of central control. Indeed, in experimental proofs of concept, most research relies on more traditional connectivity solutions and centralized approaches (Balestrieri et al., 2021)(Larrabee et al., 2013).  In recent years there have been significant advancements in this research field. However, very rarely do UAV swarms leave the controlled and safe environment of laboratories, and when they do it is for short-duration experiments. The current use of swarms is generally based on custom, centralized solutions, in environments with reliable communication (Varadharajan et al., 2017). There remains large challenges to reliable flight of UAVs: the reliability of software, the limits on the hardware, test and validation of new elements on pre-existing systems (Varadharajan et al., 2017). A further challenge concerns multi-robot functionality. Current UAVs have very limited functionality for multi-robot coordination. Market drones are too limited for swarm research, as they only supports a single point-to-point link between a program and the drone, thus, a program can only communicate with a single drone.  In line with the thesis goal, we seek to better understand how distributed systems can offer smart assistance to multi-robot development. We explore research and techniques designed to coordinate multiple robots.  Related Work  In this section, we examine how researchers tackle the challenge of swarm engineering in the past. Prior work exists in a variety of drone laboratories (Larrabee et al., 2013) (Song et al., 2021). Drones Spatial Localization, UAV Architectures and UAV Swarm Frameworks require tradeoffs.  UAV Spatial Localization    An example of drones tracked by two motion capture cameras.     The Flight of UAVs, as with any robotic system, requires accurate positioning. However, a drone suffers from cumulative drift in position data (Lupashin et al., 2014). In order to achieve autonomous flight, a drone will need to know if it is following a trajectory correctly. A localization technology allows for this centimeter level accuracy.  It is by using highly precise equipment, that UAVs can have highly precise state estimation. This setup includes the selection of onboard positioning systems as well as external positioning solutions. We focus on optical motion capture coupled with algorithms for state estimation primarily a set of technologies commonly used by UAV laboratories(“Unmanned Systems Technology: Explanations (Viewed on 28 November 2021) ,” 2021) (Phan et al., 2018) (Laclau et al., 2020):.  The high level of precision from a motion capture system allows us to synchronously hover multiple UAVs. The tracker precision can reach sub-millimeter accuracy, and drones are hovered at a precision of a few centimeters.  UAV Architectures  UAVs such as AscTec Pelican, Parrot AR.Drone, and Erle-Copter are other examples of UAVs commonly used in the literature. These MAVs have Software Development Kits (SDK) that enable applications from third-party developers to communicate with the drones. However, both SDKs are too limited for swarm research, as they only supports a single point-to-point link between a program and the drone, thus, a program can only communicate with a single drone.    The Crazyflie 2.1 miniature quadcopter.     In comparison, the Crazyflie packages the full robotic stack. This robotic stack includes its own state estimator, control architecture and trajectory follower, which work out of the box. FreeRTOS handles the scheduling of processes and control the flight calculations. The Crazyflie contains a 32-bit, 168MHz ARM microcontroller with floating-point unit that is capable of significant onboard computation. The FreeRTOS firmware is opensource and modifiable.  The Crazyflie’s small size makes it suitable for indoor flight in dense formations. As a result, it has been used widely in research. As of 2021, this drone is used to validate research: from new algorithms for agile flight (Lupashin et al., 2014) to drone swarm research (Laclau et al., 2020).  UAV Swarm Frameworks  Unmanned Aerial Vehicle (UAV) swarms have been used indoors for formation flight and collaborative behaviors, outdoors to demonstrate swarming algorithms, and in the media for artistic shows. According to  (Phan et al., 2018), The group of robots has some special characteristics, which are found in swarms of insects, that is, decentralised control, lack of synchronisation, simple and (quasi) identical members. In this section, we explore the requirements placed on the software framework for interacting with a robot swarm.  (Navarro &amp; Matía, 2012)  discuss the key requirements a successful programming language for swarm robotics must meet. According to them, the level of abstraction need be adapted to the task at hand. The complexity of concentrating on individual robots and their interactions, i.e., a bottom-up approach, increases steeply with the size of the swarm. Conversely, a purely top-down approach, i.e., focused on the behaviour of the swarm as a whole, might lack expressive power to fine-tune specific robot behaviors. The runtime platform of the language must ensure acceptable levels of scalability (for increasing swarm sizes) and robustness (in case of temporary communication issues).  (Pinciroli et al., 2015)  present The Flying Machine Arena. This was put in place in 2014 with the goal of becoming a \"demo-and-development\" arena. They include both single and multi-robot experiments. One key element of their work is that the UAV swarm can be heterogeneous. Additionally, the position controller runs offboard, that is, the UAVs all rely on a companion computer to position themselves. The additional computational power is used for a latency compensation algorithm to improve accuracy for high-speed flights. Despite this, the framework remains robust: swarms of up to 5 UAVs are flown on a regular basis.  (Lupashin et al., 2014)  define a system architecture for a large swarm of miniature quadcopters flying in dense formation indoors. The main challenges in swarm robotics are addressed in this framework, namely by reducing communication latency to 26ms. This is done in major part via the structure of messages broadcasted to the UAV. Preiss et al. (Phan et al., 2018) use a programmable UAV with an onboard position controller, making the system more robust to communication packet drops. With this method, a swarm of 49 Crazyflies have been flown using 3 radios. As a result, the drone swarm framework allows for robotics developers to send commands to drones in a fleet. A scalable and robust run-time platform is, in this way, a key element for real-world deployment of swarm behaviors.  UAV Software and Middleware  UAVs have a long tradition of being controlled with the Robotic Operating System. ROS is a meta-operating system designed for the construction of distributed systems. It provides a set of extensible tools for managing distributed robotic applications. The main goals of ROS are package management, hardware abstraction, low-level device control, message exchange between processes, and implementation of several functionalities. As a result, there are many ROS packages devoted to controlling such UAVs as individuals.  However, using multiple UAVs creates entirely new challenges that such packages cannot address. These new challenges include, but are not limited to, the physical space required to operate the robots, the interference of sensors and network communication, and safety requirements. In (Phan et al., 2018) and (Hönig &amp; Ayanian, 2020), thus motivates the use of a hardware abstraction layer on top of the Crazyflie. This abstraction layer, in the form of a ROS layer, is only used on the PC controlling one or more Crazyflies. The ROS driver sends the data to the different quadcopters using the protocol defined in the Crazyflie firmware.  (Phan et al., 2018)  demonstrates interoperability between the PC and UAV components. The crazyflie_ros framework helps wrap CRTP within a ROS framework, which is useful for scenarios of hovering and waypoint following from a single robot to the more complex multi-UAV case. It provides not only standard operating system services (hardware abstraction, contention management, process management), but also high-level functionalities (asynchronous and synchronous calls, centralised database, a robot configuration system, etc.). Additionally, this includes command-line tools and a GUI for mass rebooting, firmware updates, firmware version query, and battery voltage checks over the radio.    Overview of the Crazyswarm Control Loop, as per the Crazyswarm official documentation (August 2021)    (Hönig &amp; Ayanian, 2020)  furthers this work by offering all the necessary components for controlling multiple drones remotely, by relating the drone flight controller of the Crazyflie to a set of controllers on the PC, but also by offering ways to send trajectories to the drones in realtime. Crazyswarm attempts to couple an external motion capture technology like Optitrack with the rest of a drone’s control loop: knowing its position, the drone will be able to generate and follow a trajectory more precisely. When viewing a single body, motion capture certainly has sub-millimeter accuracy. However, as the number of drones increases, there are two limiting factors to the reliability of the control loop: the first is recognition of the drones by the optical capture system, and the second is low communication bandwidth. Multiple algorithms are therefore incorporated into this framework to mitigate the effects of these processes.  (Preiss et al., 2017)  design a distributed cloud robotic architecture for computation offloading based on Kafka middleware as messaging broker. Empowering robots with cloud computing comes with a fundamental tradeoff. Offloading the execution of a computationally intensive algorithm to the cloud can reduce resource utilization, including CPU, memory, and the battery. However, this comes with a cost: communicating with cloud resources over a congested network increases latency and can lead to delay for real-time applications.  showcase that an offloading decision need not reduce the overall execution time of the application.  System Overview  Functionality  The testbed is designed according to four functional requirements.   Managing the interface with drone firmware.  Localizing the drones in a Flight Arena.  Rendering the drones in a simulated environment.  Managing the flow of offboard code for each drone.   These elements occur separately and simultaneously. They manage individual drones asynchronously from one another, a key element in swarm engineering. Each of these requirements is fulfilled respectively by Crazyswarm, Optitrack, Unity and the Task Manager. Each of these are explored in turn in this chaper.  Network Architecture  Figure [fig:network_interfaces] gives a brief overview of the data interfaces between the four main components of this architecture.    Overview of Network Interfaces.   The data flow in the Flight Arena is as follows: vehicle/object pose measurements are provided by a motion capture system to software modules running on companion computers running consumer operating systems. Within task-specific modules (\"user code\") and the Crazyflie communication channels, estimation and control pipelines produce vehicle motion commands. The appropriate commands are transmitted to the vehicles. Onboard the vehicles, high-frequency controllers track these commands using on-board inertial sensors in feedback. All intermodule communication is via multicast UDP and the vehicles commands are sent over a dedicated wireless channel.  Chapter Structure  The drone testbed is comprised of a hardware and a software environment. First, Section 1.3.2 presents the Network Interfaces. The Hardware Environment consists of the physical Flight Arena. Section 1.4 presents this Arena, the drone model and the localization system. Section 1.5 then touches on swarm management followed by task management.   Software Environment  Modules involved in Software Environment  This section is a brief mention of all the platforms, systems, services, and processes the software environment would depend on.   Motive (Waliszkiewicz et al., 2020) processes OptiTrack camera data to deliver global 3D positions, marker IDs and rotational data.  Crazyswarm (Point, n.d.) is an swarm management layer that allows multi-drone flight of Bitcraze Crazyflie drones in tight, synchronized formations,  ROS (Preiss et al., 2017) is a set of software libraries and tools that assist in building robot applications.  SMACH (Stanford Artificial Intelligence Laboratory et al., 2018) is a task-level architecture for rapidly creating complex robot behavior and integrating ROS utilities,  Unity (et al., n.d.) is a cross-platform game engine used in a range of mixed reality research (Nicholas Francis &amp; Helgason, 2018)(Hönig et al., 2015)(Phan et al., 2018).   For the sake of replicability, the version of each module is documented in the references.  Operating Systems  We adopt a distributed systems approach, whereas various components are spread across multiple computers on a network. These devices split up the work, coordinating their efforts to complete the job more efficiently than if a single device had been responsible for the task. This section is a brief description of relationships between the modules and system features. Figure [fig:OS_diagram] encapsulates the software modules into their respective operating systems, Ubuntu and Windows.    The Motion Capture Table, net and Flex 13 cameras positioned above the platform.     Each OS accommodates compatible software technologies used in this architecture. Optitrack and Unity have been developed for Windows systems. A Windows 10 OS is loaded on a standalone PC. On the other hand, ROS have been developed for Ubuntu systems. An Ubuntu 18.04 OS is loaded on a standalone PC. The interface between the two is managed by the ROS middleware, which is expanded upon in Section 1.5.3.  Middleware Solution  In a middleware (Bischoff, 2019), modules do not need to be linked within a single process, and this instead can be separated into the following elements.   Package management: drivers and other algorithms can be contained in standalone executables,  Hardware abstraction: in software, this refers to a sets of routines that provide programs with access to hardware resources through programming interfaces. This is explored in Section 1.6: Swarm Programming Interface.  Low-level device control: the ROS interface serves as a communication layer with onboard devices such as motors and the battery sensor,  Message exchange between processes: inter-process communications allows to pass data between modules, such as data from drone poses shown in Figure [fig:ROS_in_system].  Managing robotics-related functionalities: handling the concurrent activity of multiple robots via a global parameter manager and a global task manager.   The main objective for this system’s Middleware Solution is a more flexible, more reconfigurable and generally modular layout. This proves useful in a development and demonstration environment that requires many critical moving parts. This system’s network interface is shown in Figure [fig:ROS_in_system].    Protective cork layer.   ROS provides a central role of resource management, from managing various interfaces in the system implementation to further hardware abstractions.    Lightray simulation on the Table for 45° angle from the horizonta   Virtualisation of Physical Objects  Once localized by the motion capture setup, pose data is transferred to the middleware layer. The pose data of physical objects, including the drones, becomes available in real-time to a range of companion software, via this ROS middleware layer.  Swarm Management Layer  The Crazyswarm framework (“ROS Documentation (Accessed 30 September 2021),” 2021) is adopted as an control layer for the Crazyflie drone. The main advantages of the Crazyswarm over other frameworks are:   Motion capture integration. Crazyswarm contains drivers for the Optitrack System. In contrast, the Crazyflie proprietary API can send position measurements to the Crazyflie, but does not know how to get position measurements from mocap hardware.  Python firmware bindings. Crazyswarm’s simulator is built upon automatically generated Python bindings for certain modules in the Crazyflie firmware. The binding system can be helpful when developing new firmware modules, especially when they are mathematically complex and hard to debug.  ROS foundation. The Crazyswarm server program is a ROS node. The Python API Reference is a thin wrapper around the ROS interface. The ROS interface is explored in this section.     Lightray simulation on the Table for 45° angle from the horizonta   Simulation Environment Layer  The first objective of the simulated environment is to serve as a graphical interface in order to develop tasks otherwise too difficult to deploy. The priority of the virtual reality is therefore set on rendering capabilities, and the ability to obtain camera streams from this environment. The robotics backend, described in the previous elements, can interact with the Unity3D game engine.  As shown in Figure [fig:virtual_pin], ROS has a steady stream of poses from the physical drones, allowing for virtual visualisation. Key events and data can be exchanged between ROS and Unity3D. The way this is achieved is examined in Section [section:xreality].  Task Management Layer    Modelling the Coverage Volume   A Task Manager assists in the scheduling of flight tasks relative to one another. The task manager has multiple responsibilities in this framework.   First, it loads the description of all tasks.  It then provides a service to start or stop a given task,  It keeps track of the status of all tasks currently running or recently terminated.  It is also responsible for instantiating the task scheduler that manages the threads in which tasks actually run.   This manager is implemented with a Client-Server communication as seen in Figure [fig:client-server].    Modelling the Coverage Volume   The Client directly tracks the state of each process in a larger decision process. The Action Server interacts with automated functionality, and the Flight Server interacts with the robot instruction stream. The building blocks of this approach are:   A Client State Machine  Client-Server Messages  Server Handling of Actions  A Distributed Parameter Handler  Scaling to Multiple Drones   1 | A Client State Machine  The client requires a decision-maker between each state and a set of possible future states. A state machine is chosen to coordinate the transition between different usecases. For this, the SMACH library is used (Crazyswarm Documentation (Accessed 30 July 2021), 2021). Task handling is implemented with several scheduling elements.   Concurrency: the ability for a program to be decomposed into parts that can run independently from each other. This means that tasks can be executed out of order and the result would still be the same as if they are executed in order.  Preemption: the act of temporarily interrupting an executing task, with the intention of resuming it at a later time. This interrupt is done by an external scheduler with no assistance or cooperation from the task.  Interruption: a process tells the task manager to stop running the current program so that a new one can be started.   2 | Client-Server Messages  A message transmits data values during client-server communication. ROS uses a simplified messages description language (et al., n.d.) for describing the data values (aka messages) that ROS nodes publish. This description makes it easy for ROS tools to automatically generate source code for the message type in several target languages.    Lightray simulation on the Table for 45° angle from the horizontal   In an Action Client-Server interaction, communication is ensured ROS Messages with three distinct roles: the goal, the feedback and the result (“ROS Documentation (Accessed 30 September 2021),” 2021). An action is executed when the goal requests an action with a set of parameters to the server. Feedback parameters can selected for monitoring during the action’s execution. The result informs any concurrent threads of the final state of the action.    Lightray simulation for Intersection of any 2 lightrays.   3 | Server Handling of Actions  The Action Server executes an action in the form of a callback functions. A task completes when a particular condition is met. In order to manage this, an action callback can incorporate a condition in its execution. The next section examines this in more depth.  Each of these pre-loaded behaviours needs to be scheduled. For this, an open-source Function Handler is used, referred to as the ROS Action Server (“ROS Documentation (Accessed 30 September 2021),” 2021).As a result, each function for a specific task server will be launched from a central launchfile:    html &lt;launch&gt; &lt;group&gt; &lt;remap from=’_goTo’ to=’drone1_goTo’/&gt; &lt;node name=’drone1’ pkg=’crazyswarm’ type=’ros_action_server.py’&gt; &lt;/node&gt; &lt;/group&gt;  &lt;group&gt; &lt;remap from=’_goTo’ to=’drone2_goTo’/&gt; &lt;node name=’drone2’ pkg=’crazyswarm’ type=’ros_action_server.py’&gt; &lt;/node&gt; &lt;/group&gt; &lt;/launch&gt;    This ROS launchfile loads the functions declared in the Action Server, and remaps them to each drone in the choreography. This allocates a thread under the form of a ROS node. These ROS nodes act as separate Request/Response instances.  4 | ROS Parameter Handler  The ROS main thread includes a commonly-used component called the Parameter Server, implemented in the form of XMLRPC, and which is, as the name implies, a centralised database within which nodes can store data and, in so doing, share system-wide parameters.    python crazyflies: - channel: 35 id: 1 initialPosition: [0.0, 0.0, 0.0] type: default - channel: 27 id: 2 initialPosition: [1.0, 0.0, 0.0] type: default - channel: 27 id: 5 initialPosition: [4.0, 0.0, 0.0] type: default    Multiple programs query this file upon initialization of the swarm management layer. Each robot is distinguished by their channel, id and initialPosition. This allows for identifying drone ids and other unique information.    Lightray simulation for Intersection of any 2 lightrays.   5 | Scaling to Multiple Drones  Similarly to (“ROS Documentation (Accessed 30 September 2021),” 2021), a procedural task-based programming approach is adopted. This can be likened to a centralized server that services multiple drones. Figure [fig:task_management_architecture] shows the full Task Management Layer Architecture.    Results of Coverage Optimisation Study.   This approach values granularity, being lightweight, and the ability to share similar processes across multiple apps. As a result, it is therefore highly reusable for new tasks.  High Level Interface  A high level interface is an abstraction layer for development activities. In order to simplify task development, and align with the thesis goals, we develop a framework for high level interaction between the operator and the functionalities of the testbed.  Motivation  The Testbed, as described in Section. For such purposes, it is required to test user code. Therefore an interface is a key element for the user. There are several advantages to specialised tasks for the testbed.   Handling sub-tasks to various levels of depth: microservices help automate sub-tasks at a desired complexity. When encapsulated in this way, are separate modules fit for demonstration, that can later be optimised and refined during development.  Monitoring the swarm: a central monitoring system can run in parallel with the particular algorithms that are tested and validated. For instance, a battery voltage threshold helps to monitor a correct running of the hardware.This allows for preventive maintenance during demonstrations but also during development.   In summary, more ‘complex tasks’ will allow for the automation of separate subtasks in a controlled manner.  Conceptual Overview  The high-level interface combines three major elements:   the management of low-level devices upon each robot,  communication with the swarm, and  scheduling of instructions.   In order to achieve this, the intermediary structures for tasks are laid out here. Figure [fig:loop] labels a hierarchy of tasks. The drone is instructed to alternate between two waypoints until a software condition is triggered.    Determining the intersection area of lightrays   This example serves to illustrate the conceptualisation of a subtask and a multi-step task. The concurrence of waypoints and the software trigger is consider a sub task, and within a state machine, which is referred to as a multi-step task. This framework offers a definition for complex tasks, as tasks that coordinate the scheduling of instructions, with multi-robot instructions.  Architectural Approach  To create these complex tasks, this high level interface has the following architectural choices:   Encapsulating robot instructions Robot commands are assimilated into this programming interface as individual tasks.  Encapsulating swarm instructions Robot instructions are included in generic functions as swarm instructions.  Encapsulating sub tasks Scheduling processes such as a concurrence runs in a function encapsulating it.   This architecture is written in Python, known for its ease of use and flexibility. In this way, multi-step tasks manage the swarm stack, from executing single-robot commands to ensuring the dynamic management of swarms.  Robot instructions    Overview of the Crazyswarm Control Loop, as per the Crazyswarm official documentation (August 2021)    The Crazyswarm API from Section 1.5.5 interfaces with low-level hardware for landing, takeoff and further behaviours that can be coded remotely. However, for the purpose of centralised task management, the execution of each instruction should be monitored accordingly. (Pradalier, 2020) offer ROS telemetry tools, such as battery monitoring and a reset utility. These can be used as conditions in the execution process. Ultimately, an additional layer of abstraction is required for multi-drone instructions.  Multi-robot instructions    View of the Flight Arena during the 2 Drone Hover experiment.   This section outlines the functions developed for group behaviours: concurrent takeoffs and landings, querying multiple drones for low battery level, etc. Fly-Octogon and Land-all are examples of multi-robot instructions.  This microservice model is a major component of optimizing swarm programming towards the multipurpose task model outlined in the objectives.  Sub task management    Hover Experiment: Stability Tests on Flight Arena.   The objective for sub tasks is to assist in creating, and coordinating, higher-level behaviours. An example of this is the concurrent_traj module, whereas two drones are told to fly simultaneous trajectories. This is of particular interest as two drones will take indeterminate amounts of time to respond to commands. Concurrency — in the context of programming — is the ability for a program to be decomposed into parts that can run independently of each other.  Multi-step tasks  A decision process combines the various modules developed above into a sequence of tasks. This is achieved with a Finite State Machine, which is implemented programmatically with the SMACH python library (Hönig &amp; Ayanian, 2020). A choreographic state machine is implemented in section 1.7.  Testbed Demonstration  A drone choreography is designed as a live demonstration of the Testbed’s functionality. The experiment data is accessible publicly (et al., n.d.).  Choreography Design    Hover Experiment: Stability Tests on Flight Arena.   The State Machine for the full choreography is available in Figure [diagram:fsm]. This demonstration includes:   Takeoff and landing, separately and concurrently.  A pre-loaded trajectory, from a Bezier curve: concurrently.  A polygonial shape flown by two drones, demonstrating simultaneous movement through a set of waypoints.  Autonomous state changes.   This state machine functions any number of drones: using the swarm building blocks developed in Section 1.6, the dronesexecute trajectories simultaneously; it then moves to certain waypoints indefinitely. In this case a figure of 8 is executed on both drones followed by an octogon. Finally, upon an operator signal, the drones land. The state machine is such that the drones also land if one does not reach its corresponding waypoint in time.  1 | Pre-loaded Trajectory    3D Plot of 2 Drone Hover.   Two Figures of 8 are flown simultaneously. The figures of 8 are concurrent Bezier shapes, pre-loaded onboard each drone’s trajectory follower (Carstens &amp; Dufaure, n.d.). This is coded using the high level interface as in Fig [code:exec_Fo8].    python fig8_sm = concurrent_trajs(selected_drones = ids, traj_id = 8) StateMachine.add(’FIG8_EXECUTE’, fig8_sm, transitions=’succeeded’ : ’NEXT_STATE’, ’aborted’ : ’land_all’, ’preempted’ : ’land_all’)    The Figure of 8 is assigned an id of 8. Other trajectories are assigned other ids. The concurrent_trajs function is thus called upon with the required drones and their required ids.  2 | Multi-point Trajectory.    Stability Comparison of two Drones.   This state loads a custom trajectory on the drone, which is executed, before moving to an indefinite octogonal trajectory. The use of waypoint following is an automation of the motion to specific points.  3 | Topic Monitor    Hover Experiment: Pitch, Yaw and Roll of the two drones.   The use of a Topic Monitor is useful to interface with active topics. For instance, at any one moment that a drone gets too close to a particular point, it initiates a landing. The intended behaviour is represented visually alongside.  This is another such subtask that fulfils the initial goal: monitoring the swarm with preventive measures during demonstration as well as training phases. This is performed programmatically with a concurrence between a drone and the /collision topic.  4 | Choreography State Machine  The previous sections are integrated into a State Machine. The configuration of the state machine is displayed in Figure [fig:fsm]. Individual tasks are coloured in green and swarm tasks in orange.    Network Interfaces Encapsulated in Operating Systems.    Three drones are positioned about the Flight Arena as in Figure [fig:chore_initialisation].  5 | Choreography Execution    Network interfaces with ROS.   The state machine is run on a separate thread as in Figure [code:exec_choreography].   myswarm = swarmInterface(all_ids = [1,3,5]) sm0 = myswarm.execTrajandOctogon (ids = [1,3], traj_shape = 8) myswarm.start_sm_on_thread(sm0)   This invocation of the state machine clearly shows drone ids [1,3,5] as extracted from the Parameter Server, in order to act as the drones 1,2,3. The trajectory shape 8 refers to the Figure of 8.  Results  We proceed with an inspection of the demonstration. The flightpaths of all three drones are plotted together.    Swarm solution interactions with System Architecture     Crazyswarm within the Network Implementation.   Overall, the flightpaths are smooth. The Figures of 8 are traced distinctly, as well as the two octogons. The figure of 8 of drone 3 is discontinuous, and yet there is no apparent effect on the shape. This suggests that the drone moved beyond the area localized by motion capture. When examining the octogons, the top view shows a near perfect superposition: showing small differences in position of less than 2cm. Finally, a line connects the two shapes. This shows that these figures did actually occur in sequence.       Further inspection of the octogons requires a topview and a sideview. The Octogon is traced very clearly. Near the end of the experiment, there is a noticeable wobble in the blue line. This behaviour is due to a low battery level. A notable difference is the wobble in the xz plane, which demonstrates a loss of precision as the drones get closer to the ground. There is a symmetrical behaviour. Further tests can determine what this is attributed to.       Test Description Value       Volume of Flight Arena Localized by Motion Capture      Maximum Flight Error Recorded in Hover Test         Discussion    Simulation environment interactions with System Architecture   In this chapter, a swarm programming approach is developed along similar lines to (Crazyswarm Documentation (Accessed 30 July 2021), 2021), the swarm API that was developed for swarms of nanodrones. Both frameworks load a set of functions, they allow the user to select which drones perform a certain task, and group drones according to the task at hand. While Buzz manages membership with a dedicated hash table, our interface makes use of a global parameter handler (Pinciroli et al., 2015). Both architecture allows for the development different modules can be developed independently and related dynamically.  With a high-level interface, this work concerns itself with a swarm-specific language that is not \"too top-down\" or \"too bottom-up\". This distinction is seen with increasing swarm sizes, and for creating user tasks more focused on development, or on improving safety and phone interference for demonstrations.    Task Manager interactions with System Architecture   Figure [diagram:swarm_stack] demonstrates that Buzz uses comparable structures for swarm engineering.). ROS communicates with device hardware via MAVROS, a ROS library for compatible Micro Aerial Vehicules. It then has a control distribution layer that is comparable to our Task Manager, a swarm communication layer like Crazyswarm and a swarm control layer like our high level interface. The swarm stack in other research may be composed of other technologies, but retains this structure.  The Flying Machine Arena (Varadharajan et al., 2017) is an active area of research for drone development and demonstration, and their ’Copilot’ is described as a flight monitoring solution. Figure [diagram:copilot] demonstrates the types of activities achieved via the copilot: updating drone poses during code execution (a), executing playback on recorded poses (b), and executing procedures in simulation (c). These elements are handled by the Task Manager described in this chapter, demonstrating the pertinence of a flight management solution.  The procedural task-based approach of this chapter is not unique: it figures in (Lupashin et al., 2014) who develops a generic pythonic form that need not depend on middleware for task management. All in all, this approach can aid in development, in ways that can be outlined here.   preventing mechanical failure upon software failure.  assist in creating, and coordinating, higher-level behaviours.  monitor the state of every UAV asynchronously.  Assist in troubleshooting with a modular layout.   Summary  This flight has demonstrated multiple working functionalities. The first is the use of the Swarm Programming Interface. Using the building blocks developed in this chapter, it is possible to develop a multi-stage process, one that includes preloaded trajectories as well as waypoint trajectories, choreographic positioning, and escape cases upon a system abort. With such tools for assistance during development, this set of functionalities pushes beyond previous work, as it offers a layer beyond the crazyswarm’s robot instruction set.   Chapter Summary  The proposed state-based architecture is a first step towards creating UAV operations to perform complex tasks, collaboratively or otherwise. After all, this framework has put in place the monitoring tools and the task-based framework to execute complex behaviours; and beyond that, putting in place a Flight Arena has already helped to validated these tools. Such services can easily be tested and deployed from a framework like this one.  Certain elements in this framework are taken a step further in Chapter [c2]: the ability to send streaming setpoints to a drone opens the possibility of flight piloting through other means. This would not be possible without the foundation established in this chapter: the drone architecture, the motion capture setup and the swarm framework.  The tools that were established here may need to be challenged by further research. One direction is the decentralisation of agents with respect to the platform: where this framework has a central role in allocating behaviours, one would opt for a framework that gives each agent the ability to act independently. However, the central monitoring can remain a major asset when developing such a swarm, as it serves as a safety recourse to prevent any hardware damage.  The testbed makes great use of distributed networking, and it aligns with the first approach of the thesis for task creation. From handling specific parameters, to managing the task execution and scheduling in a centralised manner, the middleware monitors the different agents in an asynchronous manner. As opposed to non-distributed systems, such as direct one-to-one links to onboard devices, it allows the developer to divert their focus from system communication to performance-cri(Pradalier, 2020)tical applications.  data-cites=”   Navarro, I., &amp; Matía, F. (2012). An Introduction to Swarm Robotics. In Hindawi. https://www.hindawi.com/journals/isrn/2013/608164/ Brambilla, M., Ferrante, E., Birattari, M., &amp; Dorigo, M. (2013). Swarm Robotics: A Review from the Swarm Engineering Perspective. Swarm Intelligence, 7, 1–41. https://doi.org/10.1007/s11721-012-0075-2 Larrabee, T., Chao, H., Kumar, T., Gururajan, S., Gu, Y., &amp; Napolitano, M. (2013). Design, simulation, and flight test validation of a UAV ground control station for Aviation safety research and PILOT MODELING. In Design, Simulation, and Flight Test Validation of a UAV Ground Control Station for Aviation Safety Research and Pilot Modeling | Guidance, Navigation, and Control and Co-located Conferences. Aerospace Research Central. https://arc.aiaa.org/doi/10.2514/6.2013-5008 Balestrieri, E., Daponte, P., De Vito, L., &amp; Lamonaca, F. (2021). Sensors and measurements for Unmanned Systems: An overview. In MDPI. Multidisciplinary Digital Publishing Institute. https://www.mdpi.com/1424-8220/21/4/1518/htm Varadharajan, V., St-Onge, D., Svogor, I., &amp; Beltrame, G. (2017, June). A Software Ecosystem for Autonomous UAV Swarms. Song, Y., Naji, S., Kaufmann, E., Loquercio, A., &amp; Scaramuzza, D. (2021). Flightmare: A FLEXIBLE Quadrotor simulator. In arXiv.org. 4th Conference on Robot Learning (CoRL), Cambridge MA, USA. 2020. https://arxiv.org/abs/2009.00563 Lupashin, S., Hehn, M., Mueller, M. W., Schoellig, A. P., Sherback, M., &amp; D’Andrea, R. (2014). A platform for aerial robotics research and demonstration: The flying machine arena. In ScienceDirect. Pergamon. https://www.sciencedirect.com/science/article/abs/pii/S0957415813002262 Unmanned systems technology: explanations (viewed on 28 November 2021) . (2021). In Unmanned Systems Technology. https://www.unmannedsystemstechnology.com Phan, T., Hönig, W., &amp; Ayanian, N. (2018). Mixed reality collaboration between human-agent teams. In IEEE Xplore. IEEE. https://ieeexplore.ieee.org/document/8446542 Laclau, P., Tempez, V., Ruffier, F., Natalizio, E., &amp; Mouret, J.-B. (2020). Signal-based self-organization of a chain of uavs for subterranean exploration. In arXiv.org. Frontiers in Robotics and AI (Journal). https://arxiv.org/abs/2003.04409 Pinciroli, C., Lee-Brown, A., &amp; Beltrame, G. (2015). Buzz: An extensible programming language for self-organizing heterogeneous robot swarms. In arXiv.org. 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). https://arxiv.org/abs/1507.05946 Hönig, W., &amp; Ayanian, N. (2020). Flying multiple UAVs using ROS. In TC MRS. IEEE Robotics and Automation Society. http://multirobotsystems.org/?q=node%2F354 Preiss, J. A., Hönig, W., Sukhatme, G. S., &amp; Ayanian, N. (2017). Crazyswarm: A Large nano-quadcopter swarm. In IEEE Xplore. 2017 IEEE International Conference on Robotics and Automation (ICRA). https://ieeexplore.ieee.org/document/7989376 Waliszkiewicz, M., Wojtowicz, K., Rochala, Z., &amp; Balestrieri, E. (2020). The Design and Implementation of a Custom Platform for the Experimental Tuning of a Quadcopter Controller. Sensors (Basel)., 20(7). https://doi.org/10.3390/s20071940 Point, N. Optitrack Motive API (Motive 2.2). Stanford Artificial Intelligence Laboratory et al. (2018). Robotic Operating System (ROS Melodic Morenia). https://www.ros.org et al., S. A. I. L. SMACH Executive Framework (ROS Melodic, 2.5.0). GitHub. https://github.com/ros/executive_smach Nicholas Francis, J. A., &amp; Helgason, D. (2018). Unity3D (Version 2018.4.20). Hönig, W., Milanes, C., Scaria, L., Phan, T., Bolas, M., &amp; Ayanian, N. (2015). Mixed reality for robotics. 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 5382–5387. https://doi.org/10.1109/IROS.2015.7354138 Bischoff, M. (2019). ROS# Framework (Version 1.5). GitHub. https://github.com/siemens/ros-sharp/releases/tag/v1.5 ROS Documentation (accessed 30 September 2021). (2021). In Ros.org. https://www.ros.org/blog/getting-started/ Crazyswarm Documentation (accessed 30 July 2021). (2021). https://crazyswarm.readthedocs.io/en/latest/ Pradalier, C. (2020). A task scheduler for ROS. In HAL Open Archives. GeorgiaTech-CNRS. https://hal.archives-ouvertes.fr/hal-01435823 Carstens, T., &amp; Dufaure, B. Experiment \\refsection:choreography Dataset. \\urlhttps://drive.google.com/drive/folders/1hh6v1r74IvHczOsqgnFKYhRzEkGcKciW?usp=sharing. ","categories": ["Project"],
        "tags": ["ROS library","Python","Motion capture","Swarm robotics"],
        "url": "/project/copilot-solution/",
        "teaser": "/assets/images/posts/swarm-solution/drone-control.jpg"
      },{
        "title": "Gesture Piloting: a performance analysis",
        "excerpt":"   Experimentations for Human-Drone Interfaces  Introduction  In (&lt;span class=\"citation\" tezza_andujar_2019\"&gt;&lt;/span&gt;), define Human-Drone Interaction (HDI) as a field of research that consists of understanding, designing and evaluating drone systems for use by humans, and in contact with humans. This field is similar to human-robot interaction (HRI), however, a drone’s unique characteristic to freely fly in a 3D space, and unprecedented shape makes human-drone interaction a research topic of its own. Researchers develop control modalities and better understand means of communicating with a drone.  Human-drone interaction is a broad research field, for instance, a researcher can design new drones’ shapes with friendly-like appearance, while another researcher can focus on designing new user interfaces that allow non-skilled pilots to accurately operate drones without extensive training.  In line with the thesis goal, we look at two types of smart systems that enhance the interactions between humans and drones. The first allows the human to pilot a drone through a gesture interface. The second looks to a virtual interface as a training ground for a real drone to avoid a virtual object. A distributed system is used to to communicate and coordinate these pipelines by passing messages to one another from any system. To better understand their tradeoffs, the distributed systems are explored and evaluated in this chapter.  Related Work      Exploring more Intuitive Gesture Control  (Tezza &amp; Andujar, 2019)  focalise on innovative methods to interact with drones, including gesture, speech, brain-computer interfaces, and others (Figure [hdi:fields]). As drones have different characteristics than ground robots, such as not allowing touch interaction, it is unclear whether existing techniques can be adapted to flying robots. Their user-centric design strategy seeks to understand how users naturally interact with drones.  Computer Vision for UAV Research  With state-of-art computer vision technology, gesture-based interaction is growing and several publications are identified.      Number Name Gesture     1 Help    2 Ok    3 Nothing    4 Peace    5 Punch          (Cauchard et al., 2015)  contribute to an opensource database of body gestures which they test in practice with a drone (Figure [tab:rescue_dataset]). This paper contributes with an outdoor recorded drone video dataset for action recognition, an outdoor dataset for UAV control and gesture recognition, and a dataset for object detection and tracking. These datasets are developed for emergency rescue services, which reveals how critical these applications can be.  (Liu &amp; Szirányi, 2021)  explores real-time vision-based Human Drone Interaction with multi-robot systems. To create a team the user focuses attention on an individual robot by simply looking at it, then adds or removes it from the current team with a motion-based hand gesture. Another gesture commands the entire team to begin task execution.  Compared to wearable sensor-based approaches, automated methods for video analysis based on computer vision technology are almost non-invasive. This is beneficial, and even critical, for applications in emergency rescue services.  Pose Recognition Algorithms  As a result, performance becomes application-critical for automated methods for video analysis. This, however, remains a technical challenge. According to (Monajjemi et al., 2013), “Robust real-time hand perception is a decidedly challenging computer vision task, as body parts often occlude themselves or each other (e.g. finger/palm occlusions and handshakes) and lack high contrast patterns (e.g. between fingers).” To respond to this challenge, the Mediapipe framework (missing reference) bases itself on a Machine Learning model, and on techniques for efficient resource management for low latency performance on CPU and GPU.      In contrast, OpenPose (missing reference) employs a convolutional neural network to produce two heap-maps, one for predicting joint positions, and the other for partnering the joints into human skeletons. In brief, the input to OpenPose is an image and the output is the skeletons of all the people this algorithm detects. Each skeleton has 18 joints, counting head, neck, arms, and legs. Each joint position is spoken to within the image arranged with coordinate values of x and y, so there’s an add up to 36 values of each skeleton.  Mixed Reality for UAV Research  Simulation systems have long been an integral part of the development of robotic vehicles. They allow engineers to identify errors early on in the development process, and allow researchers to rapidly prototype and demonstrate their idea.  One of the first simulators that could recreate complex worlds in 3D is Gazebo, circa 2004 (Liu &amp; Szirányi, 2021). The difference between Gazebo and different 3D simulation software of that time is that Gazebo was one of the first to focus on resembling the world as realistic as possible for the robot instead of for the human. Immersive robotic simulations can be used to judge the performance of the robot and/or its concept (Hofstede, 2015). In this way, simulators can increase the efficiency and decrease the costs of the development (Larrabee et al., 2013).  The first published definition of Mixed Reality (MR) was given by Milgram and Kishino (Hofstede, 2015) as the merging of physical and virtual worlds. In their definition, Augmented Reality (AR) and Augmented Virtuality (AV) are seen as special instances of MR. In Augmented Reality, virtual objects are projected onto the physical environment, while in Augmented Virtuality, physical objects are incorporated into a virtual environment.  In (Milgram &amp; Kishino, 1994), the definition of Mixed Reality is expanded to robotics by accommodating seamless interaction between physical and virtual objects in any number of physical or virtual environments. It is further demonstrated in (Hönig et al., 2015) that Mixed Reality can reduce the gap between simulation and implementation by enabling the prototyping of algorithms on a combination of physical and virtual objects within a single virtual environment.      In drone research, immersive simulators have various applications (Phan et al., 2018), of which two are explored here:   Generating exteroceptive sensor data: capturing sensor feeds of the environment for one or more drones simultaneously.  Testing navigation behaviour: Testing flight patterns subject to simulated environment stimuli, prior to real-world deployment.   Generating exteroceptive sensor data  Simulation can be a huge advantage when real robot prototypes or products are not available or cannot be used due to other circumstances. During the development, simulation can be used to assess the basic hardware functionality.  For instance, FlightGoggles is capable of high-fidelity simulation of various types of exteroceptive sensors, such as RGB-D cameras, time-of-flight distance sensors, and infrared radiation (IR) beacon sensors. This example can be extended to multiple sensors simultaneously, leading the way to richer distributed swarm systems.  However, older simulators don’t provide an efficient API to access 3D information of the environment (Hofstede, 2015). To foster research in this direction, Flightmare provides an interface to export the 3D information of the full environment (or a region of it) as point cloud with any desired resolution.  Testing navigation behaviour  Controllers evolved in simulation can be found to be inefficient once transferred onto the physical robot, remains a critical issue in robotics, referred to as the reality gap. the most efficient solutions in simulation often exploit badly modeled phenomena to achieve high fitness values with unrealistic behaviors. This gap highlights a conﬂict between the efficiency of the solutions in simulation and their transferability from simulation to reality. When deploying to real-life scenarios, there are several challenges (Song et al., 2021):   Optimising the flight control of a UAV. This is relevant with changing payloads, unexpected weather conditions (dust, rain, changing wind), as well as preventive maintenance (motor degradation, battery damage).  Optimising the flight path of a UAV. Several sensor inputs can inform the drone’s flight path and flight speed. This gives several ways to optimise the data acquisition process, from more complex data intakes and various activation/triggering optimisations.   Prior to real-world deployments, different functional elements on a robot can be tested in parallel and reduce development time. For instance, the algorithms for localization, motion planning or control can be tested, improved, and integrated continuously. There are various artificial intelligence algorithms concerned with the thematic of guidance, navigation and control (GNC). A subset of these algorithms is explored in, pertaining to Deep Reinforcement Learning (DRL). These techniques can improve the drone operation as shown in Table [tab:RL_applications].      Task Input Observations Output Actions     1. Quadrotor control [p, θ, v], dim=10 [c, ωx, ωy, ωz]], dim=4   2. Control under motor failure [p, θ, v, ω], dim=12 [f1, f2, f3], dim=3   3. Flying through a gate [p, θ, v, ω, pgate, θgate], dim=18 [f1, f2, f3, f4], dim=4     Current research of drone quadrotor control employs newly architected neural networks and learning time-optimal controllers for drone racing (Azar et al., 2021). This element is largely figured in Flightmare’s (Azar et al., 2021) simulation usecases, and echoes the state of the art research in Reinforcement Learning for UAVs (Song et al., 2021), suggesting that new RL implementations can optimising the flight stability of a UAV as well as new perception pipelines for the navigation of a UAV. Flightmare offers convenient wrappers for reinforcement learning. Those gym wrappers give researchers a user-friendly interface for the interaction between Flightmare and existing RL baselines designed around the gym interface.  Drone Piloting With Gesture  Gestures are the most natural way for people to express information in a non-verbal way. Users can simply control devices or interact without physically touching them. Nowadays, such types of control can be found from smart TV to surgery robots, and UAVs are not the exception.      Drone piloting and other control modalities (Azar et al., 2021) make use of various inputs to assist in flight. Perception modules for drone flight usually consist of data-driven models based on multiple sensor modalities. These inputs can be sensor modalities, such as camera, lidar, and radar, published in autonomous-driving related datasets, but also human commands, in the case on drone piloting. In this way, perception pipelines are routinely developed as a realtime interface for sensor data from multiple perception configurations.  As of , multiple gesture interfaces have been developed for UAVs (Tezza &amp; Andujar, 2019) (Liu &amp; Szirányi, 2021), but are lacking in drone piloting. Realtime interfaces for drone piloting are discouraged (Monajjemi et al., 2013) due to high latency and low control precision compared to other drone control modalities. As of , the literature utilizing the Crazyflie nanodrone does not include realtime streaming commands (Tezza &amp; Andujar, 2019).  Overview of Pipeline  A pipeline is implemented to ensure that the right gesture is associated to the right drone command, in real-time and continuously. This pipeline is conceptualised as in Figure [fig:handpipeline].      The Testbed of Chapter [c1] offers a working environment, as well as a command streaming interface between a drone and a companion computer. The focus is therefore on the two first elements: a gesture recognition workflow, followed by a drone control workflow.  Gesture Recognition Layer  In this project, we employ a 3D Pose Estimator, described in the following section, followed by a Gesture Classification Script.      Machine Learning 3D Pose Estimation      MediaPipe Hands (“Published Research That Makes Use of the Crazyflie Drone (Accessed 17 September 2021),” 2021) is a high-fidelity hand and finger tracking solution. It employs machine learning (ML) to infer 21 landmarks of a hand from just a single video frame.      Precise key-point localization of 21 3D hand-knuckle coordinates remain inside the detected hand regions. This allows us to have the spatial position of each of the joints of a hand using only a normal camera.  Whereas current state-of-the-art approaches rely primarily on powerful desktop environments for inference (missing reference), MediaPipe Hands achieves real-time performance on a mobile phone, and even scales to multiple hands, making it an ideal solution for real-time pose tracking.  The desired gesture is hardcoded by its absolute position. For example, if Figure 1’s landmark 8 is below the landmark 5, it can be interpreted as closing your index.      Gesture Classification Script        While the model is precise in landmark detection, there are false detections of gestures due to marginal cases. A special buffer is used to filter the most frequent gesture on a sliding window (for every 10 detections). This helps to remove glitches or inconsistent recognition.  Drone Control Layer  Using a live gesture recognition module, a system is designed for streaming commands to be sent to the drone.      Note that the critical information flow between the components of the system is unidirectional. Bidirectional communication, e.g. telemetry from the vehicles, is supported, but is not required for controlled operation. All communication is done in a distributed, one-way manner, such that the gesture recognition workflow is not affected by the drone listener and there is no reliance on high-level code to keep track of the various components, preventing unnecessary interdependence. The Gesture-to-Command script decrypts the messages encoded into a custom ROS message. This workflow serves as a fallback during experiments and demonstrations.  Message passing between applications  The following message is passed via a ROS topic (TCP/IP message). It is then depacketized upon arrival.      Mode Selection      The following element in the drone control layer is the selection of an adequate mode.   Flight Mode 1: Position Update Mode. This mode moves the drone translationally in three axes based on an absolute position.  Flight Mode 2: Velocity Update Mode. This mode moves the drone according to inputted velocities.   In each mode, the drone can move up, down, left or right. The following hand gestures are associated with these directions.      Velocity Filter      The velocity filter serves as a safety measure during development and demonstrations. Given a drone’s position in the Flight Arena, it attributes a particular value between 0 and 1. This value is then multiplied to the velocity value determined from the gesture movement.  The shape of this Velocity Filter was defined using an ellipsoid, with a near constant velocity within the ellipsoid and a sigmoid on the boundaries of this shape. An initial volume is designed on [equation:ellipsoid].      $$A_{1} = \\frac{x^2}{a^2} +\\frac{y^2}{b^2} +\\frac{z^2}{c^2} \\text{ with } (a, b, c) = (1.35, 0.85, 1.1)     \\label{equation:ellipsoid}$$  This figure is scaled according to constants (a, b, c). They are determined empirically by measuring the furthest distance measured by the motion capture. This ellipsoid is then transformed to better approximate the required filter.  A2 = α * ((1−A1)−β) with (α,β) = (18,0.3)      The volume is scaled up by α and shifted down by β. Determining the α and β scaling parameters, leading to the shape in Figure [fig:vel2].      $$A_{3} = \\frac{1}{1+e^{-A_{2}}}     \\label{equation:sigmoid}$$  A logistic regression allows for a smooth speed transition at the limits of this volume.  Gesture Speed and Angle      Hand movement is separated into angle and speed. As the hand moves in a specific direction on the screen, the components of that vector can be used to calculated the speed and angle of the drone’s movement. To help smoothen the output velocity, the mean pixel distance is taken over a rolling window.        Using these key landmarks, it is possible to discern hand poses and develop a library of drone-piloting hand signals. These are programmed accordingly in the Experimentation Section.  Performance Analysis  Aim  We put in place a demonstration for flight piloting in real-time using the developed gesture interface. We present the workflow of real-time gesture piloting pipeline and we evaluate it in terms of:   System response time  Accuracy of gesture recognition   Evaluation Techniques      System Response Time  The system response time is verified by applying a series of rapid maneuvers to register any significant delays between the pilot’s commands and their execution by the flight control system. Similarly, (Liu &amp; Szirányi, 2021) choose to modify the drone’s angle in a specified direction. This choice is arbitrary and the changes in velocity are used in this case.The input was a demanded velocity in a specified direction. The input was changed randomly by the operator with hand movements, using the workflow described in this chapter. The output was a delay of the velocity change in the drone. Finally, a system response time is determined by averaging the response delays over the experiment.  Gesture Recognition Effectiveness  In order to evaluate gesture recognition performance, we identify the false positive and negative rates of the pose detection, to compare with existing research in real-time gesture detection. In comparison, (Waliszkiewicz et al., 2020) evaluates the false positive and negative rates of the pose detection by manually identifying both the incorrectly recognised gestures, and the unrecognised gestures. Similarly, we identify the false positive and negative rates of the pose detection, but instead of doing it manually, we examine any discrepancies in the UAV’s trajectory flight. Any inconsistencies in recognition are considered false positives.  Methodology for Piloting Operation  We design our experiments for an operator to guide the drone in an intuitive way through hand commands.      Dataset The experiment was filmed from three angles, and a presentation video is uploaded on Youtube (Bolin et al., 2017). The results were saved in a rosbag format on Google Drive (Carstens &amp; Richalet, n.d.). The data that is examined extends from 11:19:20 to 11:20:30 on July 29, 2021.  Throughout this procedure, data is collected as a rosbag, a self-contained file for recording ROS nodes and topics. In post-processing, we timestamp the hand signal stream. This file is available at (Carstens &amp; Richalet, n.d.) and contains:   The poses of the drone, ordered by timestamp: /tf topic.  The hand gesture message contents: /hand_signal topic.   Results  Overview of Results The two flight regions were plotted separately. The trajectory is plotted on 3 planes. The drone’s trajectory is first plotted on the X-Z plane (as per Figure [fig:refframe1]). The two flight regions are separated by locating the transition gesture’s timestamp.                  Gesture Recognition Effectiveness In preparation for this evaluation, we label the drone positions where each hand signal is detected. Figure [fig:command_labels] superposes the drone’s trajectory and the hand gestures identified at that particular point on the drone’s path.  Flight Timeline with Annotated Hand Signs          The detection of hand gestures is found to be mostly continuous. Using the methodology outlined earlier, the false positive and false negative rates can be determined.      Criteria RIGHT LEFT UP DOWN THUMBUP     len 342 280 150 87 201   False Positives 5 33 3 6 201   False Negatives 30 175 10 5 0   Accuracy (% correct) 89.7 25.7 93.33 87.35 0         Criteria INDEX PEACE THUMBDOWN TOTAL     len 192 131 100 1483   False Positives 15 6 100 369   False Negatives 23 36 0 299   Accuracy (% correct) 80.2 67.94 0 56.3     Looking at the graphs, it seems that left and down gestures are quite regularly mistaken for one another. In contrast, gestures are different enough (such as right and index) are recognised at 90%.This demonstrates an interesting limitation in the pipeline’s design: the recognition seems stumble between two similar gestures.  The effectiveness is averaged as the total percentage of correct gestures over the full dataset. The accuracy is determined as of the full gesture dataset.  System Response Time In preparation for this evaluation, we to plot the speeds at which the poses are streamed, as well as the desired speed transmitted from the gesture script. The velocities of the actual drone are calculated as per Equation [eq:speeds] from successive pose data points over the period of interest.  Δx = xf − xi and similarly for Δy, Δz  $$v^* = \\frac{\\delta u}{\\delta t} = \\frac{\\sqrt{\\Delta_{x}^2 + \\Delta_{y}^2 + \\Delta_{z}^2}}{t_f - t_i}     \\label{eq:speeds}$$ This calculation is a simplification given the pose data has a stable 120 ± 0.4 Hz transmission frequency, which is ascertained during the experiment (Figure [fig:frequency_check]).      Figure [fig:velocities] plots the drone’s position and its associated velocity in Position Update Mode (blue) followed by Velocity Update Mode (red).      The red graph is significantly more jaggered. This is expected since the velocity updates depend on fast moving hand movements. In Figure [fig:responsiveness], we take a closer look at the interactions between a drone’s trajectory and the signs identified at that particular instance. The velocity commands (in green) are plotted alongside the drone responses (red).      The system response time is determined from Figure [fig:responsiveness] by locating specific spikes of velocity change, in the velocity command stream, and locating spikes of velocity change in the drone flight stream. Their timestamps are recorded in Table [tab:velocity_latency].      Points 1 2 3 4 5     Velocity Command 19.49.636 19.50.2053 50.4723 51.3419 52.2437   Velocity Response 19.9789 19.568 50.728 51.6482 52.5579   Duration of Latency 342.9 362.7 255.7 306.3 314.2         Points 6 7 8 9 10     Velocity Command 53.753 54.7583 55.3996 56.4005 57.6692   Velocity Response 53.9478 54.898 55.6579 56.6375 57.9679   Duration of Latency 194.8 139.7 258.3 237.0 298.7     The average latency is found to be from the average latency of 10 points. This latency remains rather consistently between 200 and 300 ms, which demonstrates stability over time. We can perform a cross-validation this result with a visual check: we superpose the graphs by eye to determine an approximate value (Figure [fig:response_shift]).      To create this superposition, we shift the second graph by a difference of 250ms. This agrees with the experimental latency of 200-300ms.  Discussion      While Chang Liu et al. focus on outdoor datasets for single large drones, this work looks towards interacting specifically on the drone’s position. Such a specific usecase of hand-following seems to be relatively rare in the literature. In fact, Tezza et al. (Carstens &amp; Richalet, n.d.), despite their survey of the research field, remain sceptical as to whether this method might be the best approach to applications that require fine and precise control, as they pose the problems of higher latency and lower accuracy than other methods such as a remote controller. This vision is coerced with other members of the HDI community, and most datasets focus rather on signaling events to the drone, instead of direct piloting (Figure [tab:rescue_dataset] from (Tezza &amp; Andujar, 2019)). .      Model Pixel 3 Samsung S20 iPhone11       Light 6.6 5.6 1.1     Full 16.1 11.1 5.3     Heavy 36.9 25.8 7.5       This performance analysis has measured the pipeline latency is evaluated at 270ms. This is in large part thanks to MediaPipe Hands algorithm (Liu &amp; Szirányi, 2021). This algorithm is relatively new, and demonstrates real-time inference capabilities, with a maximum inference of 36ms for hand landmarks (as shown in Figure [fig:mediapipe_alone]). This performance is evidently far different from that of the perception pipeline developed here around the Mediapipe framework, with different equipment.  This experiment has offered a way to approach hand-drone interaction. Other approaches can offer a fuller exploration of the operator’s ease in controlling the drone, by examining the frequency at which different hand signals are used. As the operator controls the drone by sight, it is possible for them to make minute readjustments. As a result, further research could examine the role of intuition within this gesture loop. It might also be possible to explore instances where the operator does not look at the drone. Without visual feedback, this could give better hints as to the controller’s effectiveness subject to clear hand commands.  Regarding the Thesis      In creating better service drones, one might wonder if a piloting system is an effective means to research and development. It could easily help manage swarms of drones, but is drone development the type of research that requires the operator to make split-second decisions? After all, certain tasks require split-second reactions: drones doing free-fall recovery for instance. Perhaps it could be the beginning of an era of drone real-time learning, where drones can develop functionalities more rapidly than before, through kinetic means. Figure [fig:selfie] is taken from the DJI website, and shows a gesture instruction for a drone to take a picture. Perhaps functionalities like this can become more natural, more closely coupled with human behaviour.  Summary  The gesture interface used to pilot the drones is given 56% accuracy. While the pipeline is based on MediaPipe Hands, the pose classification was hardcoded, and the software can then be improved with a neural classifier or an ML pipeline. In practice, the errors were filtered out by the drone control pipeline.  Mixed Reality Interface      Drone piloting and other control modalities (missing reference) make use of various inputs to assist in flight. Perception modules for drone flight usually consist of data-driven models based on multiple sensor modalities. These inputs can be sensor modalities, such as camera, lidar, and radar, published in autonomous-driving related datasets, but also human commands, in the case on drone piloting. In this way, perception pipelines are routinely developed as a realtime interface for sensor data from multiple perception configurations.  A mixed reality interface serves to enable data transmission between a physical drone and its virtual equivalent. This section documents the design of a mixed reality environment (Figure [fig:reality_interface]). The first objective of the simulated environment is to serve as a graphical interface in order to develop tasks otherwise too difficult to deploy. The priority of the virtual reality is therefore set on rendering capabilities, and the ability to obtain camera streams from this environment.  Selected modules and technologies  This section is a brief mention of all the platforms, systems, services, and processes this interface depends on.   Unity3D (Tezza &amp; Andujar, 2019) is a popular game engine which offers a simulated environment. It is set up as the virtual companion to the Flight Arena. Unity is well suited since it enables high-fidelity graphical rendering, including realistic pre-baked or real-time lighting, flexible combinations of different meshes, materials, shaders, and textures for 3D objects, skyboxes for generating realistic ambient lighting in the scene, and camera post-processing (Nicholas Francis &amp; Helgason, 2018).  ROS Sharp (Song et al., 2021) is a set of open source software libraries and tools in C# for communicating with ROS from .NET applications, in particular Unity.  ROS (Bischoff, 2019) is a set of software libraries and tools that assist in building robot applications.   For the sake of replicability, the version of each module is documented in the references.  Conceptual Overview  The link between the real and the mixed reality is designed with the following core capabilities:   Transmitting the pose of a real drone into a virtual environment.  Transmitting an event between the physical and the virtual environment.   For instance, a drone collision with a virtual object would have the following workflow (Figure [proposed_workflow]).          The process of transmitting the pose of the drone to the simulator is referred to as pose injection. This is done via the Network interface, from ROS to the simulator. The process of collision occurs in the simulator, between the injected pose, and a virtual body. This is done via a collision interface within Unity.  These two elements can be readapted to a variety of event-driven scenarios. For this reason, a mixed reality setup offers inexhaustive resources to drone development.  Overview of System Network Interfaces  In order to establish a two-way mixed reality interface, the simulator and the robotics backend are configured to communicate to each other.      To return to the System Network Layout (Chapter [c1], Figure [fig:network_reference]), the Mixed Reality Interface involves Unity3D as well as the Task Manager.  Event Sharing Workflow  A Network Interface for Mixed Reality Event Sharing      A Network Interface is used for the two objectives: injecting pose messages into the game engine and retrieving event data to be sent to the ROS Task Manager.  Collision Detection in the Virtual Environment  The virtual environment is a Virtual Arena. A particularity of this arena is that it is 3 times larger than the actual flight arena. In other words, the drone’s recorded position differs from the real arena by a factor of 3, and the drone’s velocity also differs by 3. The two agents are also embodied by virtual characters, annotated as 1 and 2 in the visual below. Video feeds (Figure [fig:video_feed]) show the perspectives of both agents, and these are recorded as part of the experiment.  An Event Stream Using this Network Interface      An Event Detection is triggered within the game engine when a particular condition is met, and it then publishes the corresponding message.  The Message Stream communicates the event data using ROS Messages (Stanford Artificial Intelligence Laboratory et al., 2018). ROS messages cater to a variety of sensor formats, from cameras, to pointcloud data, allowing for the ROS backend to make further decisions upon processing this data.  Registering an Event in the Robotics backend  The robotics interface, explored in Chapter [c1], functions on a Task basis. Events that are streamed on the network therefore need to be connected to processes for task rescheduling as well as drone state changes. Using the Topic Monitor from Section [section:SPI], changes in a streamed message can be made to induce state changes which, in turn, affects task management processes.  Until the event reaches the task management interface, the real drone is programmed to fly using Chapter [c1]’s high level interface. The full behaviour of the drone can be visualised as in Figure [collision:sm_design].      This state machine functions for a single drone: using the swarm building blocks developed in Section [section:SPI], the real drone to move to certain waypoints indefinitely. When a virtual collision is detected by the robotics backend, it induces a state change. The next state loads a custom trajectory on the drone, which is executed, before returning to an its looping trajectory.  In the next section, an experiment demonstrates the proposed workflow with a collision between a drone and a virtual body, and then to examine the performance of such a system.  Performance Analysis  We set up a virtual interface between real and virtual objects in real time. This MR simulation consists of a network interface between a robotics backend (ROS) and virtual environments (Unity3D). Similarly to (“ROS Documentation (Accessed 30 September 2021),” 2021), the pipeline is then evaluated in terms of communication latency for two separate scenarios.   when transmitting parameters into the simulated environment  when transmitting parameters to the robotics backend.   Prediction  Latency of drone pose into a virtual environment      The latency of the pose injection is measured by determining the time difference between the ROS position and the time when it was received by the simulator.  Latency of event from the simulator to ROS      We can answer the performance question by investigating the lag time between the moment of collision and the moment the drone reacts. We choose the moment of a Virtual Collision because it is the ideal moment of a collision between the drone’s virtual avatar, and the bot agent. The collision lag time is illustrated in Figure [collision_lag_time].  Method  The time of different events is recorded as shown in Figure [feedback_loop]. The experiment runs as such:   a single drone is flown in the Flight Arena and it is virtualised as the drone agent.  Likewise, a virtual bot agent flies a trajectory in a game Engine.  When the drone and the bot collide, the drone is designed to react, by flying a pre-programmed spiral trajectory.   Data logger setups          This requires two separate data loggers: the one, monitoring the Unity environment, logs the timestamp and pose upon the virtual collision, and the other logs the timestamp of the drone State Change from within the Task Manager.  Results      Real-to-virtual Recorded Positions The drone poses are obtained through ROS and the simulator. In Figure [fig:superposition], these posees are superposed. The drone can be seen in green for ROS-times at high-rate sampling (120Hz) and in purple for Unity-times at a lower sampling (10Hz). The positions superpose perfectly. This is expected. This low sampling is sufficient to show the accuracy of the real-to-virtual procedure.  Real-to-virtual Timestamps A second graph examines the differences in timestamps of simulator time (green) relative to ROS-time (red). Figure [fig:pos_time] shows a substantial lag in positions.          The avatar positions occur \"before\" real positions. This is due to the logs being based on simulation time, which records events slower than real time. This simulator clock seems to be affected by a system latency.  Latency of Pose Injection We determine the latency of elements when injected into the virtual environment. The latency of the system is associated to the time between simulation and ROS time when recording the same drone pose. The resulting graph is plotted in Figure [fig:xr_poses] and shows a linear trend.      By adapting a linear regression, we determine a gradient of of cumulative latency.  Latency During State Change In order to visualise the event sequence, each collision is assigned a red marker, with the moment of robotics backend state change being assigned a yellow marker. All the collisions are taken from the virtual logs and assigned ROS-compatible timestamps.      Collision Graphs of the Latency Experiment          The first plot shows a timeline view, where the moments of start and end of the experiment clearly show a change in Z, on three different occasions. These three collisions are associated to state changes.  These three collisions in particular are investigated, occurring 15 seconds from each other. Each collision latency is calculated according to the method set in the methodology. Figure [fig:latency_collisions_xr] demonstrates a growing lag time that approximates an exponential - a similar performance bottleneck to the previous section.          This trendline is modelled after an exponential as follows: Lout = (6.67589×10−8)e0.190664t ms  We can model the full end-to-end system latency as the addition of the latency and the 98t ms above:  Lsystem = Lin + Lout = 89(7.501×10−7e0.190664t+t)ms  Evaluation      Similarly to the Swarm Application Interface of Chapter [c1], Flightmare offers several tasks as part of their simulator, however they do not undergo tests with real hardware. This discrepancy naturally reflects in the differences in latency, where our system is dependent on a robotics backend on top of a simulator. This chapter can be considered a perception pipeline as opposed to a set of tests that undergo in simulation.  As opposed to simulators with an independent block for the physics engine, this experiment has focused mainly on visualising drone flight. Flight physics modelling is deliberately excluded. This lends itself well to a more photo-realistic, but slower, configuration.  On a functional standpoint, the proposed workflow worked as expected. A virtual body did come in collision a number of times with the drone’s avatar; through event data, the drone has reacted accordingly. This sequence of events was ensured by the choices of software architectures.  To respond to the performance question, we focus on the three key aspects highlighted in the Mixed Reality literature review:   Fast prototyping of new environments: programmability.  A wide suite of sensors and of physical effects: variability.  A true-to-reality physical environment: the physical model.   While the Mixed Reality Interface provides us with a simulated graphics engine, a communication channel was put in place that would communicate virtual events to the robot swarm. However, the collision experiment has demonstrated a cumulative delay of for a single quadrotor, and this can only increase with larger swarms and more complex manoeuvres. Since latency is a primary measure for image streaming and high performance drone tasks, we suggest the exploration of a network interface more focused on performance, and possibly the integration of existing simulators like Flightmare within the testbed.       Test Description Result     Gesture Piloting    Gesture Recognition Effectiveness    System Response Time    Mixed Reality Interface    Latency of Pose Transmission    Latency of State Changes    System Latency       Summary  According to (Kumar et al., 2020), a fully functional GCS provides for research capabilities which cannot be achieved through R/C flight alone. They emphasize Parameter Identification (PID) research as an alternative for flying a UAV by tracking values of flight and performing precise maneuvers. The second emphasis is into \"research into new applications of subscale aircraft for otherwise dangerous or long mundane tasks\". These goals echo the elements for live performances and flight recording used in our testbed.  Preiss et al. (Larrabee et al., 2013) envision that mixed reality would interconnect a wide variety of physical spaces for collaboration. Humans can work safely within their own physical confines, while their intelligent counterparts can operate in more hazardous environments.  With these new mixed reality tools, Preiss et al. position their robotics testbed as \"serving to acclimate end users with autonomous systems\". They believe their approach is also well suited for mixed reality prototyping since they \"will be able to substitute networking and AI components with alternative implementations\", for instance by substituting onboard path finding onto offboard components. They further demonstrate that peer-to-peer networking can better simulate intercommunication between drones. With this, Preiss et al. uphold that mixed reality is a vital addition to better human-drone interfaces.  This chapter demonstrates a similar vision, through the practical means of human-agent interactions. With the data streaming interfaces of this Mixed Reality section, as well as from the Piloting section, this shows that new modalities can be created for autonomous vehicles.  Using the networked approach of events and sensor data, further tasks can be prototyped. This aligns with the goals of better service drones. Using the example tasks developed in the previous chapter, various GNC algorithms can be programmed, developed, and tested on hardware conditions.  This method encompasses a major amount of development on the drone platform. From the development of a custom backend, to the interconnection of a graphical simulator, this completes an ecosystem for research and development. While this work has been in major part, infrastructural, it opens the door to the development and testing of GNC algorithms, for instance Reinforcement Learning algorithms, a common occurrence in recent robotics.  Chapter Conclusion  This chapter presents a streaming architecture for piloting UAVs using a webcam, and various forays into Human-Drone interactions. This architecture, which makes use of the drones’ command architecture, but also of a shared network, has lent itself to integrating various inputs – in this case webcam images. The output of this exercise is evident in the precision of the drones’ movement, as it was noticed in the various visualisations of the data.  However, While the Mixed Reality Interface provides us with a simulated graphics engine, a communication channel was put in place that would communicate virtual events to the robot swarm. However, the collision experiment has demonstrated a cumulative delay of for a single quadrotor, and this can only increase with larger swarms and more complex manoeuvres. Since latency is a primary measure for image streaming and high performance drone tasks, we suggest the exploration of a network interface more focused on performance, and possibly the integration of existing simulators like Flightmare within the testbed.  Using the networked approach of events and sensor data, further tasks can be prototyped. Various GNC algorithms can be programmed, developed, and tested on hardware conditions.  All in all, this work has come to demonstrate that a swarm setup can be rather easily adapted to human-drone research, and despite the performance bottlenecks, it succeeds in providing an end-to-end experience for the pilot, and further work should aid in(Phan et al., 2018) streamlining this.  ","categories": ["Project"],
        "tags": ["ROS library","Python","Unity3D","Motion capture","Hardware-in-the-loop robotics"],
        "url": "/project/gesture-piloting/",
        "teaser": "/assets/images/posts/virtual-training/FG-topview.jpg"
      }]
